{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load emnist samples and adjust data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "def load_emnist(file_path='emnist-bymerge.mat'):\n",
    "    \"\"\"\n",
    "    Loads training and test data with ntr and nts training and test samples\n",
    "    The `file_path` is the location of the `eminst-balanced.mat`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Load the MATLAB file\n",
    "    mat = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Get the training data\n",
    "    Xtr = mat['dataset'][0][0][0][0][0][0][:]\n",
    "    ntr = Xtr.shape[0]\n",
    "    ytr = mat['dataset'][0][0][0][0][0][1][:].reshape(ntr).astype(int)\n",
    "    \n",
    "    # Get the test data\n",
    "    Xts = mat['dataset'][0][0][1][0][0][0][:]\n",
    "    nts = Xts.shape[0]\n",
    "    yts = mat['dataset'][0][0][1][0][0][1][:].reshape(nts).astype(int)\n",
    "    \n",
    "    print(\"%d training samples, %d test samples loaded\" % (ntr, nts))\n",
    "\n",
    "    return [Xtr, Xts, ytr, yts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697932 training samples, 116323 test samples loaded\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xts, ytr, yts = load_emnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697932, 784) (116323, 784) (697932,) (116323,)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape,Xts.shape,ytr.shape,yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrd=np.reshape(Xtr,(697932,28,28),order='F')\n",
    "Xtsd=np.reshape(Xts,(116323,28,28),order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10fbfb2b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181f904470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Xtrd[np.random.randint(1,20000),:,:],cmap='Greys_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Xtsd[np.random.randint(1,10000),:,:],cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntr = 46000\n",
    "nts = 10000\n",
    "\n",
    "# TODO: proper decide the number of samples and the ratio between dig and let\n",
    "\n",
    "# Create sub-sampled training and test data\n",
    "nsamp = Xtr.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xtr1 = Xtrd[Iperm[:ntr],:,:]\n",
    "ytr1 = ytr[Iperm[:ntr]]\n",
    "nsamp = Xts.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xts1 = Xtsd[Iperm[:nts],:,:]\n",
    "yts1 = yts[Iperm[:nts]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28)\n",
      "[[234  95   7   0   0]\n",
      " [253 202  32   0   0]\n",
      " [254 217  39   0   0]\n",
      " [254 233  82   2   0]\n",
      " [252 249 125   4   0]]\n",
      "[ 3  9  9 ... 42  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr1.shape)\n",
    "print(Xtr1[233,15:20,15:20])\n",
    "print(ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28, 1) (10000, 28, 28, 1) (46000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = Xtr1.astype('float32')\n",
    "x_test = Xts1.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train=x_train.reshape((ntr,28,28,1))\n",
    "x_test=x_test.reshape((nts,28,28,1))\n",
    "y_train=ytr1.reshape((len(ytr1),1))\n",
    "y_test=yts1.reshape((len(yts1),1))\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1820ca1c50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAADh9JREFUeJzt3X+MVfWZx/HPw2+dkgg2O4zWlS7i\nRsSE6sRsCJqaSMOSGuw/pvyhs9m6I6boNtnEJe4fa7KuMZuWzSYqcbCE6aYr3UQbsVEpS8wOJptG\nhCo4QMclNEAGWEVF4w8c5tk/5tBMlfM9w73n3nOG5/1KJtx7n3POfbiZz9xz7/ec8zV3F4B4plTd\nAIBqEH4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0FNa+eTmRmHEwIt5u42keWaeuc3sxVmdtDM\n3jGzdc1sC0B7WaPH9pvZVEm/k7Rc0lFJr0ta7e6DiXV45wdarB3v/DdLesfdD7n7GUlbJK1qYnsA\n2qiZ8F8p6ci4+0ezx/6ImfWa2S4z29XEcwEoWcu/8HP3Pkl9Erv9QJ00885/TNJV4+5/I3sMwCTQ\nTPhfl7TQzL5pZjMkfV/S1nLaAtBqDe/2u/uIma2VtE3SVEmb3P3t0joD0FIND/U19GR85gdari0H\n+QCYvAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIP\nBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IquEpuiXJzA5L\n+kjSWUkj7t5dRlMAWq+p8Gduc/d3S9gOgDZitx8Iqtnwu6Rfm9kbZtZbRkMA2qPZ3f5l7n7MzP5E\n0nYzO+DuA+MXyP4o8IcBqBlz93I2ZPaIpI/d/ceJZcp5MgC53N0mslzDu/1m1mFms8/dlvQdSfsa\n3R6A9mpmt79T0i/N7Nx2/sPdXymlKwAtV9pu/4SejN1+oOVavtsPYHIj/EBQhB8IivADQRF+ICjC\nDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFXG1XtD6O7Ovyr5Pffck1z3qaeeStaHhoaS\n9bNnzybrQCN45weCIvxAUIQfCIrwA0ERfiAowg8ERfiBoCbVpbtnzJjR8LpnzpxJ1mfNmpWsDwwM\n5NZSxwBI0sjISMPblqS77747WR8eHk7Wq7Rw4cLc2syZM5PrHjp0KFn/5JNPGurpYseluwEkEX4g\nKMIPBEX4gaAIPxAU4QeCIvxAUIXj/Ga2SdJ3JZ1098XZY3Ml/ULSfEmHJd3l7u8XPlnBOP/UqVOT\n67/yyiu5tYMHDybXfeCBB5L1xYsXJ+t79uzJrZmlh1WL6kVOnTqVrN966625tcHBwaaeu8jSpUuT\n9W3btuXWLrnkkuS6H374YbK+ZcuWZP3RRx/NrdX52IhmlTnOv1nSii89tk7SDndfKGlHdh/AJFIY\nfncfkPTlt55Vkvqz2/2S7iy5LwAt1uhn/k53P7ffdFxSZ0n9AGiTpq/h5+6e+ixvZr2Sept9HgDl\navSd/4SZdUlS9u/JvAXdvc/du909ffYLgLZqNPxbJfVkt3skvVBOOwDapTD8ZvaspP+R9OdmdtTM\nfiDpcUnLzWxI0u3ZfQCTSK3O57/66quT6x84cCC3NmVK+u/Y7bffnqyvXr06WV+1alVuraenJ7cm\nSY899liyXnQ9gCKp89rXrl2bXHfz5s3J+rx585L1ojkHOjo6kvVWSh0fcf311yfXPXHiRNnttA3n\n8wNIIvxAUIQfCIrwA0ERfiAowg8EVauhvqJTX5988snc2po1a5Lrfvrpp8n69OnTk/X169fn1tat\nS5/UeOmllybrL730UrKeOmW3yAcffJCsL1++PFm/7bbbkvXHH08f4tHf359be++995Lr3nvvvcn6\nZZddlqynfrc3bdqUXPfBBx9M1ot+n6rEUB+AJMIPBEX4gaAIPxAU4QeCIvxAUIQfCKpW4/xFrr32\n2tza7t27k+sWjbWPjo4m6zfddFNu7c0330yuW6TK4wA+//zzZH3atPSV3k6fPp2sL1q0KLdWdNrs\nddddl6y/9tpryfqcOXOS9ZRnnnkmWS86DuCzzz5r+LmbxTg/gCTCDwRF+IGgCD8QFOEHgiL8QFCE\nHwhqUo3zp/T2pmcE27BhQ1Pbv//++3NrfX19TW27SNFxAC+//HJu7ZZbbkmuW3QNhaLfj6effjpZ\nT71uzSo6DmD79u25tSuuuCK5btH/e+fOncn6ypUrk/XU5dabxTg/gCTCDwRF+IGgCD8QFOEHgiL8\nQFCEHwiqcJzfzDZJ+q6kk+6+OHvsEUl/I+n/ssUedvf0Sedq7Th/Z2dnsj44OJisF537feTIkdza\nggULkuuOjIwk683q6urKre3duze57ty5c5P1ot+P++67L1kvOi++lWbPnp1b279/f3LdouMAigwM\nDCTrK1asyK01ey2AMsf5N0s6X6f/6u5Lsp/C4AOol8Lwu/uApFNt6AVAGzXzmX+tmb1lZpvMrPHr\nJQGoRKPh3yBpgaQlkoYl/SRvQTPrNbNdZrarwecC0AINhd/dT7j7WXcflbRR0s2JZfvcvdvduxtt\nEkD5Ggq/mY3/evl7kvaV0w6Adklfl1mSmT0r6duSvm5mRyX9o6Rvm9kSSS7psKT0eA+A2rlozucv\nUjSP/EMPPZSsp86/vuaaa5LrHj9+PFlvpaVLlybr27ZtS9ZnzpyZrN94443J+r599dwpbPWcAEW5\nWrNmTW5t48aNyXWLcD4/gCTCDwRF+IGgCD8QFOEHgiL8QFBhhvpuuOGGZH3Pnj0Nb3vJkiXJel2H\nuyRp3rx5yXrRKb9DQ0PJ+hdffHHBPdVB0RDpq6++mqxPnz49WX///fdza6lpzaXiqc0Z6gOQRPiB\noAg/EBThB4Ii/EBQhB8IivADQYUZ5581a1ay/uKLLza87TvuuCNZb/ZSzGi/oqnLn3jiiWS9aGry\n0dHR3Fqzx40wzg8gifADQRF+ICjCDwRF+IGgCD8QFOEHggozzl9kypTG/w6mxmxxcero6EjWDxw4\nkKynppRv9nLojPMDSCL8QFCEHwiK8ANBEX4gKMIPBEX4gaAKx/nN7CpJP5PUKckl9bn7v5nZXEm/\nkDRf0mFJd7l7/sXIVe9xfqBMqXF8Sbr88stza83OhVDmOP+IpL9z90WS/kLSD81skaR1kna4+0JJ\nO7L7ACaJwvC7+7C7785ufyRpv6QrJa2S1J8t1i/pzlY1CaB8F/SZ38zmS/qWpN9I6nT34ax0XGMf\nCwBMEtMmuqCZfU3Sc5J+5O6nx1/jzN097/O8mfVK6m22UQDlmtA7v5lN11jwf+7uz2cPnzCzrqze\nJenk+dZ19z5373b37jIaBlCOwvDb2Fv8TyXtd/f140pbJfVkt3skvVB+ewBaZSJDfcsk7ZS0V9K5\nc1cf1tjn/v+U9KeSfq+xob5TBdtiqA9osYkO9XE+P3CR4Xx+AEmEHwiK8ANBEX4gKMIPBEX4gaAI\nPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGg\nCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAKw29mV5nZq2Y2aGZvm9nfZo8/YmbHzOy32c/K\n1rcLoCzm7ukFzLokdbn7bjObLekNSXdKukvSx+7+4wk/mVn6yQA0zd1tIstNm8CGhiUNZ7c/MrP9\nkq5srj0AVbugz/xmNl/StyT9JntorZm9ZWabzGxOzjq9ZrbLzHY11SmAUhXu9v9hQbOvSfpvSf/s\n7s+bWaekdyW5pH/S2EeDvy7YBrv9QItNdLd/QuE3s+mSfiVpm7uvP099vqRfufvigu0QfqDFJhr+\niXzbb5J+Kmn/+OBnXwSe8z1J+y60SQDVmci3/csk7ZS0V9Jo9vDDklZLWqKx3f7Dku7LvhxMbYt3\nfqDFSt3tLwvhB1qvtN1+ABcnwg8ERfiBoAg/EBThB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8Q\nFOEHgiL8QFCFF/As2buSfj/u/tezx+qorr3VtS+J3hpVZm9XT3TBtp7P/5UnN9vl7t2VNZBQ197q\n2pdEb42qqjd2+4GgCD8QVNXh76v4+VPq2ltd+5LorVGV9FbpZ34A1an6nR9ARSoJv5mtMLODZvaO\nma2rooc8ZnbYzPZmMw9XOsVYNg3aSTPbN+6xuWa23cyGsn/PO01aRb3VYubmxMzSlb52dZvxuu27\n/WY2VdLvJC2XdFTS65JWu/tgWxvJYWaHJXW7e+VjwmZ2q6SPJf3s3GxIZvYvkk65++PZH8457v73\nNentEV3gzM0t6i1vZum/UoWvXZkzXpehinf+myW94+6H3P2MpC2SVlXQR+25+4CkU196eJWk/ux2\nv8Z+edoup7dacPdhd9+d3f5I0rmZpSt97RJ9VaKK8F8p6ci4+0dVrym/XdKvzewNM+utupnz6Bw3\nM9JxSZ1VNnMehTM3t9OXZpauzWvXyIzXZeMLv69a5u43SvpLST/Mdm9rycc+s9VpuGaDpAUam8Zt\nWNJPqmwmm1n6OUk/cvfT42tVvnbn6auS162K8B+TdNW4+9/IHqsFdz+W/XtS0i819jGlTk6cmyQ1\n+/dkxf38gbufcPez7j4qaaMqfO2ymaWfk/Rzd38+e7jy1+58fVX1ulUR/tclLTSzb5rZDEnfl7S1\ngj6+wsw6si9iZGYdkr6j+s0+vFVST3a7R9ILFfbyR+oyc3PezNKq+LWr3YzX7t72H0krNfaN//9K\n+ocqesjp688kvZn9vF11b5Ke1dhu4Bca+27kB5Iul7RD0pCk/5I0t0a9/bvGZnN+S2NB66qot2Ua\n26V/S9Jvs5+VVb92ib4qed04wg8Iii/8gKAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8E9f92Ucyk\nQadnKAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181ff42b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myxt=np.zeros((28,28))\n",
    "myxt[:,:]=x_test[np.random.randint(1,10000),:,:,0]\n",
    "plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum=15\\nfor m in range(len(y_train)):\\n    r=np.where(y_train==num)\\n    x_train[r,:,:,0]=0\\nfor m in range(len(y_test)):\\n    r=np.where(y_test==num)\\n    x_test[r,:,:,0]=0\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the confusing data: F\n",
    "'''\n",
    "num=15\n",
    "for m in range(len(y_train)):\n",
    "    r=np.where(y_train==num)\n",
    "    x_train[r,:,:,0]=0\n",
    "for m in range(len(y_test)):\n",
    "    r=np.where(y_test==num)\n",
    "    x_test[r,:,:,0]=0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJztnXew1cX5xj8bBHtHAZGIBVGCirG3\nGOdnNxPUWDC2qKNRo0YTY4xxbMnEmDomMbGMOhYSdaJjiCXYY4koIiSARERFhSCKDayI7u+Pex/2\n3nMOcMsp37P3+cww955yz9nzsN89z777vrshxogxxpjm5wuNboAxxpjq4AHdGGMywQO6McZkggd0\nY4zJBA/oxhiTCR7QjTEmEzygG2NMJnRrQA8h7BtCeD6EMCOEcG61GtXMWJPKWJdyrEk51qR7hK4W\nFoUQegHTgb2AWcB44IgY43PVa15zYU0qY13KsSblWJPus1w3/nZ7YEaM8SWAEMItwEhgieKHEHpK\nWepTMcZ1rEk7Pu1oX7EmlekpuliTisyLMa6zrCd1J+QyEHitze1ZrfcZeKX1pzVJvNfmd+vSgjVZ\nOtYk8cqyn9I9h94hQggnASfV+n2aCWtSjjWpTKN1+cIX2nu+zz//vEEtSTRakyUhrUIIAHz22Wd1\nb0N3BvTZwKA2t9dvva8dMcargauhR02PhDVJ9Gnze5ku1sR9pQLWpJN0Z0AfDwwJIWxIi+ijgG9W\npVUNRN+u3dyFsk8IoQ8F12SjjTYC4PTTTwfg/PPPB+CDDz6oxdut0Ex9pdRtqT9U2aEWVpMVVlgB\ngEMPPbTd/TfffDPQ7etjqRRVkyUhrUaNGgXApptuCsAll1wCwMcff1y3tnR5QI8xLgohnAaMBXoB\n18UYp1atZc3NpsA0rElbXsV9pRRrUhlr0kW6nLbYpTcr4PRIDmz55ZcHYMUVVwTg/fffX/ycTz/9\ntLMvOyHGuG0H379hmmy11VYAPPPMMwDMnTsXgMsuuwyAP/zhD0DV3FihNSntB6utthqQ+oNmLe+9\n17KO2YU+UYkOa9LaxrrpMnz4cAAeeughANZYYw0ATjzxRABuuukmoDYx9Rhj6OhzG3n9qM/svPPO\nANx2220A9O3bF4AtttgCgOnTp1fj7TrUV1wpaowxmVDzLJeis9lmmwGw1157AbDlllsCyaUCvPzy\nywAsWrSozq2rDcst1/LfvsMOOwApXrzeeusBcOGFFwLJcci554zioIMGtazzS5t11mlJ/Z05cyYA\nTz75JABz5sypcwvryyeffAKk+K/6zLbbtpjE0aNHA8XIemkUQ4cOBeDSSy8FoF+/fkBjNbFDN8aY\nTOixDl2udJdddgHg4IMPBmDAgAEAXH755Y1pWA1RzO/Xv/41ACeccEK7+8Wqq64KpDhyzg5dn33w\n4MEA7LnnngDsv//+QIqHjhs3DoCPPvoIgHnz5gFVi6UXjtmzW7IFH3vsMQAOP/xwoDwvvSei2cq3\nvvUtAHbccUcAevXqBcCkSZMAePXVV+veNv/vGGNMJvRYh67shf322w+ArbfeGkjxrxydSP/+/QE4\n6qijAFhppZWA5FK1RiA3+tprr5W+RHYoq0VrKJqpfelLXwKgd+/eQOov0mr8+PEAvP3224tfK6d4\n8sKFCwF49913G9yS4qE+sOaaa7a7/frrrwNwxRVXAPXNPxf5jVrGGNND6XEOXXEu5dWqWlJxsfnz\n5wNplT8H9NnOOOMMIDkL8c477wAwduxYAC644AKgMQ6jXmh2oqymc845B0hZLXLmYpNNNmn3d1pj\nkXa5obWDbbbZBihfZ+nJSJOvf/3rQLpOzj77bAD+8pe/NKZh2KEbY0w2eEA3xphM6DEhF00Ztbi1\n8cYbAylNUelnM2bMANovBuWy2KU0RKEUvF133RWAadOmAbXdeKnRKOS2+uqrA6k8WyE4haeEtkBV\n6f9LL70EpNBcrlop5KKQpEMuqW8ceOCBQNJIY8WUKVMAFxYZY4ypAj3GocuJf+1rXwPgmGOOaff4\n7bffDsANN9wAtC+mydWFffjhhwC88MILQL6fE5LDVIn/hhtuCKR01T59+rR7nrTQ4ric+T333AOk\nTbpymb0tCTvzhPrOBhtsAMCCBQuAtAhahO0g7NCNMSYTsnfochhyZNp0SZvQq8T52WefBeDFF18E\n8narQq5Upf5ti2RyQwVE2nxL5drbbbcdkGLrQrFzOXEVEqnoKqe0VrN0dJ0cd9xxAOy9995A6gsX\nX3wxAG+99VYDWtceO3RjjMmEbB26vlW1Ev3tb38bSCXeKhz505/+BMBdd90F5LkRlbRQJodmH8ro\naXuYR27IeauYaqeddgLggAMOAFK2k2ZyionLgWuDpYkTJwJpK+WeMIOrRO5rBpXQDPaggw4C0nWk\nmHqR+oIdujHGZEK2Dn2ttdYCYPfddwdSrFQo51rbgyp+rNhpThx55JEAjBw5EkhuVGXuObouba6m\nzyhHri2DlX+uugSVb7/yyisAPPzww+1+PvroowC8+eabNW97EdDMVrnXui50XGGOfWZJaP1FTl2O\n/PnnnwdStksRsEM3xphMyM6hy30qNvqVr3wFSHnoioGOGTMGSI4sx6wFuavzzz8fSBtLyWHMmjUL\nyNNtyaHLVak/DBw4ECjPO9f/v6r9HnjgASAdOacZXI5atUV9Zp999gFg5ZVXBlJlrOo1ctcB0vqL\nalc233xzIGnx1FNPAcWa1duhG2NMJjS9Q5fDUpxLMdNTTz0VgH333RdIDuyRRx4BkkNXtWSOyIWW\n7uGivSeuueYaIE+3VVoZKg10Ww5ez9NePoqRqzJUucW5HjVXiq6fPfbYA0j6/OMf/wDyzogqZciQ\nIUDaTloHwBxyyCFAWl9xlosxxpiq0/QOXc5ch/wqZq5KQMWNtTo/depUIO+qSMX+lN0idyonce65\n5wJw4403NqB19UF1BmuvvTaQ8tB1f+meLcpyUWWoZm45zl6WhrKBhg8fDiSdcr5eStHsbbfddgNS\n35EGWnsqkjMXdujGGJMJTe/QFfOTM5crXW+99YAU+7zzzjsBeOKJJ4C8D79Vzv1FF10EJJel3eBu\nvvlmoFir89VGR8sdeuihQKpHUNZG6cHYf//73wH429/+BqQDsvV4T+GrX/0qkNYaNHP597//3agm\n1R19dmmhKMDjjz8OJIdeROzQjTEmE5rOoSs+LGd+ySWXAMmhazc9VW8pm+Wqq64C0skzRYx/VQud\nqFJaCaoMH51UlBuKfUKKAX/5y18G0glF6j/6/1+4cCEAkyZNAlJdgu7PHWl28MEHA2m/EvUZnQ8w\nevToBrSuMay77rpAmulqlqJzA4rcN+zQjTEmE5rCobfdq1pZK9rPXLsn6ltVlW7aq0WVfnKlOTvz\nfv36AXDiiScCSQtlbChvNlfaOvRlVYYqNq6slsmTJ7e7nfP6Qlt0PWlWp71t3nnnHQBuvfVWIN9Z\nXSU0O5Ezv/feewG48sorgWKvq9ihG2NMJizToYcQBgE3Av2ACFwdY7w8hLAWcCswGJgJHBZjfKea\njZPjUmUWpJVnnVSvPVrkQnXy0Mknnwykir86O67hIYT7qYEmS0OzFMWLhdzn3XffXa+mVKKummgP\nl9LKUM3QlP2kbJYG7ekzJITwAjW6fpaGnPjxxx8PwP777w8k96nrSDsK1pNGaSI22mgjIGXKKTPu\njTfeqHdTOk1HHPoi4PsxxmHAjsB3QgjDgHOBB2OMQ4AHW2+bFqZgTUqxJuUs8PVTjjXpOst06DHG\nOcCc1t8XhBCmAQOBkcBXW592A/AI8MNqNk6uSiezA+y3334A9O/fH0jOXBWgf/3rX4HkzBsY+6uJ\nJktDsxLppljgD3/Y0oQCnEped01KKa0M1alNDYqd6xDKuumi9ahRo0YBcOGFFwLpFB5VVH/3u98F\nGupK695XtJ5w+OGHA2mW10x0KoYeQhgMbA08BfRrHewBXqclJGMS1qQca9Ie7fhlXcqxJl2gw1ku\nIYRVgNuBM2OM85UtABBjjCGEiukjIYSTgJO60rjSk3Ug7aug6i3lD8uZa1e4Rme11EqTSshZyHWJ\n6dOnA/D0009X6626Ra01abvvivqFZnfDhg0DUuxclY/3338/kBx7I6hnXznmmGMA+NWvfgWk60mx\n8zPOOAOA5557rkOvp2tUDl8zne7ugVNPTYR2V1QuvmpWJkyYADTHvj4dcughhN60DOajY4x3tN49\nN4QwoPXxAUDFuVmM8eoY47Yxxm2r0eBmwZqUY03K6A3WpRLWpGssc0APLV/B1wLTYoy/afPQGODY\n1t+PBf5W/eY1NdakHGvSnrVbf1qXcqxJF+hIyGUX4GhgcghhUut95wE/B24LIZwAvAIcVq1GaRqn\nbV+1NS6ksttXX30VgB/84AcATJw4EShMaf9w4F2qqMnSUPFM6SG2c+fOBQpzvF7NNWk7JVaq5rhx\n44C0ba4OrLjnnnuAVHjWoOn0aq0pelW9fkpRiiLAKaecAqQQidB189///hdIRWlLQoVaI0aMAOCs\ns84CUiHSHXfcsfi5ndW2HppUYvvttwfSGKOwlLYPaYaQS0eyXB4HwhIe/r/qNicbpsQY92x0IwqG\nNSlnusMG5bSmLZouUOjSfznNmTNnLr5P35Ja8NOGOY1eBG0kWiBuW/oOKbVT9/eUcnZIfeaxxx4D\nYPbs2UA6Yk4H/KqwKEf0/37YYcnoylFrFlx6hOOll14KLNuNyuGr0K9v375A2p74oYceWvzcZjkc\nQxu5afahBfNmKCgSLv03xphMKKRDl8vWBkGKk0NyFCoo6kmbBnUUpeDpEOie5MyFXKEOJdAai/rL\n/PnzgcKsL9QEOfShQ4eW3VfqwOVKjz76aCCld8pxl/6dDouWjj/60Y+AtM2urt1mQLMNHVupArxm\nmVm0xQ7dGGMyoZAOXZQ69UqPGXj99dcBeOCBB4BUSJTzIdDLQg5TLkt9SP2mGTIWuouKha6//vrF\n9ykTqnS9RWgm8+KLLwKw9957AynjTI5cW8rKzWotopEFWl1Fs31tXqfiKB2S00zYoRtjTCaEejrd\nJZXyZsiEjqajVVMTrS8UdPbSEE0KToc1gerosqz8cs1c9HNJz6/lIQ8xxiWlSZdRzb6iz6rrp2Br\nTx3qK3boxhiTCYWOoZvOUVBnbgpEZ511kY9bqzY5fFY7dGOMyYR6O/R5wAetP3OgL5U/ywadeI3c\nNIHKuliT7mkC+eliTcrp1phS10VRgBDCM7nsX1Gtz5KTJlCdz2NNavs6RcCalNPdz+KQizHGZIIH\ndGOMyYRGDOhXN+A9a0W1PktOmkB1Po81qe3rFAFrUk63PkvdY+jGGGNqg0MuxhiTCXUb0EMI+4YQ\nng8hzAghnFuv960WIYRBIYSHQwjPhRCmhhC+23r/RSGE2SGESa3/9u/k6zatLtakHGtSmVroYk0q\nEGOs+T+gF/AisBHQB/g3MKwe713FzzAA+HLr76sC04FhwEXA2T1RF2tiTRqlizWp/K9eDn17YEaM\n8aUY40LgFmBknd67KsQY58QYn239fQEwDRjYzZdtal2sSTnWpDI10MWaVKBeA/pAoO3hjbPofidv\nGCGEwcDWwFOtd50WQvhPCOG6EMKanXipbHSxJuVYk8pUSRdrUgEvinaSEMIqwO3AmTHG+cCfgI2B\nEcAc4NcNbF5DsCblWJPKWJdyqqlJvQb02cCgNrfXb72vqQgh9KZF+NExxjsAYoxzY4yfxRg/B66h\nZSrYUZpeF2tSjjWpTJV1sSYVqNeAPh4YEkLYMITQBxgFjKnTe1eF0HJ6xLXAtBjjb9rcP6DN0w4C\npnTiZZtaF2tSjjWpTA10sSYVqMtuizHGRSGE04CxtKxOXxdjnFqP964iuwBHA5NDCJNa7zsPOCKE\nMAKIwEzg2x19wQx0sSblWJPKVFUXa1IZV4oaY0wmeFHUGGMywQO6McZkggd0Y4zJBA/oxhiTCR7Q\njTEmEzygG2NMJnhAN8aYTPCAbowxmeAB3RhjMsEDujHGZIIHdGOMyQQP6MYYkwke0I0xJhM8oBtj\nTCZ4QDfGmEzwgG6MMZngAd0YYzLBA7oxxmSCB3RjjMkED+jGGJMJHtCNMSYTPKAbY0wmeEA3xphM\n8IBujDGZ4AHdGGMywQO6McZkggd0Y4zJBA/oxhiTCR7QjTEmEzygG2NMJnhAN8aYTPCAbowxmeAB\n3RhjMsEDujHGZIIHdGOMyQQP6MYYkwke0I0xJhM8oBtjTCZ4QDfGmEzwgG6MMZngAd0YYzLBA7ox\nxmRCtwb0EMK+IYTnQwgzQgjnVqtRzYw1qYx1KcealGNNukeIMXbtD0PoBUwH9gJmAeOBI2KMz1Wv\nec2FNamMdSnHmpRjTbpPdxz69sCMGONLMcaFwC3AyOo0q2mxJpWxLuVYk3KsSTdZrht/OxB4rc3t\nWcAOS/uDEELXpgNNRgjhzRjjOliTtnzc5vel6mJNKtODdBHWJDGvdUxZKt0Z0DtECOEk4KRav0/B\neGVpD/ZQTd5f2oPWpDI9VJel0kM1WeqYIrozoM8GBrW5vX7rfe2IMV4NXA096ttUWJNEnza/l+nS\nLJp84QstUcoQAgCfffZZd15uqZpA8+hSI6xJJ+lODH08MCSEsGEIoQ8wChhTnWY1PX2sSRkruK+U\nYU0qYE26TpcdeoxxUQjhNGAs0Au4LsY4tWotWwJyRl3NzqkTmwLTqJMmXaXUbUrTzz//vBZv9yp1\n7iu14KCDDgJg+PDhAFxxxRWLH5s3b15nX67wmvTu3RuAL37xi0DqM6+91rJ89vHHH1f+w+7RZU3U\nl9ddd10AVlttNaDm7S0M3YqhxxjvAe6pUltyYkqMcdtGN6JgvGdNyrAmFYgxbtroNjQrNV8U7Q79\n+vUDYNCgFKrfcMMNAZgwYQIA//vf/4D8v3mridzLrrvuCsAWW2wBwOTJkwG48847gZo59aakV69e\nAJx99tkADB48GIDbb7998XO64NALi5z46quvDsBRRx0FwHLLtQwZN910EwAvvPACUJwZs/r29ddf\nD8Dmm28OwMUXXwykdndz7aOwuPTfGGMyoZAOXe5ADnLnnXde/Ji+cVdYYQUA7rvvPsAOvTPIdW21\n1VYAHHjggUDSfcyYlnUoO/SEnJ9mMx999FEjm1Nz1Bf69+8PwCmnnAKkvrPbbrsBcNJJLdmDr776\nKtD463CddVpStbfdtiWStfbaawNw5JFHAqlvv/322w1oXe2xQzfGmEwotEPfcsstAdh///0XP6a4\n+rPPPgvAuHHjAJg7d249m9jUrLTSSkByXwMGDACSuzEJ9cUDDjgAgBVXXBGAKVOmAHnFzZeG1hCU\n9aKZyo477gjAe++9BzTeoQv9v4k333wTKE77aoUdujHGZEIhHXppLmnfvn0XP7bKKqsAKTYmt2k6\nzpprrgmkHF0h3U1CazV77LEHkLIj/vnPfwI9x6Grb+indJFjLypq70YbbQSkPv/hhx82rE21xA7d\nGGMyoZAOXd/6+jZV7iukb1w9Jrdplo103HfffYGURSRNndWSkANVdocygSZOnAjA5ZdfDsCiRYsa\n0LrGoyyfZ555Bih+1ojWiVZddVUAXn/99UY2p2bYoRtjTCZ4QDfGmEwoVMhFIYGBAwcCqXhh5ZVX\nXvwcpU/pMRU0PP7440DPnQJ3Bk07FVZYuHAhAB988EHD2lQ0RowYAcD3vvc9IC2+a8sJpcHlisJw\nut6WhBaJi1L6X4ra9cQTTwAwe3bZbrxZYYdujDGZUCiHLrQouvzyywPlRQKQ3KXSGM2y6dOn5TwF\nLSRrRqSiLC0UFdVt1QPNBo899lggpc5qEfCpp54C8t3cSSgtePvttwdSQZVQ2t8nn3xS34aZpWKH\nbowxmVAoh660OaVAvfzyy0D7GLqcglz8WmutBST36Rh6OYqHSjulcMmhS7ueXKSlWeChhx4KwKhR\no9rdf/fddwNw6623AvmneK6xxhoAbLLJJkB5AZG2rV6wYAFQXD3U95Wiq/U5bfubG3boxhiTCYV0\n6PPnzwfgxRdfBGDo0KGLn1NacrzxxhsDaVtPxfRyj3F2BrlMOXDFR5XBoMwNZQr1RO2kzV577QWk\nwjXFzseOHQv0gM2dWvuKssh23313IM3mtL7y0ksvAelaLapD72nYoRtjTCYUyqGLTz/9FEjf/pXi\n4or76ng6/Xz33XeB/A8g6AyKI5bGynX/O++80+5nT0IzvuOPPx5IWzXLcSqrRTH03JEecuabbtpy\nvKf6imZv6kvKRNP1VrQMKbVHs0/noRtjjGkKCunQ9a26tLic4r+qetTh0TNmzABSrLNojqERKBNI\nOcVyU1OnTgXgoYceAmDOnDkNaF1j0WHPOvxZazE6sOGnP/0pkO9mTkKxc60dKLtFDlzXotaoVHmp\n6uKiX2eauasqOlfs0I0xJhMK6dAVr6tUIVqK4uvKiy1qLK+RyHUOGzas3W25rH/9619Az4qhb7bZ\nZgBcddVVQMrNlzO/8MILAXjyySeB/PuTZrw6hlDZY3LoqgzV0Xu33HILUDzHu6y9Z3LHDt0YYzKh\nUA5djlwx36222gpov4+E3Lsck1bd5S6VIWMSqo6T61IO/1tvvQXAG2+8AfSMKlvlUx933HEA7LTT\nTkDqe/fccw8A1113HZB/3rlQn1CNgnRS7Fz7/Tz99NNAmskUBf3/bbfddkD53jM9BTt0Y4zJhEI5\n9NIDaLUrYNu4WKlD7y76Zl9avL5Znas+05AhQwD44he/CKQV/6IfG1ZN1G+UV73nnnsCyYlqhnf/\n/fcD+R4ivCS23HJLAA477DAg1Sooi+Wcc84BYNy4cUDx9Ck9DLroh1fXCjt0Y4zJhEI5dMXDdRrM\ns88+C0D//v0XP0cVavpGlsPSvtXKFy6NCZbSt29fIOXbKl4v5z9v3rzFz/3jH/8IJEdb9H0rlJuv\nDAV9Nu2g9+Mf/xhIbqtZZyCdQX3o97//PZAcqbKifvaznwFw2223AcX/P64WijUfc8wxAIwcORJI\ns2Jdk9r5tKiZUGrnfffdB6TDvZVX31OwQzfGmEwolEMXylRRrLdt5oocdOn+JMpWUIaMfm6xxRZA\neYxcblWZH6o0VV7ta6+9tvi5N9xwA1BcdyKkyQYbbACk3GrNXuS6dA5rT4ihKxb8zW9+E4Add9yx\n3eOapfz5z38GihcbrjWqSdh6663b3S49U1TrMJrRqCK7aLM7zaxLdwzV59HPXLFDN8aYTFimQw8h\nDAJuBPoBEbg6xnh5CGEt4FZgMDATOCzGWBULKxeu3Rbb5gIrtll62s43vvENIMXQFTuX8y79Ztbf\nK9au23rPtnm2Xag+HR5CuJ8qatIR1D65Uu0cqJ96XG5s1qxZQN32KWmIJocffjgA5513HpAyqLQ+\nc/rppwMN26tlSAjhBap8/XSGfv36AWmmqutJ14H2bjnzzDOBtPvkxRdfDKRZdDWppia67g844AAA\nfvvb3wIwffr07rxsYemIQ18EfD/GOAzYEfhOCGEYcC7wYIxxCPBg623TwhSsSSnWpJwFvn7KsSZd\nZ5kOPcY4B5jT+vuCEMI0YCAwEvhq69NuAB4BfliNRikud++99wJpr3OA/fbbDyg/uV6ZHPoplhUz\nk2vVXjBjxowBYPTo0Yufo6ybTua+V1WTziBXpSwXuVK1X25MGQB1dKd100SzlFNPPRVIayZylMpc\nev7554GG7dXyVuvPuvcVXRdaa1KsXH3nwQcfBFLf10/pVYe1hi5povarfZqpK/Nr/fXXBxrv0KV3\nR3aW7QydWhQNIQwGtgaeAvq1DvYAr9MSkqn0NycBJ3W9iU2LNSnHmrRHq/3WpRxr0gU6PKCHEFYB\nbgfOjDHOb+t8Y4wxhFDR4sQYrwaubn2NTtkg7TGiWC/A+++/DySHXpr1sixK94BRVsvkyZMBGD9+\nPJDybtv+TWeolSYdQd/2S9q1Ulkw6623HlA/t1JrTVZeeeXFv//yl78EYJtttgHSXu+qhNQOk0XY\nRbGefUWzNVUNX3TRRUCa6WqP/LPOOgtIzlyzZvWtWufpd1UTjRV33XUXACeeeCIAq6yyCgC/+MUv\nADjqqKMW/43OLlZfWFLmjjRaErreNCtQrYtoe3vvvfcG0tmsWptQP21bB9MZOpTlEkLoTctgPjrG\neEfr3XNDCANaHx8AvNGlFmSKNSnHmpTRG6xLJaxJ11jmgB5avnauBabFGH/T5qExwLGtvx8L/K36\nzWtqrEk51qQ9KmO2LuVYky7QkZDLLsDRwOQQwqTW+84Dfg7cFkI4AXgFOKzajZs5cyaQtjKFdMjr\n97//fSAtfmkqKTRt0k+lPmrx85FHHgFSgYQOAVbxkBZXushw4F1qoElHKF1o0W2FXrSBkYpFHn30\n0XbPrxE100T/9yeccMLi+4444gggpZ9edtllAEyYMAEoRqgFWK01Ra8m108llGCgUv8ddtgBSDpJ\nHy2UN2I76u5oohTjG2+8EUhbGajITim7bZMetCWwFszHjh0LpO2lFULZZ599gFR8VaHd7d5D15mu\nu7abDCo8KH0VSr7yyisB+MlPfgJ0vnCrI1kujwNLClD/X6ferecwJca4Z6MbUTCsSTnTY4zbNroR\nRaM1bdF0gUKW/gu55LZl+Pr21Deuvi2HDh3a7m/lMOS49Q2oBZDHH38cSN/KWvwpLRluRrSY/PDD\nDwNpYVAHXcgd6Hazl0PLdWrWBqlfyPFce+21QM85sGJZaIYiPdT/tRhXkBlMl5k0qSWY8POf/xyA\nc89tSWnXNhhy0QAjRowA0rV/8sknt7stZ63rpprH3Om1VOCoNNKu4tJ/Y4zJhEI7dLkExcUgpdhd\ncMEFQHKXKhwQ+pvSGGC90q4aiWYljz32GJBSPFVQpFTN//znP0Dzu7HSAipIGvzud78D0kENPR3N\ndq+//nogpevK0WrGWrRNtzqLZh7XXHMNkNKRtXZwyCGHLH6u3LEc+JK23C097rIUjSlKRZw4cSLQ\nsetLr61ZdVfHJzt0Y4zJhFBPd1aLIpo2r93udoNd54SOLnbVUhOxpGP26jxbqZkmKtj4zne+s/i+\nV155BUhbHxd0FtJhTaA6fUXdI1avAAAC60lEQVTXiVypCm60GVcRZrAxxg4v6nRWEx3oodkqpP6j\nLBY59NLjLqXRLbfcAix5vU3rEMqS6UzfW4r+HeordujGGJMJ2Tj0glEoh14Qaq5J2+yDam96VCPq\n7tCbgVo69KWxrNJ+0aD1BTt0Y4zpSRQ6y8WYzpBDDYFpHM2e2QN26MYYkw31dujzgA9af+ZAXyp/\nlg068Rq5aQKVdbEm3dME8tPFmpTTrTGlrouiACGEZ3LZv6JanyUnTaA6n8ea1PZ1ioA1Kae7n8Uh\nF2OMyQQP6MYYkwmNGNCvbsB71opqfZacNIHqfB5rUtvXKQLWpJxufZa6x9CNMcbUBodcjDEmE+o2\noIcQ9g0hPB9CmBFCOLde71stQgiDQggPhxCeCyFMDSF8t/X+i0IIs0MIk1r/7d/J121aXaxJOdak\nMrXQxZpUIMZY839AL+BFYCOgD/BvYFg93ruKn2EA8OXW31cFpgPDgIuAs3uiLtbEmjRKF2tS+V+9\nHPr2wIwY40sxxoXALcDIOr13VYgxzokxPtv6+wJgGjCwmy/b1LpYk3KsSWVqoIs1qUC9BvSBwGtt\nbs+i+528YYQQBgNbA0+13nVaCOE/IYTrQghrduKlstHFmpRjTSpTJV2sSQW8KNpJQgirALcDZ8YY\n5wN/AjYGRgBzgF83sHkNwZqUY00qY13KqaYm9RrQZwOD2txev/W+piKE0JsW4UfHGO8AiDHOjTF+\nFmP8HLiGlqlgR2l6XaxJOdakMlXWxZpUoF4D+nhgSAhhwxBCH2AUMKZO710VQst5VNcC02KMv2lz\n/4A2TzsImNKJl21qXaxJOdakMjXQxZpUoC67LcYYF4UQTgPG0rI6fV2McWo93ruK7AIcDUwOIUxq\nve884IgQwgggAjOBb3f0BTPQxZqUY00qU1VdrEllXClqjDGZ4EVRY4zJBA/oxhiTCR7QjTEmEzyg\nG2NMJnhAN8aYTPCAbowxmeAB3RhjMsEDujHGZML/A/URoXIlz2DlAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1820cb1470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the respective letter\n",
    "num=21\n",
    "for m in range(10):\n",
    "    r=np.where(y_train==num)[0][m]\n",
    "    myxt[:,:]=x_train[r,:,:,0]\n",
    "    plt.subplot(2,5,m+1)\n",
    "    plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "num_classes = 47\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('Number of classes:', y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 8\n",
    "lrate = 0.05\n",
    "decay = lrate/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: 36/62 channels?\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='valid', \n",
    "                 input_shape=x_train.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 800)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                24111     \n",
      "=================================================================\n",
      "Total params: 449,167\n",
      "Trainable params: 446,479\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.6302 - acc: 0.8027 - val_loss: 0.4271 - val_acc: 0.8616\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.3775 - acc: 0.8687 - val_loss: 0.4012 - val_acc: 0.8638\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.3123 - acc: 0.8860 - val_loss: 0.4087 - val_acc: 0.8629\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2725 - acc: 0.8980 - val_loss: 0.3972 - val_acc: 0.8673\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2424 - acc: 0.9065 - val_loss: 0.4307 - val_acc: 0.8664\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2147 - acc: 0.9152 - val_loss: 0.4057 - val_acc: 0.8675\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.1971 - acc: 0.9217 - val_loss: 0.4199 - val_acc: 0.8656\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.1821 - acc: 0.9266 - val_loss: 0.4359 - val_acc: 0.8622\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed=7\n",
    "k=1\n",
    "c=4\n",
    "# Fit the model\n",
    "np.random.seed(seed)\n",
    "class_weight={0:k*1.3,1:k/1.8,2:k/1.1,3:k,4:k,5:k*1.5,6:k,7:k,8:k*2,9:k*2,10:k,11:k,12:k,13:k,14:k,15:k*2,16:k,17:k,18:k,19:k,20:k,21:k*2,\n",
    "              22:k,23:k,24:k*2,25:k,26:k,27:k,28:k,29:k,30:k,31:k,32:k,33:k,34:k,35:k*3,36:k,37:k*2,38:k,39:k,40:k*10,41:k*8,42:k,\n",
    "              43:k,44:k*9,45:k,46:k}\n",
    "hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True,\n",
    "                       #class_weight='auto'\n",
    "                      )\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")\n",
    "model = load_model(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prediction based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23515804e-07 3.72122955e-08 3.33188154e-07 5.02231833e-11\n",
      "  2.66159816e-07 7.74245723e-10 2.93974267e-06 8.25428259e-09\n",
      "  1.30503670e-08 1.94423663e-07 3.92194522e-07 9.07756856e-08\n",
      "  1.39135006e-11 5.67333132e-08 9.90347626e-09 1.37257697e-12\n",
      "  2.46301897e-06 1.12099215e-05 5.48474244e-09 7.67718422e-09\n",
      "  4.68045869e-07 3.39324941e-08 7.82226408e-08 5.11993903e-07\n",
      "  8.78020046e-10 9.56579260e-12 2.21214600e-06 4.08123579e-09\n",
      "  1.49083142e-11 3.07458663e-08 9.69479024e-01 2.14808570e-05\n",
      "  3.04293744e-02 2.45321161e-07 4.07663583e-06 3.00070724e-08\n",
      "  8.79328638e-07 2.48769254e-08 9.38984249e-06 4.17549190e-10\n",
      "  1.87652158e-10 1.93214706e-08 3.35925979e-05 6.83860790e-08\n",
      "  2.19290456e-07 5.72550236e-11 3.96266930e-09]]\n",
      "30\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "u\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfiCo/wdWrFdE5cT/PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1854d95390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myn=np.random.randint(1,20000)\n",
    "myxtr=np.array(x_train[myn,:,:,:])\n",
    "myxtrp=np.reshape(myxtr,(28,28))\n",
    "plt.imshow(myxtrp,cmap='Greys_r')\n",
    "myxtr=np.reshape(myxtr,(1,28,28,1))\n",
    "preds = model.predict(myxtr)\n",
    "print(preds)\n",
    "print(np.argmax(preds))\n",
    "print(y_train[myn])\n",
    "if np.argmax(y_train[myn])<10:\n",
    "    ascii=48+np.argmax(y_train[myn])\n",
    "else:\n",
    "    ascii=87+np.argmax(y_train[myn])\n",
    "print(chr(ascii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.862200\n"
     ]
    }
   ],
   "source": [
    "yhat=model.predict(x_test)\n",
    "yhatp=np.argmax(yhat,axis=1)\n",
    "ytsp=np.argmax(y_test,axis=1)\n",
    "acc = np.mean(yhatp == ytsp)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.843 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.739 0.002 ... 0.    0.    0.   ]\n",
      " [0.    0.    0.957 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.2   0.    0.   ]\n",
      " [0.    0.014 0.    ... 0.    0.829 0.014]\n",
      " [0.    0.    0.    ... 0.    0.    0.895]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x18304090b8>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAJCCAYAAADnUI67AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XuwrWddJ/jv7+xzkmMi4RakMSch\nQRIxoHKJkUuqpFHaaDOgpWPBKINWCsoe6ZHSVqG7xRF7qtWuUXuqmO6JHYRhHJHBtsnQmcogl7KR\nWxJANEkDxwgk4RKBhNA5ncve+5k/zgY3x3N5n5zz7vd91/58qlad9a79O8/+rfWutfazf/tZz69a\nawEAgDnaM3UCAABwLCarAADMlskqAACzZbIKAMBsmawCADBbJqsAAMyWySoAALNlsgoAwGyZrAIA\nMFt7d/KbPeTh+9ojz9k/OP5LN+4bMRtgJZwx/D0lh+4dLw9glu7NPbm/3VdT5zGW7//7Z7Yvfmlj\nR77XDR+979rW2uU78s222dHJ6iPP2Z9/9kdPHhz/5m/7eyNmw8qozvcgLYZXSj3hiYNj24dvHDET\nYI4+0N4xdQqj+uKXNvLBa8/bke+19phPnL0j3+gIJ7UMoKour6qPVdXBqnrlqUoKAACSk6isVtVa\nktcmeW6S25JcV1VXt9ZuOlXJAQBwbC3JZjanTmNUJ1NZvTTJwdbaLa21+5O8KckLTk1aAABwcmtW\nz0ly67bj25J898mlAwDAcC0bTWX1pFTVy6rq+qq6/it3PjD2twMAYIWcTGX19iTnbjs+sHXb12mt\nXZnkyiQ5/0kP8TFsAIBT5PCa1dWeXp1MZfW6JBdW1QVVdVqSFya5+tSkBQAAJ1FZba2tV9XLk1yb\nZC3J61prNjEEANhBq74bwEk1BWitXZPkmlOUCwAAfJ0d7WAFAMCp09KyseKdGXd0svqlG/d1tVB9\n1HsfNjj2b55514NJiVWw4i/Sr6p9p3XFtwfuHymT5N7/5tKu+P3/zwdHykQLVYBVp7IKALBgdgMA\nAICJmKwCADBblgEAACxUS7JhGQAAAExDZRUAYMF8wAoAACaisgoAsFAtWfmmACqrAADMlsoqAMCC\nbU6dwMhmPVntaaH6xlv/rGvsF5/7rN505qGqL37F/zSwm4zZPrXX/rddN3UK0N+CeGNjePBmRyww\nqllPVgEAOLaWZp9VAACYisoqAMBStWRjtQurKqsAAMyXyioAwEK1rP5uACqrAADMlsoqAMBiVTbS\nua3lwqisAgAwWyarAADMlmUAAAAL1ZJs2roKAACmsTKV1Ref+6yu+Gs/85HBsd//zU/uTWc8bcV/\nfWIZPA+ZgfbA/VOnALPgA1YAADCRlamsAgDsNi0qqwAAMBmVVQCABdtsKqsAADAJlVUAgIWyZhUA\nACaksgoAsFAtlY0Vrz2u9r0DAGDRVFYBABZs1XcD2LWT1Z4Wqr/76fd0jf3S8y7rTWceqvPJruXm\nyet5zD3eDLXg1/Ke/fsHx27ee++ImQBzsWsnqwAAS2c3AAAAmJDJKgAAs2UZAADAYlU22mrXHlf7\n3gEAsGgqqwAAC9WSbK547XG17x0AAIumsgoAsGC2rgIAgImorAIALFRrdgMAAIDJqKwO8NLzLuuK\n/7efes/g2J9+bN/YrJgxe7L39IefUW/43WLPmWd2xW/ec8/w4AWfz8177506BVicTWtWAQBgGiqr\nAAAL1ZJsrHjtcbXvHQAAi6ayCgCwWHYDAACAyaisAgAsVEuyueK1x9W+dwAALJrJKgAAs2UZAADA\ngm00TQEAAGASKqsj6Gmh+se3fbBr7B8+cGlvOsPNqUXjnrXhsZsb4+XRq6fFaTLuYz6n88nf0dU+\nFeAYWkpTAAAAmIrKKgDAgm1qCgAAANNQWQUAWKiWWLMKAABTUVkFAFiolrLPKgAATEVlFQBgwTZX\nvPa42vcOAIBFU1kFAFio1pIN+6wCAMA05l1Z7emzvtA+6D984NKu+D+67f1d8T9y4OnDg+fU135z\nY7yxx7TQ5yELsGdteOxSXz/Jrnjfh1Orshm7AQAAwCRMVgEAmK15LwMAAOCYWnzACgAAJqOyCgCw\nYBsrXntc7XsHAMCiqawCACxUS2Wz2boKAAAmobIKALBg1qwCAMBEVFYBABaqJdlc8X1W5z1Z7en7\nPKe+9r25dPiRA0/viv/9W/9scOyPn3dZbzrATtncmDqDnTHme/NcjP3zqmf8Of0s3A3nngdl3pNV\nAACOo7IRuwEAAMAkVFYBABZqN6xZXe17BwDAoqmsAgAsmDWrAAAwEZVVAICFaq2sWQUAgKmYrAIA\nMFsmqwAAC7bR9uzIZYiquryqPlZVB6vqlUf5+nlV9a6q+nBVfbSqfvBEY67OmtU5tWmbUfu6Hz/3\nWYNj//i2D3SN/cMHLu2KB5jUXNqQjv3zqjrqUG3ENr5z+rnMjqiqtSSvTfLcJLclua6qrm6t3bQt\n7J8neXNr7d9U1cVJrkly/vHGXZ3JKgDALtOSbM5n66pLkxxsrd2SJFX1piQvSLJ9stqSnLV1/aFJ\nPnOiQU1WAQAY4uyqun7b8ZWttSu3HZ+T5NZtx7cl+e4jxvifkvx/VfWPk5yZ5PtO9E1NVgEAFqsG\nryc9Bb7QWrvkJMd4UZLXt9b+l6p6RpI3VtWTWmubx/oPPmAFAMCpcHuSc7cdH9i6bbsrkrw5SVpr\n70uyP8nZxxvUZBUAYKFaks1WO3IZ4LokF1bVBVV1WpIXJrn6iJhPJ/neJKmqb8vhyerfHG9Qk1UA\nAE5aa209ycuTXJvk5hz+1P+NVfWaqnr+VtjPJ3lpVf15kj9I8pOtHX/rCGtWAQAWbGNGtcfW2jU5\nvB3V9tteve36TUmG76sZlVUAAGZMZRUAYKFaBq8nXSyVVQAAZktlFQBgwTZXvPa4MpPVOv30rvh2\n330jZTKyEXst//CBS7viv+NDw//s8NFLOl9ImyP2q56TPWvDY8d8THrySPpzmcv97DX248KOWrvo\nWwbHbnzs4IiZjGwu7xVzycPLcvFWZrIKALDbtJZs7PY1q1X1uqq6o6r+ctttj6iqt1fVJ7b+ffi4\naQIAsBsN+dvs65NcfsRtr0zyjtbahUnesXUMAACn1AmXAbTW/rSqzj/i5hckefbW9TckeXeSXzqF\neQEAMICtq47u0a21z25d/1ySR5+ifAAA4GtO+gNWrbVWVcf8iHpVvSzJy5Jkf8442W8HAMCWw00B\nVnvrqgd77z5fVY9Jkq1/7zhWYGvtytbaJa21S/alb3spAAB2twc7Wb06yUu2rr8kyVtPTToAAPTY\nSO3IZSpDtq76gyTvS/KtVXVbVV2R5NeTPLeqPpHk+7aOAQDglBqyG8CLjvGl7z3FuQAA0KFl9XcD\nWJkOVr3tU/eceebg2M177ulNZ7Dad1pXfHvg/q74vX9v+EYN65/7fNfYH33q8NavV3y8r3XhVRdd\n0BW/WHNpzzl2HnO5n7068+55Pfe+ljl5i26hOhdzeS3PJQ92xMpMVgEAdh+7AQAAwGRUVgEAFmxz\nwk/q7wSVVQAAZktlFQBgoVpLNlZ8NwCVVQAAZktlFQBgwewGAAAAEzFZBQBgtiwDAABYqJZa+Xar\nKqsAAMzW6lRWq++3is1Dh0ZKpNOecX8bWv/c50cdf6irLrqgK/5Xb7lhcOyvPO5pvelwpM7XT1ob\nJ4+Faw/cP3UKhzmfR9fzuIz4mOx5yEO64je/8pWRMhlZ7/Owx255zg6kKQAAAExkdSqrAAC7TEus\nWQUAgKmorAIALJimAAAAMBGVVQCApWr2WQUAgMmorAIALFSLfVYBAGAyKqsAAAu26mtWd3SyWvv2\nZe+jv3lw/Prtnxk++EJbr7X77ps6hVnqaaH6i3/1F11j/+a3fHtvOqtvoa+fJKm9472NtfX10cYe\n1YLP56hm8rgstn1qr5k83iyfyioAwELpYAUAABMyWQUAYLYsAwAAWDDLAAAAYCIqqwAAC9Wi3SoA\nAExGZRUAYMG0WwUAgImorAIALFWzGwAAAExmRyur7YEHsn77Z0YZu/ad1pnL/aPkkSR7zjxzcOzm\nPfeMlseS9ZzP3/yWb+8a+7//2K1d8f/nU58wONb5nEAN/517zNd9rzm9Z7GznHtOJe1WAQBgQtas\nAgAsmMoqAABMRGUVAGChdLACAIAJqawCACxYU1kFAIBpmKwCADBblgEAACzYZiwDAACASaisAgAs\nVGur3xRgZSar3b2Tq+PEttY19OahQ3259OjJe2ydj0vX0CP2wv4/vvXcrvgnf/iewbEfeUpvNpys\nnudKnX5639j33debzvCx1x8Ybewlq73Dfyy19fURMxlP7/vb2sUXdcVv3PTxrvhF6voZPl4a7IyV\nmawCAOxGtq4CAICJqKwCACyWdqsAADAZlVUAgAWzZhUAACaisgoAsFAtq7/PqsoqAACzpbIKALBU\nbdQePbOgsgoAwGztbGW1qqvd4ZitDkf9NaRj7L2PO79r6PVbPtmXS4feVpRjGvXcd+ppofqo9z6s\na+y/eeZdndlwMub0vBrzPWhObWW7ra0Nj11ou9Veve1T1846a/jYd9/dm848rHopsdNmrFkFAIBJ\nmKwCADBbPmAFALBQLZoCAADAZFRWAQAWqzQFAACAqaisAgAs2Krv5KWyCgDAbKmsAgAsmN0AAABg\nIiqrAAAL1drqV1Z3drLa2rx6UA+09vCHd8Vv3Hnn4Nj1Wz7Zmc14lnhu5uZvnnlXV/z/ePA/D479\nXx//hN502KXm9Fres39/V/zmvfeOlMl8jP2YbNx9d1d8j57cd8O5ZGeorAIALJh9VgEAYCIqqwAA\nC2afVQAAmIjKKgDAgq36bgAqqwAAzJbJKgAAs2UZAADAQrWUZQAAADAVlVUAgAVb8Z2r5j1Z/at/\n9YzBsd/yC+/rG7yGl8x72qdCj54Wql/6qeGvhyR5xO91viY6rD/naV3xe995w0iZMHdabv5dS35M\nlpw7yzXrySoAAMfRbF0FAACTUVkFAFiyFV+0qrIKAMApUVWXV9XHqupgVb3yGDE/VlU3VdWNVfV/\nnWhMlVUAgAWby5rVqlpL8tokz01yW5Lrqurq1tpN22IuTPKqJM9qrd1ZVd90onFVVgEAOBUuTXKw\ntXZLa+3+JG9K8oIjYl6a5LWttTuTpLV2x4kGNVkFAFiw1nbmMsA5SW7ddnzb1m3bXZTkoqr6s6p6\nf1VdfqJBLQMAAGCIs6vq+m3HV7bWruwcY2+SC5M8O8mBJH9aVd/eWrvreP8BAIAFatnRNatfaK1d\ncpyv357k3G3HB7Zu2+62JB9orT2Q5K+r6uM5PHm97liDWgYAAMCpcF2SC6vqgqo6LckLk1x9RMx/\nyOGqaqrq7BxeFnDL8QZVWQUAWKqWZCa7AbTW1qvq5UmuTbKW5HWttRur6jVJrm+tXb31tX9QVTcl\n2UjyC621Lx5v3FlPVi/6tRsHx270Dj5wpXCS1N6+h2nPGWcMjt24++6+sffv74rXx/nk9Zz/tr4+\nWh7f9PZPd8VfeMPwvG9+Wl/e+/70z7viV3y/agC2tNauSXLNEbe9etv1luTnti6DWAYAAMBszbqy\nCgDA8XX8sXiRVFYBAJgtlVUAgCVTWQUAgGmorAIALFbtZFOASaisAgAwWyqrAABLZs0qAABMQ2UV\nAGCpWlZ+zeqsJ6s9rUh7W6L2tMXsbaHZ20K1h/apO2/MFqo9rXnXb7u9a+ybnzY89tf++rqusX/5\ngu/qimd19L7X1mmndcVvHjrUFQ+svllPVgEAOAFrVgEAYBonnKxW1blV9a6quqmqbqyqn926/RFV\n9faq+sTWvw8fP10AAL5e7dBlGkMqq+tJfr61dnGSpyf5maq6OMkrk7yjtXZhkndsHQMAwClzwslq\na+2zrbUPbV3/SpKbk5yT5AVJ3rAV9oYkPzRWkgAAHEPboctEuj5gVVXnJ3lKkg8keXRr7bNbX/pc\nkkcf4/+8LMnLkmR/hn/yGQAABn/Aqqq+MckfJXlFa+3r9mZqrR1zzt1au7K1dklr7ZJ9Of2kkgUA\nYHcZNFmtqn05PFH9/dbav9+6+fNV9Zitrz8myR3jpAgAwDGt+DKAIbsBVJKrktzcWvutbV+6OslL\ntq6/JMlbT316AADsZkPWrD4ryYuT/EVVfWTrtn+a5NeTvLmqrkjyqSQ/Nk6KAAAcVUuy29utttbe\nk2NvrvW9pzYdAAD4W7Nut/qZX3zm4Nhv/s33jpjJePbs398Vv3nvvSNlwhS6+qDvWescfGNw6C9f\n8F1dQ3/8957WFX/RT93QFc98tfX1UeOBfk27VQAAmMasK6sAAJyAyioAAExDZRUAYMlWfDcAlVUA\nAGZLZRUAYMHKmlUAAJiGyioAwFK12A0AAACmorIKALBYZTcAAACYyqwrq9/8m++dOoXRbd5779Qp\nPHjV8ZvcqjcufrB6HsPNjfHy6HTRT93QFf/GW/9scOyLz31Wbzocqed5lXh9Mtie/fsHxy765xuz\nMuvJKgAAJ7Div29aBgAAwGyprAIALJnKKgAATENlFQBgyVRWAQBgGiqrAABL1aIpAAAATEVlFQBg\nwcqaVQAAmMbqVFa1F9x5PY+h87Orvfi8ywbH/s4nh7dmTZJXnP/M3nRWn9cPY9m3b3isdqs7Z8Vf\n8iqrAADMlskqAACzZbIKAMBsrc6aVQCAXchuAAAAMBGVVQCAJdPBCgAApmGyCgDAbFkGAACwVC2a\nAgAAwFRUVgEAlmzFK6s7Olm979wz84lf+u7B8Rf+4w8MH1wv7Hlzfo5utzwuHffzFec/s2voW379\nGYNjH/fK93WNDaPYs9YXv7kxTh4PwuZXvjJ1CuxCKqsAAAumKQAAAExEZRUAYMlUVgEAYBoqqwAA\nS6ayCgAA01BZBQBYqGp2AwAAgMmorAIALFmrqTMYlcoqAACztaOV1dNvvaevhSonrfYOP8Vtfb1z\n8I7f5EZsK9pzH5MHcT+Ztce96v2DY6/69Hu6xr7ivMt604ETa5tTZzBPM/mZskgr/nCorAIAMFsm\nqwAAzJYPWAEALJitqwAAYCIqqwAAS6ayCgAA01BZBQBYKu1WAQBgOiqrAABLprIKAADTUFkFAFiy\nFa+s7uhktfbsyZ5vOGNw/OahQ4Nj95wxfNzesZesra8Pjl07+5FdY2984Yu96Yyi5z6OzfNwAh09\nwq8477KuoZ9/U99z/OqL+15D7Kye1+eor0197Y/O48IxqKwCACyY3QAAAGAiJqsAAMyWySoAALNl\nzSoAwJJZswoAANMwWQUAYLYsAwAAWKpm6yoAAJiMyioAwJKteGV1RyerbXNztBZ22lYeXe07bXBs\nb/vUnrHbA/d3jb1US30e7jnzzK74zXvuGSmTcfU8Z5P+9qlXfPyvB8deddEFXWNz8toD82jNvFte\nb3CqqKwCACzZildWrVkFAGC2VFYBABaqYjcAAACYjMoqAMCSqawCAMA0VFYBAJZKBysAAJiOyioA\nwJKprAIAwIlV1eVV9bGqOlhVrzxO3I9UVauqS040pskqAMCStR26nEBVrSV5bZIfSHJxkhdV1cVH\niXtIkp9N8oEhd2/3LgOoGh7b+urre570hMGxmzd+rGvs3lzaA/f3jT+TsTl5tXf4y3u39B4f+zl7\n1UUXDI593o13do39tic+vDcdjjCX96zd8npjV7o0ycHW2i1JUlVvSvKCJDcdEfdrSX4jyS8MGVRl\nFQCAIc6uquu3XV52xNfPSXLrtuPbtm77mqp6apJzW2v/ceg33b2VVQCAFbCDW1d9obV2wjWmx1JV\ne5L8VpKf7Pl/KqsAAJwKtyc5d9vxga3bvuohSZ6U5N1V9ckkT09y9Yk+ZKWyCgCwZPPZuuq6JBdW\n1QU5PEl9YZL/7qtfbK19OcnZXz2uqncn+SetteuPN6jKKgAAJ621tp7k5UmuTXJzkje31m6sqtdU\n1fMf7LgqqwAASzVwW6md0lq7Jsk1R9z26mPEPnvImCqrAADMlsoqAMCC7eBuAJNQWQUAYLZUVgEA\nlkxlFQAAprF7K6ttvF9DNv/yP4829tpZZ3XFb9x990iZMHdtfX3qFDiOtz3x4V3xv3rLDYNjf+Vx\nT+tLpmp47IjvnTDU2sMeOji27l4bMZN5sGYVAAAmsnsrqwAAq0BlFQAApqGyCgCwVDPrYDUGlVUA\nAGbLZBUAgNmyDAAAYKFq67LKVFYBAJgtlVUAgCXzASsAAJjGzldWx2rr1zNu79hj6sxb+9QVo80l\nA/W0UL3q0+/pGvulT/gHg2M3Dx3qGntWvN5WxsZdXx4c29rGiJnMg3arAAAwkRNOVqtqf1V9sKr+\nvKpurKpf3br9gqr6QFUdrKo/rKrTxk8XAICv03boMpEhldX7kjyntfadSZ6c5PKqenqS30jy2621\nxye5M8kV46UJAMBudMLJajvsv2wd7tu6tCTPSfKWrdvfkOSHRskQAIBjU1lNqmqtqj6S5I4kb0/y\nV0nuaq2tb4XcluSccVIEAGC3GrQbQDv8UbonV9XDkvxxkicM/QZV9bIkL0uS/TnjweQIAMDRNLsB\nfJ3W2l1J3pXkGUkeVlVfneweSHL7Mf7Pla21S1prl+zL6SeVLAAAu8uQ3QAetVVRTVV9Q5LnJrk5\nhyetP7oV9pIkbx0rSQAAjmHF16wOWQbwmCRvqKq1HJ7cvrm19raquinJm6rqXyT5cJKrRswTAIBd\n6IST1dbaR5M85Si335Lk0jGSAgBgGGtWAQBgIoN2Azilevot71kbHFr7+u5Ku+++rvjRjN1/uuMx\nzObq90+eHf3HGcFLv+37u+L/+V/+p8Gxr3ncU3vT6VKnD/8gbvf7uNcbLNLOT1YBADh1Vvz3MMsA\nAACYLZVVAIAF8wErAACYiMoqAMBSTbxh/05QWQUAYLZUVgEAlkxlFQAApqGyCgCwUBW7AQAAwGTm\nXVntaP/Z7tMq9Ki0UGWInra8Sf/zqnf8HmM+x8d+XEbS7n+gK76nheq//dR7usb+6cde1hU/m1bY\nCz337FIqqwAAMI15V1YBADiuaqtdWlVZBQBgtlRWAQCWSgcrAACYjskqAACzZRkAAMCCaQoAAAAT\nUVkFAFgylVUAAJiGyioAwIKt+prVlZms1t6+u9LW10fKBJZnz2n7uuI37+3sg77UvukLzbs9cH9X\nfM/7508/9rKusV99y4e64v/Fd37P4Nj2X/9r19hd7/sLPfewilZmsgoAsCuteGXVmlUAAGZLZRUA\nYKna6q9ZVVkFAGC2VFYBAJZMZRUAAKahsgoAsFAVa1YBAGAyKqsAAEvWVru0qrIKAMBsmawCADBb\nK7MMoKvnc6e1Rz6iK37ji18aKZMke9bGG1sv7F1r8957+/5D7/PQc2vW6vTTB8f2vte+5nFP7Yr/\nmU/cMDj2tRde1DU2K6TnPWgXvP34gBUAAExkZSqrAAC7ToumAAAAMBWVVQCABavNqTMYl8oqAACz\npbIKALBk1qwCAMA0VFYBABbMPqsAADARlVUAgKVqSdpql1ZnPVmtvcPT++KLv6tr7Ef83vsGx47a\nPrVXZ9vKnsewrfjWF5xC2qeulM177pk6ha/paaH65Wse3zX2Q3/wYG86zJX3oF1l1pNVAACOz5pV\nAACYiMoqAMCSqawCAMA0TFYBAJgtywAAABaq4gNWAAAwGZVVAIClam3lmwKorAIAMFsqqwAAC2bN\nKgAATGTWldW2vj449hGvf/+ImSxXz2NYe8d7OvTkATDEQ3/wYFf8m2973+DYHzvwjN50YDoqqwAA\nMI1ZV1YBADg+a1YBAGAiKqsAAEvVkmyudmlVZRUAgNlSWQUAWLLVLqyqrAIAMF8qqwAAC2Y3AAAA\nmIjJKgAAs7WzywCqr6VnV4vOtswa+N/8o76Wfo/6N8PbBSYjPt6delu5as/KUHN5jjN/PS1UP/GG\np3aNfeFLPtSbzixos70iFjoHGkplFQCA2fIBKwCABfMBKwAAGKCqLq+qj1XVwap65VG+/nNVdVNV\nfbSq3lFVjz3RmCarAABL1XbwcgJVtZbktUl+IMnFSV5UVRcfEfbhJJe01r4jyVuS/OaJxjVZBQDg\nVLg0ycHW2i2ttfuTvCnJC7YHtNbe1Vo7tHX4/iQHTjSoNasAAAtVSWrndgM4u6qu33Z8ZWvtym3H\n5yS5ddvxbUm++zjjXZHk/z3RNzVZBQBgiC+01i45FQNV1U8kuSTJ95wo1mQVAGDJNqdO4GtuT3Lu\ntuMDW7d9nar6viT/LMn3tNbuO9Gg1qwCAHAqXJfkwqq6oKpOS/LCJFdvD6iqpyT535M8v7V2x5BB\nVVYBABZsB9esHldrbb2qXp7k2iRrSV7XWruxql6T5PrW2tVJ/lWSb0zyf1dVkny6tfb8441rsgoA\nwCnRWrsmyTVH3Pbqbde/r3fMnZ2stvF6BS+19/w3/e51XfG9vzvN5X7OJY9d5fBvrMPM5LfyB6Nt\nbEydAgtRp58+OPbCl3yoa+z/7VPvGRz7Pzz2sq6xx+S9eQUM3AN1yaxZBQBgtiwDAABYrLbov44N\nobIKAMBsqawCACxYrXZhVWUVAID5MlkFAGC2LAMAAFgyH7ACAIBpqKwCACxVS2pz6iTGpbIKAMBs\nrUxldakt45aa99j2nHHG4NjNQ4dGzGTBVnwN09fslvvJSWv33z/a2D0tVF//6eGtWZPkJ8+bT3tW\nZmrF3wdVVgEAmK2VqawCAOxKq11YVVkFAGC+VFYBABasrFkFAIBpqKwCACyZyioAAExDZRUAYKla\nEh2sAABgGiqrAAALVWl2AwAAgKmorA5Qp5/eFd/uu2+kTOaVy5g2Dx2aOoWvOfjGpwyOffyLPzxi\nJpysOb1+5pTLrjGT6tNPnndZV/xXXvj0rviHvOn9XfFLtPawhw6OrbvXRsyEnWCyCgCwZDP5RWws\ng5cBVNVaVX24qt62dXxBVX2gqg5W1R9W1WnjpQkAwG7Us2b1Z5PcvO34N5L8dmvt8UnuTHLFqUwM\nAIABWtuZy0QGTVar6kCSf5jk320dV5LnJHnLVsgbkvzQGAkCALB7DV2z+jtJfjHJQ7aOH5nkrtba\n+tbxbUnOOcW5AQBwPJoCJFXEYOUGAAAJE0lEQVT1vCR3tNZueDDfoKpeVlXXV9X1D8SnWQEAGG5I\nZfVZSZ5fVT+YZH+Ss5L86yQPq6q9W9XVA0luP9p/bq1dmeTKJDmrHrHaH1cDANhhu74pQGvtVa21\nA62185O8MMk7W2s/nuRdSX50K+wlSd46WpYAAOxKJ9PB6peS/FxVHczhNaxXnZqUAAAYbMV3A+hq\nCtBae3eSd29dvyXJpac+JQAAOEwHKwCAxZq26rkTdu1ktbcv91z09gffe+6BwbGf/InzusY+8C/f\n2xW/VI9/8YcHx+r3Pm9jP94959+5Xy09535P5/vEQ970/q74L17xjMGxj7zqfV1j91g766yu+I27\n7x4ee9eXB8e2ttGVB/OzayerAACL17LyldWT+YAVAACMSmUVAGDJdnsHKwAAmIrJKgAAs2UZAADA\ngu36dqsAADAVlVUAgCVTWQUAgGmorAIALFVLsrnaldWVmazW3r67Mmqrw6rxxu4s9a/fetvg2AP/\ncngsR7dbWmjuOeOMrvjNQ4dGymRedsv55+/qOfcbIz9Pelqo/tpfX9c19q886dmDY3vap8LxrMxk\nFQBg92nWrAIAwFRUVgEAlkxlFQAApqGyCgCwZCqrAAAwDZVVAICl2gX7rKqsAgAwWyqrAACL1ZK2\nOXUSo1JZBQBgtkxWAQCYrZVZBtDW16dO4W+t+BYSs1PVF+/8nLTNQ4f6/sOetcGhtTY8NknaA/f3\n5bIbdDzeSZLNjXHyYPZ++XGXdsX/7qeuHRz70vMu60um573c+/jXW/HHQ2UVAIDZWpnKKgDArmPr\nKgAAmI7KKgDAklmzCgAA01BZBQBYMpVVAACYhsoqAMBiNZVVAACYisoqAMBStSSbm1NnMSqT1QH2\n7N/fFb95770jZdJv7YnfOjj25lec1TX2RS+9rjedcaz4nz9WQkc7z6b158nzGB5Vz3v5nN7He+09\n55sHx67f/pmusXtaqK4/52ldY+995w1d8eweJqsAAEu24kUba1YBAJgtlVUAgCVTWQUAgGmYrAIA\nMFuWAQAALFZLNi0DAACASaisAgAsVUtaW+2mACqrAADMlsoqAMCSWbMKAADTmHVltfYOT6+tr4+W\nx6x6RFd1hW/c+LHBsRe9tDeZhep8DFd9s+XdpOc9JXkQ7yu9z60Oe04/fXDs5n33dY299shHDI7d\n+MIXu8buNeb7fu/jslTrt39mvME7nuN733lD19DPu/HOwbH/8TsfNXzg8aYH87HiP6dUVgEAmK1Z\nV1YBADiO1pJNuwEAAMAkVFYBAJbMmlUAAJiGyioAwII1a1YBAGAaKqsAAIvVrFkFAICpmKwCADBb\ns14GMGYL1R6jt2gcU0/7xxX/M8LX9N7PpT6GS817RKO/Nkd8HOuhZw0P/vwdXWOP3UK1R9vYGHHw\njvMzYuvcRb/eRsz9bU98+ODYV/3VhwbH/qPnH3ow6SxHS7K54OfUACqrAADM1qwrqwAAnECzdRUA\nAExCZRUAYKFakmbNKgAATENlFQBgqVqzZhUAAKZisgoAsGBts+3IZYiquryqPlZVB6vqlUf5+ulV\n9YdbX/9AVZ1/ojFNVgEAOGlVtZbktUl+IMnFSV5UVRcfEXZFkjtba49P8ttJfuNE45qsAgAsWdvc\nmcuJXZrkYGvtltba/UnelOQFR8S8IMkbtq6/Jcn3Vh2/ZZzJKgAAp8I5SW7ddnzb1m1HjWmtrSf5\ncpJHHm/QHd0N4Cu58wt/0t7yqaN86ewkX9jJXLo8MHUC28x/K7V5n8sHY/6P+dGdmrxX73wu1edO\nySjzP59zeb3NJY9jm/+5HNGfPK4r/LEjpTELX8md1/5Je8vZO/Tt9lfV9duOr2ytXTn2N93RyWpr\n7VFHu72qrm+tXbKTuTAO53K1OJ+rxflcHc4lX9Vau3zqHLa5Pcm5244PbN12tJjbqmpvkocm+eLx\nBrUMAACAU+G6JBdW1QVVdVqSFya5+oiYq5O8ZOv6jyZ5Z2vtuH/L0BQAAICT1lpbr6qXJ7k2yVqS\n17XWbqyq1yS5vrV2dZKrkryxqg4m+VIOT2iPay6T1dHXO7BjnMvV4nyuFudzdTiXzFJr7Zok1xxx\n26u3Xb83yX/bM2adoPIKAACTsWYVAIDZmnSyeqKWXMxbVb2uqu6oqr/cdtsjqurtVfWJrX8fPmWO\nDFNV51bVu6rqpqq6sap+dut253OBqmp/VX2wqv5863z+6tbtF2y1Nzy41e7wtKlzZZiqWquqD1fV\n27aOnUt2jckmqwNbcjFvr09y5JYZr0zyjtbahUnesXXM/K0n+fnW2sVJnp7kZ7Zej87nMt2X5Dmt\nte9M8uQkl1fV03O4reFvb7U5vDOH2x6yDD+b5OZtx84lu8aUldUhLbmYsdban+bwJ/m2295G7Q1J\nfmhHk+JBaa19trX2oa3rX8nhH4rnxPlcpHbYf9k63Ld1aUmek8PtDRPnczGq6kCSf5jk320dV5xL\ndpEpJ6tDWnKxPI9urX126/rnkjx6ymToV1XnJ3lKkg/E+VysrT8bfyTJHUnenuSvkty11d4w8Z67\nJL+T5BeTfLU5+yPjXLKL+IAVo9na5Nd2EwtSVd+Y5I+SvKK1dvf2rzmfy9Ja22itPTmHO8hcmuQJ\nE6fEg1BVz0tyR2vthqlzgalMuc/qkJZcLM/nq+oxrbXPVtVjcriqwwJU1b4cnqj+fmvt32/d7Hwu\nXGvtrqp6V5JnJHlYVe3dqsh5z12GZyV5flX9YJL9Sc5K8q/jXLKLTFlZHdKSi+XZ3kbtJUneOmEu\nDLS1Bu6qJDe31n5r25eczwWqqkdV1cO2rn9Dkufm8Drkd+Vwe8PE+VyE1tqrWmsHWmvn5/DPyXe2\n1n48ziW7yKRNAbZ+U/yd/G1Lrv95smToVlV/kOTZSc5O8vkkv5LkPyR5c5LzknwqyY+11o78EBYz\nU1WXJflPSf4if7su7p/m8LpV53Nhquo7cvhDN2s5XJR4c2vtNVX1uBz+MOsjknw4yU+01u6bLlN6\nVNWzk/yT1trznEt2Ex2sAACYLR+wAgBgtkxWAQCYLZNVAABmy2QVAIDZMlkFAGC2TFYBAJgtk1UA\nAGbLZBUAgNn6/wFVHMd/Ug+EkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1824c50da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "C = confusion_matrix(ytsp,yhatp)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "#Csum = np.sum(C,1)\n",
    "#C = C / Csum[None,:]\n",
    "C = normalize(C, norm='l1', axis=1)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(np.array_str(C, precision=3, suppress_small=True))\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(C, interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84325397 0.73854962 0.95687885 0.99441341 0.94639175 0.94305239\n",
      " 0.97797357 0.97194389 0.96082474 0.975      0.78409091 0.86440678\n",
      " 0.90769231 0.89230769 0.85483871 0.78787879 0.74       0.87804878\n",
      " 0.31279621 0.77333333 0.90909091 0.56610169 0.95679012 0.93023256\n",
      " 0.47150259 0.95092025 0.83333333 0.86440678 0.82857143 0.90849673\n",
      " 0.94977169 0.90322581 0.94117647 0.9047619  0.875      0.66233766\n",
      " 0.96969697 0.57142857 0.93835616 0.95396419 0.27906977 0.31914894\n",
      " 0.9109589  0.94565217 0.2        0.82938389 0.89495798]\n",
      "(array([ 1, 10, 15, 16, 18, 19, 21, 24, 35, 37, 40, 41, 44]),)\n"
     ]
    }
   ],
   "source": [
    "Cd=C.diagonal()\n",
    "print(Cd)\n",
    "print(np.where(Cd<0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a loop to find the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=16,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 29s 640us/step - loss: 0.7548 - acc: 0.7863 - val_loss: 0.4637 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 28s 614us/step - loss: 0.3944 - acc: 0.8632 - val_loss: 0.3924 - val_acc: 0.8644\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 28s 606us/step - loss: 0.3256 - acc: 0.8814 - val_loss: 0.3784 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 29s 625us/step - loss: 0.2899 - acc: 0.8925 - val_loss: 0.3865 - val_acc: 0.8741\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 28s 608us/step - loss: 0.2595 - acc: 0.9013 - val_loss: 0.3888 - val_acc: 0.8737\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 28s 611us/step - loss: 0.2366 - acc: 0.9104 - val_loss: 0.3883 - val_acc: 0.8748\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 28s 612us/step - loss: 0.2188 - acc: 0.9144 - val_loss: 0.3990 - val_acc: 0.8720\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 28s 619us/step - loss: 0.2039 - acc: 0.9202 - val_loss: 0.3967 - val_acc: 0.8731\n",
      "nc=16,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 33s 718us/step - loss: 0.9367 - acc: 0.7661 - val_loss: 0.4698 - val_acc: 0.8445\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 32s 695us/step - loss: 0.4140 - acc: 0.8562 - val_loss: 0.4129 - val_acc: 0.8624\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 732us/step - loss: 0.3476 - acc: 0.8755 - val_loss: 0.4079 - val_acc: 0.8610\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 32s 705us/step - loss: 0.3040 - acc: 0.8877 - val_loss: 0.3806 - val_acc: 0.8751\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2752 - acc: 0.8972 - val_loss: 0.3831 - val_acc: 0.8743\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 32s 697us/step - loss: 0.2509 - acc: 0.9032 - val_loss: 0.3857 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 32s 698us/step - loss: 0.2324 - acc: 0.9101 - val_loss: 0.3840 - val_acc: 0.8749\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2162 - acc: 0.9161 - val_loss: 0.3982 - val_acc: 0.8698\n",
      "nc=16,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 1.1055 - acc: 0.7681 - val_loss: 0.4701 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 34s 735us/step - loss: 0.4161 - acc: 0.8578 - val_loss: 0.4386 - val_acc: 0.8524\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 736us/step - loss: 0.3495 - acc: 0.8747 - val_loss: 0.3977 - val_acc: 0.8678\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 34s 742us/step - loss: 0.3093 - acc: 0.8872 - val_loss: 0.3826 - val_acc: 0.8720\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2801 - acc: 0.8947 - val_loss: 0.3785 - val_acc: 0.8744\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 36s 785us/step - loss: 0.2580 - acc: 0.9020 - val_loss: 0.3776 - val_acc: 0.8766\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 0.2370 - acc: 0.9081 - val_loss: 0.3822 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2226 - acc: 0.9134 - val_loss: 0.3790 - val_acc: 0.8799\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7467 - acc: 0.7870 - val_loss: 0.4430 - val_acc: 0.8529\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.3786 - acc: 0.8685 - val_loss: 0.4073 - val_acc: 0.8492\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3143 - acc: 0.8846 - val_loss: 0.3723 - val_acc: 0.8758\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2737 - acc: 0.8976 - val_loss: 0.3672 - val_acc: 0.8773\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2433 - acc: 0.9068 - val_loss: 0.3808 - val_acc: 0.8786\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2220 - acc: 0.9125 - val_loss: 0.3756 - val_acc: 0.8825\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2002 - acc: 0.9217 - val_loss: 0.3798 - val_acc: 0.8818\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.1863 - acc: 0.9258 - val_loss: 0.3988 - val_acc: 0.8786\n",
      "nc=32,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8393 - acc: 0.7738 - val_loss: 0.4995 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3981 - acc: 0.8619 - val_loss: 0.4007 - val_acc: 0.8703\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3368 - acc: 0.8789 - val_loss: 0.4001 - val_acc: 0.8655\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.2970 - acc: 0.8907 - val_loss: 0.3835 - val_acc: 0.8703\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2661 - acc: 0.8997 - val_loss: 0.4000 - val_acc: 0.8728\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2428 - acc: 0.9067 - val_loss: 0.3850 - val_acc: 0.8765\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2253 - acc: 0.9131 - val_loss: 0.3970 - val_acc: 0.8721\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2074 - acc: 0.9179 - val_loss: 0.3977 - val_acc: 0.8760\n",
      "nc=32,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 1.0933 - acc: 0.7525 - val_loss: 0.5016 - val_acc: 0.8304\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.4315 - acc: 0.8511 - val_loss: 0.4396 - val_acc: 0.8518\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.3682 - acc: 0.8710 - val_loss: 0.3942 - val_acc: 0.8636\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.3274 - acc: 0.8834 - val_loss: 0.4291 - val_acc: 0.8579\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 1ms/step - loss: 0.3016 - acc: 0.8899 - val_loss: 0.4213 - val_acc: 0.8612\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2770 - acc: 0.8971 - val_loss: 0.3989 - val_acc: 0.8724\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2579 - acc: 0.9027 - val_loss: 0.3909 - val_acc: 0.8725\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2439 - acc: 0.9063 - val_loss: 0.4026 - val_acc: 0.8754\n",
      "nc=48,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.7368 - acc: 0.7928 - val_loss: 0.4215 - val_acc: 0.8560\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.3713 - acc: 0.8690 - val_loss: 0.3846 - val_acc: 0.8729\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.3077 - acc: 0.8882 - val_loss: 0.3720 - val_acc: 0.8741\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.2687 - acc: 0.8981 - val_loss: 0.3581 - val_acc: 0.8798\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 89s 2ms/step - loss: 0.2377 - acc: 0.9083 - val_loss: 0.3744 - val_acc: 0.8811\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.2139 - acc: 0.9153 - val_loss: 0.3856 - val_acc: 0.8696\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1950 - acc: 0.9222 - val_loss: 0.3846 - val_acc: 0.8770\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1801 - acc: 0.9273 - val_loss: 0.4065 - val_acc: 0.8778\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.8842 - acc: 0.7788 - val_loss: 0.4717 - val_acc: 0.8389\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3909 - acc: 0.8625 - val_loss: 0.3847 - val_acc: 0.8690\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3213 - acc: 0.8834 - val_loss: 0.3819 - val_acc: 0.8684\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2832 - acc: 0.8954 - val_loss: 0.3800 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2515 - acc: 0.9045 - val_loss: 0.3748 - val_acc: 0.8773\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2298 - acc: 0.9128 - val_loss: 0.3798 - val_acc: 0.8769\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2072 - acc: 0.9190 - val_loss: 0.3862 - val_acc: 0.8799\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.1919 - acc: 0.9245 - val_loss: 0.3916 - val_acc: 0.8775\n",
      "nc=48,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 1.1306 - acc: 0.7643 - val_loss: 0.4796 - val_acc: 0.8355\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.4054 - acc: 0.8600 - val_loss: 0.4278 - val_acc: 0.8609\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.3385 - acc: 0.8786 - val_loss: 0.3986 - val_acc: 0.8627\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2957 - acc: 0.8904 - val_loss: 0.3771 - val_acc: 0.8767\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2683 - acc: 0.8990 - val_loss: 0.3837 - val_acc: 0.8752\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.2419 - acc: 0.9078 - val_loss: 0.3949 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.2225 - acc: 0.9138 - val_loss: 0.4025 - val_acc: 0.8731\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2052 - acc: 0.9203 - val_loss: 0.3930 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([384,512,640])\n",
    "nconvs=np.array([16,32,48])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8748 0.8751 0.8799]\n",
      " [0.8825 0.8765 0.8754]\n",
      " [0.8811 0.8799 0.8785]]\n",
      "(3, 3)\n",
      "the best nconvs is 32,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8825\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go further from above to find better nconvs and n node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=24,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.6819 - acc: 0.7966 - val_loss: 0.4265 - val_acc: 0.8564\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 978us/step - loss: 0.3733 - acc: 0.8688 - val_loss: 0.3767 - val_acc: 0.8716\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 45s 981us/step - loss: 0.3129 - acc: 0.8860 - val_loss: 0.3692 - val_acc: 0.8732\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 46s 996us/step - loss: 0.2722 - acc: 0.8973 - val_loss: 0.3689 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2435 - acc: 0.9066 - val_loss: 0.3786 - val_acc: 0.8713\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2218 - acc: 0.9136 - val_loss: 0.3775 - val_acc: 0.8767\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2036 - acc: 0.9215 - val_loss: 0.3817 - val_acc: 0.8789\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.1874 - acc: 0.9259 - val_loss: 0.3933 - val_acc: 0.8784\n",
      "nc=24,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.7690 - acc: 0.7808 - val_loss: 0.5000 - val_acc: 0.8369\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 983us/step - loss: 0.3935 - acc: 0.8620 - val_loss: 0.4044 - val_acc: 0.8660\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 46s 991us/step - loss: 0.3333 - acc: 0.8807 - val_loss: 0.3885 - val_acc: 0.8681\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.2929 - acc: 0.8925 - val_loss: 0.3682 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.2651 - acc: 0.9006 - val_loss: 0.3882 - val_acc: 0.8704\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 46s 994us/step - loss: 0.2411 - acc: 0.9090 - val_loss: 0.3714 - val_acc: 0.8795\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 46s 999us/step - loss: 0.2201 - acc: 0.9150 - val_loss: 0.3832 - val_acc: 0.8778\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 46s 1000us/step - loss: 0.2071 - acc: 0.9185 - val_loss: 0.3901 - val_acc: 0.8772\n",
      "nc=24,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.8109 - acc: 0.7831 - val_loss: 0.4671 - val_acc: 0.8441\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3964 - acc: 0.8613 - val_loss: 0.3873 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3324 - acc: 0.8814 - val_loss: 0.3888 - val_acc: 0.8712\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2894 - acc: 0.8936 - val_loss: 0.3917 - val_acc: 0.8714\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2648 - acc: 0.8996 - val_loss: 0.3812 - val_acc: 0.8801\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2417 - acc: 0.9075 - val_loss: 0.3808 - val_acc: 0.8787\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2232 - acc: 0.9136 - val_loss: 0.3913 - val_acc: 0.8702\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2078 - acc: 0.9188 - val_loss: 0.4019 - val_acc: 0.8776\n",
      "nc=32,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7116 - acc: 0.7918 - val_loss: 0.4572 - val_acc: 0.8496\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3812 - acc: 0.8660 - val_loss: 0.3714 - val_acc: 0.8718\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3205 - acc: 0.8832 - val_loss: 0.3627 - val_acc: 0.8745\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2816 - acc: 0.8960 - val_loss: 0.3585 - val_acc: 0.8771\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2538 - acc: 0.9049 - val_loss: 0.3656 - val_acc: 0.8797\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2294 - acc: 0.9112 - val_loss: 0.3647 - val_acc: 0.8806\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2098 - acc: 0.9179 - val_loss: 0.3715 - val_acc: 0.8808\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.1939 - acc: 0.9227 - val_loss: 0.3870 - val_acc: 0.8790\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.8128 - acc: 0.7774 - val_loss: 0.4525 - val_acc: 0.8467\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3975 - acc: 0.8618 - val_loss: 0.3925 - val_acc: 0.8663\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3298 - acc: 0.8815 - val_loss: 0.3806 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2913 - acc: 0.8930 - val_loss: 0.3850 - val_acc: 0.8708\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2621 - acc: 0.9032 - val_loss: 0.3690 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2389 - acc: 0.9090 - val_loss: 0.3833 - val_acc: 0.8792\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2200 - acc: 0.9156 - val_loss: 0.3751 - val_acc: 0.8794\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2038 - acc: 0.9205 - val_loss: 0.3897 - val_acc: 0.8780\n",
      "nc=32,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8310 - acc: 0.7800 - val_loss: 0.4775 - val_acc: 0.8469\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3909 - acc: 0.8627 - val_loss: 0.4053 - val_acc: 0.8635\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3223 - acc: 0.8837 - val_loss: 0.3899 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2803 - acc: 0.8947 - val_loss: 0.3653 - val_acc: 0.8786\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2470 - acc: 0.9056 - val_loss: 0.3732 - val_acc: 0.8790\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2245 - acc: 0.9127 - val_loss: 0.3889 - val_acc: 0.8734\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2038 - acc: 0.9200 - val_loss: 0.3866 - val_acc: 0.8780\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.1895 - acc: 0.9256 - val_loss: 0.4003 - val_acc: 0.8761\n",
      "nc=40,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.7113 - acc: 0.7892 - val_loss: 0.4624 - val_acc: 0.8470\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 76s 2ms/step - loss: 0.3717 - acc: 0.8683 - val_loss: 0.3713 - val_acc: 0.8695\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3081 - acc: 0.8891 - val_loss: 0.3742 - val_acc: 0.8725\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2702 - acc: 0.8975 - val_loss: 0.3625 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2395 - acc: 0.9074 - val_loss: 0.3652 - val_acc: 0.8756\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2171 - acc: 0.9156 - val_loss: 0.3814 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1967 - acc: 0.9228 - val_loss: 0.3803 - val_acc: 0.8768\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1831 - acc: 0.9267 - val_loss: 0.3785 - val_acc: 0.8786\n",
      "nc=40,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.7509 - acc: 0.7862 - val_loss: 0.4441 - val_acc: 0.8465\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3778 - acc: 0.8675 - val_loss: 0.3792 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3165 - acc: 0.8853 - val_loss: 0.3916 - val_acc: 0.8672\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2771 - acc: 0.8974 - val_loss: 0.3649 - val_acc: 0.8750\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2486 - acc: 0.9049 - val_loss: 0.3561 - val_acc: 0.8791\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2245 - acc: 0.9128 - val_loss: 0.3640 - val_acc: 0.8813\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2052 - acc: 0.9194 - val_loss: 0.3761 - val_acc: 0.8823\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1915 - acc: 0.9252 - val_loss: 0.3983 - val_acc: 0.8824\n",
      "nc=40,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.8051 - acc: 0.7795 - val_loss: 0.5280 - val_acc: 0.8251\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.3920 - acc: 0.8632 - val_loss: 0.3846 - val_acc: 0.8642\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.3228 - acc: 0.8827 - val_loss: 0.3765 - val_acc: 0.8719\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2829 - acc: 0.8955 - val_loss: 0.3778 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2494 - acc: 0.9052 - val_loss: 0.3731 - val_acc: 0.8770\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2253 - acc: 0.9132 - val_loss: 0.3756 - val_acc: 0.8777\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2030 - acc: 0.9196 - val_loss: 0.3984 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1875 - acc: 0.9262 - val_loss: 0.4055 - val_acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([320,384,448])\n",
    "nconvs=np.array([24,32,40])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8789 0.8795 0.8801]\n",
      " [0.8808 0.8794 0.879 ]\n",
      " [0.8789 0.8824 0.8777]]\n",
      "(3, 3)\n",
      "the best nconvs is 40,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8824\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the best parameters in adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlrate=0.5\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 15.3239 - acc: 0.0481 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3373 - val_acc: 0.0484\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "nlrate=0.1\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 1.0963 - acc: 0.7346 - val_loss: 0.9226 - val_acc: 0.7157\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4747 - acc: 0.8390 - val_loss: 0.4604 - val_acc: 0.8503\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4100 - acc: 0.8592 - val_loss: 0.4354 - val_acc: 0.8538\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.3737 - acc: 0.8695 - val_loss: 0.4122 - val_acc: 0.8661\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.3481 - acc: 0.8754 - val_loss: 0.4121 - val_acc: 0.8616\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3329 - acc: 0.8828 - val_loss: 0.4099 - val_acc: 0.8646\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3160 - acc: 0.8868 - val_loss: 0.4043 - val_acc: 0.8681\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3060 - acc: 0.8900 - val_loss: 0.3969 - val_acc: 0.8676\n",
      "nlrate=0.05\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 84s 2ms/step - loss: 0.7607 - acc: 0.7818 - val_loss: 0.5171 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3861 - acc: 0.8643 - val_loss: 0.3910 - val_acc: 0.8650\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3209 - acc: 0.8834 - val_loss: 0.3837 - val_acc: 0.8688\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 80s 2ms/step - loss: 0.2822 - acc: 0.8963 - val_loss: 0.4366 - val_acc: 0.8546\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2535 - acc: 0.9035 - val_loss: 0.3835 - val_acc: 0.8735\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.2317 - acc: 0.9097 - val_loss: 0.3803 - val_acc: 0.8773\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2111 - acc: 0.9179 - val_loss: 0.3918 - val_acc: 0.8766\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1954 - acc: 0.9233 - val_loss: 0.3879 - val_acc: 0.8778\n",
      "nlrate=0.01\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.6256 - acc: 0.8043 - val_loss: 0.4380 - val_acc: 0.8444\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3671 - acc: 0.8697 - val_loss: 0.3794 - val_acc: 0.8731\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2971 - acc: 0.8896 - val_loss: 0.3908 - val_acc: 0.8694\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.2508 - acc: 0.9035 - val_loss: 0.3646 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.2177 - acc: 0.9149 - val_loss: 0.3810 - val_acc: 0.8764\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.1913 - acc: 0.9236 - val_loss: 0.3842 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1673 - acc: 0.9328 - val_loss: 0.3969 - val_acc: 0.8751\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1506 - acc: 0.9382 - val_loss: 0.4106 - val_acc: 0.8787\n",
      "nlrate=0.005\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.6163 - acc: 0.8041 - val_loss: 0.4270 - val_acc: 0.8538\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3637 - acc: 0.8706 - val_loss: 0.3810 - val_acc: 0.8679\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2955 - acc: 0.8908 - val_loss: 0.3866 - val_acc: 0.8659\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2517 - acc: 0.9033 - val_loss: 0.3508 - val_acc: 0.8832\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2184 - acc: 0.9144 - val_loss: 0.3793 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1919 - acc: 0.9229 - val_loss: 0.4034 - val_acc: 0.8738\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1701 - acc: 0.9308 - val_loss: 0.3874 - val_acc: 0.8777\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1522 - acc: 0.9385 - val_loss: 0.3983 - val_acc: 0.8771\n"
     ]
    }
   ],
   "source": [
    "history1=[]\n",
    "nlrate=np.array([0.5,0.1,0.05,0.01,0.005])\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(convmax, (3, 3), \n",
    "                     padding='valid', \n",
    "                     input_shape=x_train.shape[1:],\n",
    "                     activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(convmax, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nodemax, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "    #model.add(BatchNormalization())\n",
    "    decay = lrate/epochs\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    print('nlrate={}'.format(lrate))\n",
    "    hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,\n",
    "                           validation_data=(x_test, y_test),shuffle=True)\n",
    "    history1.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0485]\n",
      " [0.8681]\n",
      " [0.8778]\n",
      " [0.8789]\n",
      " [0.8832]]\n",
      "the best nlrate is 0.005\n",
      "the maximum val_accuracy is 0.8832\n"
     ]
    }
   ],
   "source": [
    "hh=np.zeros((5,1))\n",
    "for n in range(5):\n",
    "    hh[n]=np.max(history1[n].history['val_acc'])\n",
    "print(hh)\n",
    "c=0\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    if hh[i]>c:\n",
    "        c=hh[i]\n",
    "        lratemax=lrate\n",
    "print('the best nlrate is {}'.format(lratemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(hh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 800)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                24111     \n",
      "=================================================================\n",
      "Total params: 449,167\n",
      "Trainable params: 446,479\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 1.3759 - acc: 0.6288 - val_loss: 0.5408 - val_acc: 0.8247\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.7441 - acc: 0.7658 - val_loss: 0.4594 - val_acc: 0.8515\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.6293 - acc: 0.7939 - val_loss: 0.4471 - val_acc: 0.8543\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5839 - acc: 0.8069 - val_loss: 0.4046 - val_acc: 0.8635\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5515 - acc: 0.8158 - val_loss: 0.3880 - val_acc: 0.8694\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5279 - acc: 0.8240 - val_loss: 0.3807 - val_acc: 0.8707\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.5115 - acc: 0.8296 - val_loss: 0.3806 - val_acc: 0.8692\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4951 - acc: 0.8330 - val_loss: 0.3661 - val_acc: 0.8736\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='valid', \n",
    "                 input_shape=x_train.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "seed=7\n",
    "# Fit the model\n",
    "np.random.seed(seed)\n",
    "hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=16,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 37s 796us/step - loss: 1.2729 - acc: 0.6410 - val_loss: 0.5674 - val_acc: 0.8055\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 36s 779us/step - loss: 0.7804 - acc: 0.7468 - val_loss: 0.4584 - val_acc: 0.8478\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.6805 - acc: 0.7771 - val_loss: 0.4319 - val_acc: 0.8491\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 36s 775us/step - loss: 0.6393 - acc: 0.7880 - val_loss: 0.4124 - val_acc: 0.8636\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 36s 779us/step - loss: 0.5937 - acc: 0.8018 - val_loss: 0.3883 - val_acc: 0.8653\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 37s 805us/step - loss: 0.5775 - acc: 0.8049 - val_loss: 0.3867 - val_acc: 0.8647\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 37s 808us/step - loss: 0.5607 - acc: 0.8110 - val_loss: 0.3786 - val_acc: 0.8699\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 37s 815us/step - loss: 0.5412 - acc: 0.8159 - val_loss: 0.3699 - val_acc: 0.8688\n",
      "nc=16,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 41s 901us/step - loss: 1.3346 - acc: 0.6392 - val_loss: 0.6212 - val_acc: 0.8090\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 40s 867us/step - loss: 0.8131 - acc: 0.7401 - val_loss: 0.4785 - val_acc: 0.8366\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6981 - acc: 0.7670 - val_loss: 0.4590 - val_acc: 0.8447\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6484 - acc: 0.7845 - val_loss: 0.4146 - val_acc: 0.8608\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6149 - acc: 0.7942 - val_loss: 0.4003 - val_acc: 0.8615\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 42s 909us/step - loss: 0.5765 - acc: 0.8076 - val_loss: 0.3867 - val_acc: 0.8673\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 40s 876us/step - loss: 0.5633 - acc: 0.8073 - val_loss: 0.3749 - val_acc: 0.8695\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 40s 880us/step - loss: 0.5497 - acc: 0.8138 - val_loss: 0.3660 - val_acc: 0.8719\n",
      "nc=16,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 44s 947us/step - loss: 1.3598 - acc: 0.6418 - val_loss: 0.5219 - val_acc: 0.8391\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 42s 915us/step - loss: 0.7936 - acc: 0.7502 - val_loss: 0.4434 - val_acc: 0.8509\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 42s 914us/step - loss: 0.6773 - acc: 0.7789 - val_loss: 0.4229 - val_acc: 0.8539\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 42s 918us/step - loss: 0.6164 - acc: 0.7935 - val_loss: 0.4258 - val_acc: 0.8542\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 43s 936us/step - loss: 0.5852 - acc: 0.8010 - val_loss: 0.4110 - val_acc: 0.8607\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 41s 897us/step - loss: 0.5554 - acc: 0.8106 - val_loss: 0.3787 - val_acc: 0.8684\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 42s 903us/step - loss: 0.5415 - acc: 0.8161 - val_loss: 0.3738 - val_acc: 0.8702\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 41s 901us/step - loss: 0.5295 - acc: 0.8191 - val_loss: 0.3640 - val_acc: 0.8723\n",
      "nc=32,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 1.1349 - acc: 0.6782 - val_loss: 0.5100 - val_acc: 0.8386\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.6749 - acc: 0.7797 - val_loss: 0.4271 - val_acc: 0.8536\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5776 - acc: 0.8067 - val_loss: 0.3877 - val_acc: 0.8683\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5389 - acc: 0.8187 - val_loss: 0.3714 - val_acc: 0.8711\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5080 - acc: 0.8282 - val_loss: 0.3638 - val_acc: 0.8730\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4843 - acc: 0.8330 - val_loss: 0.3535 - val_acc: 0.8802\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4644 - acc: 0.8391 - val_loss: 0.3509 - val_acc: 0.8767\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4511 - acc: 0.8435 - val_loss: 0.3407 - val_acc: 0.8809\n",
      "nc=32,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 1.1723 - acc: 0.6795 - val_loss: 0.4996 - val_acc: 0.8317\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.6826 - acc: 0.7793 - val_loss: 0.4339 - val_acc: 0.8549\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.5816 - acc: 0.8035 - val_loss: 0.3944 - val_acc: 0.8654\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5239 - acc: 0.8205 - val_loss: 0.3891 - val_acc: 0.8635\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4963 - acc: 0.8290 - val_loss: 0.3595 - val_acc: 0.8764\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4777 - acc: 0.8348 - val_loss: 0.3571 - val_acc: 0.8767\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4546 - acc: 0.8417 - val_loss: 0.3523 - val_acc: 0.8781\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4429 - acc: 0.8455 - val_loss: 0.3452 - val_acc: 0.8840\n",
      "nc=32,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 1.2270 - acc: 0.6756 - val_loss: 0.5179 - val_acc: 0.8399\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.6895 - acc: 0.7800 - val_loss: 0.4390 - val_acc: 0.8503\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.5845 - acc: 0.8052 - val_loss: 0.3838 - val_acc: 0.8710\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.5290 - acc: 0.8210 - val_loss: 0.3860 - val_acc: 0.8665\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4901 - acc: 0.8312 - val_loss: 0.3652 - val_acc: 0.8731\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4714 - acc: 0.8367 - val_loss: 0.3511 - val_acc: 0.8763\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4517 - acc: 0.8417 - val_loss: 0.3434 - val_acc: 0.8797\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4396 - acc: 0.8448 - val_loss: 0.3443 - val_acc: 0.8774\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.0758 - acc: 0.6925 - val_loss: 0.5251 - val_acc: 0.8308\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.6324 - acc: 0.7931 - val_loss: 0.4158 - val_acc: 0.8581\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.5486 - acc: 0.8146 - val_loss: 0.3914 - val_acc: 0.8617\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.4994 - acc: 0.8279 - val_loss: 0.3591 - val_acc: 0.8746\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4665 - acc: 0.8383 - val_loss: 0.3488 - val_acc: 0.8789\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4476 - acc: 0.8434 - val_loss: 0.3461 - val_acc: 0.8825\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4322 - acc: 0.8489 - val_loss: 0.3383 - val_acc: 0.8843\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4181 - acc: 0.8538 - val_loss: 0.3354 - val_acc: 0.8843\n",
      "nc=48,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.1281 - acc: 0.6913 - val_loss: 0.5035 - val_acc: 0.8354\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.6516 - acc: 0.7875 - val_loss: 0.4171 - val_acc: 0.8629\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.5504 - acc: 0.8166 - val_loss: 0.4056 - val_acc: 0.8603\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.4978 - acc: 0.8319 - val_loss: 0.3809 - val_acc: 0.8651\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4719 - acc: 0.8366 - val_loss: 0.3474 - val_acc: 0.8803\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.4444 - acc: 0.8439 - val_loss: 0.3524 - val_acc: 0.8758\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4270 - acc: 0.8517 - val_loss: 0.3423 - val_acc: 0.8798\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4150 - acc: 0.8538 - val_loss: 0.3395 - val_acc: 0.8797\n",
      "nc=48,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 103s 2ms/step - loss: 1.1533 - acc: 0.6935 - val_loss: 0.5473 - val_acc: 0.8152\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.6521 - acc: 0.7908 - val_loss: 0.4104 - val_acc: 0.8612\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.5419 - acc: 0.8181 - val_loss: 0.3869 - val_acc: 0.8659\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4921 - acc: 0.8304 - val_loss: 0.3684 - val_acc: 0.8692\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4597 - acc: 0.8403 - val_loss: 0.3562 - val_acc: 0.8767\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 108s 2ms/step - loss: 0.4366 - acc: 0.8469 - val_loss: 0.3424 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.4199 - acc: 0.8522 - val_loss: 0.3447 - val_acc: 0.8776\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 107s 2ms/step - loss: 0.4030 - acc: 0.8551 - val_loss: 0.3310 - val_acc: 0.8817\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([512,640,768])\n",
    "nconvs=np.array([16,32,48])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.45))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        decay = 0.005/epochs\n",
    "        opt = keras.optimizers.adam(lr=0.005, decay=decay)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8699 0.8719 0.8723]\n",
      " [0.8809 0.884  0.8797]\n",
      " [0.8843 0.8803 0.8817]]\n",
      "(3, 3)\n",
      "the best nconvs is 48,the best nnode is 512\n",
      "the maximum val_accuracy is 0.8843\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=40,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 82s 2ms/step - loss: 1.0752 - acc: 0.6863 - val_loss: 0.4895 - val_acc: 0.8375\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.6582 - acc: 0.7842 - val_loss: 0.4107 - val_acc: 0.8603\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 79s 2ms/step - loss: 0.5750 - acc: 0.8083 - val_loss: 0.3882 - val_acc: 0.8663\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 81s 2ms/step - loss: 0.5250 - acc: 0.8217 - val_loss: 0.3738 - val_acc: 0.8697\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 81s 2ms/step - loss: 0.5053 - acc: 0.8291 - val_loss: 0.3694 - val_acc: 0.8748\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 82s 2ms/step - loss: 0.4780 - acc: 0.8376 - val_loss: 0.3529 - val_acc: 0.8781\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 79s 2ms/step - loss: 0.4576 - acc: 0.8413 - val_loss: 0.3447 - val_acc: 0.8788\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 84s 2ms/step - loss: 0.4474 - acc: 0.8456 - val_loss: 0.3397 - val_acc: 0.8798\n",
      "nc=40,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 85s 2ms/step - loss: 1.1003 - acc: 0.6875 - val_loss: 0.5015 - val_acc: 0.8349\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.6545 - acc: 0.7861 - val_loss: 0.4229 - val_acc: 0.8571\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.5659 - acc: 0.8091 - val_loss: 0.4093 - val_acc: 0.8632\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 86s 2ms/step - loss: 0.5264 - acc: 0.8199 - val_loss: 0.3847 - val_acc: 0.8644\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.4991 - acc: 0.8288 - val_loss: 0.3658 - val_acc: 0.8745\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4678 - acc: 0.8405 - val_loss: 0.3589 - val_acc: 0.8762\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.4561 - acc: 0.8428 - val_loss: 0.3537 - val_acc: 0.8793\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 90s 2ms/step - loss: 0.4425 - acc: 0.8459 - val_loss: 0.3403 - val_acc: 0.8818\n",
      "nc=40,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.1038 - acc: 0.6884 - val_loss: 0.4882 - val_acc: 0.8383\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.6549 - acc: 0.7887 - val_loss: 0.4467 - val_acc: 0.8471\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 91s 2ms/step - loss: 0.5650 - acc: 0.8113 - val_loss: 0.3991 - val_acc: 0.8631\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.5174 - acc: 0.8247 - val_loss: 0.3805 - val_acc: 0.8693\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 90s 2ms/step - loss: 0.4929 - acc: 0.8307 - val_loss: 0.3537 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 100s 2ms/step - loss: 0.4641 - acc: 0.8397 - val_loss: 0.3490 - val_acc: 0.8791\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.4499 - acc: 0.8444 - val_loss: 0.3433 - val_acc: 0.8791\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.4319 - acc: 0.8479 - val_loss: 0.3436 - val_acc: 0.8790\n",
      "nc=48,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 100s 2ms/step - loss: 1.0457 - acc: 0.6952 - val_loss: 0.5170 - val_acc: 0.8286\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.6355 - acc: 0.7892 - val_loss: 0.4384 - val_acc: 0.8501\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 108s 2ms/step - loss: 0.5656 - acc: 0.8097 - val_loss: 0.4064 - val_acc: 0.8556\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.5124 - acc: 0.8260 - val_loss: 0.3686 - val_acc: 0.8742\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.4886 - acc: 0.8347 - val_loss: 0.3578 - val_acc: 0.8749\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4681 - acc: 0.8403 - val_loss: 0.3511 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 104s 2ms/step - loss: 0.4450 - acc: 0.8450 - val_loss: 0.3398 - val_acc: 0.8808\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4361 - acc: 0.8478 - val_loss: 0.3323 - val_acc: 0.8835\n",
      "nc=48,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 105s 2ms/step - loss: 1.0653 - acc: 0.6941 - val_loss: 0.4666 - val_acc: 0.8464\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 109s 2ms/step - loss: 0.6332 - acc: 0.7906 - val_loss: 0.4300 - val_acc: 0.8522\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 104s 2ms/step - loss: 0.5474 - acc: 0.8136 - val_loss: 0.3901 - val_acc: 0.8679\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.5023 - acc: 0.8280 - val_loss: 0.3654 - val_acc: 0.8744\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.4734 - acc: 0.8393 - val_loss: 0.3648 - val_acc: 0.8719\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 109s 2ms/step - loss: 0.4594 - acc: 0.8401 - val_loss: 0.3515 - val_acc: 0.8803\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 118s 3ms/step - loss: 0.4406 - acc: 0.8487 - val_loss: 0.3416 - val_acc: 0.8836\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 107s 2ms/step - loss: 0.4247 - acc: 0.8513 - val_loss: 0.3481 - val_acc: 0.8810\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 1.0833 - acc: 0.6926 - val_loss: 0.5314 - val_acc: 0.8313\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 0.6381 - acc: 0.7911 - val_loss: 0.4092 - val_acc: 0.8555\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 0.5485 - acc: 0.8146 - val_loss: 0.3858 - val_acc: 0.8728\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.5069 - acc: 0.8273 - val_loss: 0.3635 - val_acc: 0.8770\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4704 - acc: 0.8380 - val_loss: 0.3632 - val_acc: 0.8734\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4558 - acc: 0.8431 - val_loss: 0.3424 - val_acc: 0.8828\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4355 - acc: 0.8485 - val_loss: 0.3460 - val_acc: 0.8811\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4149 - acc: 0.8541 - val_loss: 0.3302 - val_acc: 0.8845\n",
      "nc=56,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 115s 3ms/step - loss: 1.0240 - acc: 0.7032 - val_loss: 0.5051 - val_acc: 0.8292\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.6298 - acc: 0.7922 - val_loss: 0.4298 - val_acc: 0.8581\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.5586 - acc: 0.8143 - val_loss: 0.3833 - val_acc: 0.8642\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.5036 - acc: 0.8298 - val_loss: 0.3682 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.4776 - acc: 0.8373 - val_loss: 0.3636 - val_acc: 0.8758\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.4537 - acc: 0.8428 - val_loss: 0.3483 - val_acc: 0.8798\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4362 - acc: 0.8463 - val_loss: 0.3468 - val_acc: 0.8785\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4271 - acc: 0.8500 - val_loss: 0.3345 - val_acc: 0.8839\n",
      "nc=56,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 111s 2ms/step - loss: 1.0446 - acc: 0.6986 - val_loss: 0.5040 - val_acc: 0.8311\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.6290 - acc: 0.7943 - val_loss: 0.4286 - val_acc: 0.8554\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.5382 - acc: 0.8173 - val_loss: 0.3962 - val_acc: 0.8671\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4979 - acc: 0.8320 - val_loss: 0.3711 - val_acc: 0.8693\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4682 - acc: 0.8381 - val_loss: 0.3619 - val_acc: 0.8786\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4456 - acc: 0.8440 - val_loss: 0.3439 - val_acc: 0.8809\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4272 - acc: 0.8510 - val_loss: 0.3545 - val_acc: 0.8796\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4074 - acc: 0.8566 - val_loss: 0.3356 - val_acc: 0.8836\n",
      "nc=56,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 1.0466 - acc: 0.7035 - val_loss: 0.4966 - val_acc: 0.8411\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.6194 - acc: 0.7956 - val_loss: 0.4209 - val_acc: 0.8579\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.5297 - acc: 0.8215 - val_loss: 0.3742 - val_acc: 0.8702\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4857 - acc: 0.8341 - val_loss: 0.3782 - val_acc: 0.8707\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4574 - acc: 0.8446 - val_loss: 0.3570 - val_acc: 0.8791\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4387 - acc: 0.8469 - val_loss: 0.3459 - val_acc: 0.8762\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4168 - acc: 0.8547 - val_loss: 0.3332 - val_acc: 0.8839\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.4045 - acc: 0.8574 - val_loss: 0.3380 - val_acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([384,448,512])\n",
    "nconvs=np.array([40,48,56])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.45))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        decay = 0.005/epochs\n",
    "        opt = keras.optimizers.adam(lr=0.005, decay=decay)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8798 0.8818 0.8791]\n",
      " [0.8835 0.8836 0.8845]\n",
      " [0.8839 0.8836 0.8839]]\n",
      "(3, 3)\n",
      "the best nconvs is 48,the best nnode is 512\n",
      "the maximum val_accuracy is 0.8845\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
