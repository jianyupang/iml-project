{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load emnist samples and adjust data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "def load_emnist(file_path='emnist-bymerge.mat'):\n",
    "    \"\"\"\n",
    "    Loads training and test data with ntr and nts training and test samples\n",
    "    The `file_path` is the location of the `eminst-balanced.mat`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Load the MATLAB file\n",
    "    mat = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Get the training data\n",
    "    Xtr = mat['dataset'][0][0][0][0][0][0][:]\n",
    "    ntr = Xtr.shape[0]\n",
    "    ytr = mat['dataset'][0][0][0][0][0][1][:].reshape(ntr).astype(int)\n",
    "    \n",
    "    # Get the test data\n",
    "    Xts = mat['dataset'][0][0][1][0][0][0][:]\n",
    "    nts = Xts.shape[0]\n",
    "    yts = mat['dataset'][0][0][1][0][0][1][:].reshape(nts).astype(int)\n",
    "    \n",
    "    print(\"%d training samples, %d test samples loaded\" % (ntr, nts))\n",
    "\n",
    "    return [Xtr, Xts, ytr, yts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697932 training samples, 116323 test samples loaded\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xts, ytr, yts = load_emnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697932, 784) (116323, 784) (697932,) (116323,)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape,Xts.shape,ytr.shape,yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrd=np.reshape(Xtr,(697932,28,28),order='F')\n",
    "Xtsd=np.reshape(Xts,(116323,28,28),order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1855129780>"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182e7c72b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Xtrd[np.random.randint(1,20000),:,:],cmap='Greys_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Xtsd[np.random.randint(1,10000),:,:],cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntr = 46000\n",
    "nts = 10000\n",
    "\n",
    "# TODO: proper decide the number of samples and the ratio between dig and let\n",
    "\n",
    "# Create sub-sampled training and test data\n",
    "nsamp = Xtr.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xtr1 = Xtrd[Iperm[:ntr],:,:]\n",
    "ytr1 = ytr[Iperm[:ntr]]\n",
    "nsamp = Xts.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xts1 = Xtsd[Iperm[:nts],:,:]\n",
    "yts1 = yts[Iperm[:nts]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28)\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[12  6  4 ... 41  0 10]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr1.shape)\n",
    "print(Xtr1[233,15:20,15:20])\n",
    "print(ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28, 1) (10000, 28, 28, 1) (46000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = Xtr1.astype('float32')\n",
    "x_test = Xts1.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train=x_train.reshape((ntr,28,28,1))\n",
    "x_test=x_test.reshape((nts,28,28,1))\n",
    "y_train=ytr1.reshape((len(ytr1),1))\n",
    "y_test=yts1.reshape((len(yts1),1))\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x182064ae80>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwg8ERfiBoAg/EBThB4Ii/EBQ/w/1VtnVbzTypQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1857f4cac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myxt=np.zeros((28,28))\n",
    "myxt[:,:]=x_test[np.random.randint(1,10000),:,:,0]\n",
    "plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum=15\\nfor m in range(len(y_train)):\\n    r=np.where(y_train==num)\\n    x_train[r,:,:,0]=0\\nfor m in range(len(y_test)):\\n    r=np.where(y_test==num)\\n    x_test[r,:,:,0]=0\\n'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the confusing data: F\n",
    "'''\n",
    "num=15\n",
    "for m in range(len(y_train)):\n",
    "    r=np.where(y_train==num)\n",
    "    x_train[r,:,:,0]=0\n",
    "for m in range(len(y_test)):\n",
    "    r=np.where(y_test==num)\n",
    "    x_test[r,:,:,0]=0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1823cbe940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the respective letter\n",
    "num=21\n",
    "for m in range(10):\n",
    "    r=np.where(y_train==num)[0][m]\n",
    "    myxt[:,:]=x_train[r,:,:,0]\n",
    "    plt.subplot(2,5,m+1)\n",
    "    plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "num_classes = 47\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('Number of classes:', y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 8\n",
    "lrate = 0.05\n",
    "decay = lrate/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: 36/62 channels?\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='valid', \n",
    "                 input_shape=x_train.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 800)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                24111     \n",
      "=================================================================\n",
      "Total params: 449,167\n",
      "Trainable params: 446,479\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.6302 - acc: 0.8027 - val_loss: 0.4271 - val_acc: 0.8616\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.3775 - acc: 0.8687 - val_loss: 0.4012 - val_acc: 0.8638\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.3123 - acc: 0.8860 - val_loss: 0.4087 - val_acc: 0.8629\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2725 - acc: 0.8980 - val_loss: 0.3972 - val_acc: 0.8673\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2424 - acc: 0.9065 - val_loss: 0.4307 - val_acc: 0.8664\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2147 - acc: 0.9152 - val_loss: 0.4057 - val_acc: 0.8675\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.1971 - acc: 0.9217 - val_loss: 0.4199 - val_acc: 0.8656\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.1821 - acc: 0.9266 - val_loss: 0.4359 - val_acc: 0.8622\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed=7\n",
    "k=1\n",
    "c=4\n",
    "# Fit the model\n",
    "np.random.seed(seed)\n",
    "class_weight={0:k*1.3,1:k/1.8,2:k/1.1,3:k,4:k,5:k*1.5,6:k,7:k,8:k*2,9:k*2,10:k,11:k,12:k,13:k,14:k,15:k*2,16:k,17:k,18:k,19:k,20:k,21:k*2,\n",
    "              22:k,23:k,24:k*2,25:k,26:k,27:k,28:k,29:k,30:k,31:k,32:k,33:k,34:k,35:k*3,36:k,37:k*2,38:k,39:k,40:k*10,41:k*8,42:k,\n",
    "              43:k,44:k*9,45:k,46:k}\n",
    "hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True,\n",
    "                       #class_weight='auto'\n",
    "                      )\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")\n",
    "model = load_model(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prediction based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23515804e-07 3.72122955e-08 3.33188154e-07 5.02231833e-11\n",
      "  2.66159816e-07 7.74245723e-10 2.93974267e-06 8.25428259e-09\n",
      "  1.30503670e-08 1.94423663e-07 3.92194522e-07 9.07756856e-08\n",
      "  1.39135006e-11 5.67333132e-08 9.90347626e-09 1.37257697e-12\n",
      "  2.46301897e-06 1.12099215e-05 5.48474244e-09 7.67718422e-09\n",
      "  4.68045869e-07 3.39324941e-08 7.82226408e-08 5.11993903e-07\n",
      "  8.78020046e-10 9.56579260e-12 2.21214600e-06 4.08123579e-09\n",
      "  1.49083142e-11 3.07458663e-08 9.69479024e-01 2.14808570e-05\n",
      "  3.04293744e-02 2.45321161e-07 4.07663583e-06 3.00070724e-08\n",
      "  8.79328638e-07 2.48769254e-08 9.38984249e-06 4.17549190e-10\n",
      "  1.87652158e-10 1.93214706e-08 3.35925979e-05 6.83860790e-08\n",
      "  2.19290456e-07 5.72550236e-11 3.96266930e-09]]\n",
      "30\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "u\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfiCo/wdWrFdE5cT/PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1854d95390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myn=np.random.randint(1,20000)\n",
    "myxtr=np.array(x_train[myn,:,:,:])\n",
    "myxtrp=np.reshape(myxtr,(28,28))\n",
    "plt.imshow(myxtrp,cmap='Greys_r')\n",
    "myxtr=np.reshape(myxtr,(1,28,28,1))\n",
    "preds = model.predict(myxtr)\n",
    "print(preds)\n",
    "print(np.argmax(preds))\n",
    "print(y_train[myn])\n",
    "if np.argmax(y_train[myn])<10:\n",
    "    ascii=48+np.argmax(y_train[myn])\n",
    "else:\n",
    "    ascii=87+np.argmax(y_train[myn])\n",
    "print(chr(ascii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.862200\n"
     ]
    }
   ],
   "source": [
    "yhat=model.predict(x_test)\n",
    "yhatp=np.argmax(yhat,axis=1)\n",
    "ytsp=np.argmax(y_test,axis=1)\n",
    "acc = np.mean(yhatp == ytsp)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.843 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.739 0.002 ... 0.    0.    0.   ]\n",
      " [0.    0.    0.957 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.2   0.    0.   ]\n",
      " [0.    0.014 0.    ... 0.    0.829 0.014]\n",
      " [0.    0.    0.    ... 0.    0.    0.895]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x18304090b8>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAJCCAYAAADnUI67AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XuwrWddJ/jv7+xzkmMi4RakMSch\nQRIxoHKJkUuqpFHaaDOgpWPBKINWCsoe6ZHSVqG7xRF7qtWuUXuqmO6JHYRhHJHBtsnQmcogl7KR\nWxJANEkDxwgk4RKBhNA5ncve+5k/zgY3x3N5n5zz7vd91/58qlad9a79O8/+rfWutfazf/tZz69a\nawEAgDnaM3UCAABwLCarAADMlskqAACzZbIKAMBsmawCADBbJqsAAMyWySoAALNlsgoAwGyZrAIA\nMFt7d/KbPeTh+9ojz9k/OP5LN+4bMRtgJZwx/D0lh+4dLw9glu7NPbm/3VdT5zGW7//7Z7Yvfmlj\nR77XDR+979rW2uU78s222dHJ6iPP2Z9/9kdPHhz/5m/7eyNmw8qozvcgLYZXSj3hiYNj24dvHDET\nYI4+0N4xdQqj+uKXNvLBa8/bke+19phPnL0j3+gIJ7UMoKour6qPVdXBqnrlqUoKAACSk6isVtVa\nktcmeW6S25JcV1VXt9ZuOlXJAQBwbC3JZjanTmNUJ1NZvTTJwdbaLa21+5O8KckLTk1aAABwcmtW\nz0ly67bj25J898mlAwDAcC0bTWX1pFTVy6rq+qq6/it3PjD2twMAYIWcTGX19iTnbjs+sHXb12mt\nXZnkyiQ5/0kP8TFsAIBT5PCa1dWeXp1MZfW6JBdW1QVVdVqSFya5+tSkBQAAJ1FZba2tV9XLk1yb\nZC3J61prNjEEANhBq74bwEk1BWitXZPkmlOUCwAAfJ0d7WAFAMCp09KyseKdGXd0svqlG/d1tVB9\n1HsfNjj2b55514NJiVWw4i/Sr6p9p3XFtwfuHymT5N7/5tKu+P3/zwdHykQLVYBVp7IKALBgdgMA\nAICJmKwCADBblgEAACxUS7JhGQAAAExDZRUAYMF8wAoAACaisgoAsFAtWfmmACqrAADMlsoqAMCC\nbU6dwMhmPVntaaH6xlv/rGvsF5/7rN505qGqL37F/zSwm4zZPrXX/rddN3UK0N+CeGNjePBmRyww\nqllPVgEAOLaWZp9VAACYisoqAMBStWRjtQurKqsAAMyXyioAwEK1rP5uACqrAADMlsoqAMBiVTbS\nua3lwqisAgAwWyarAADMlmUAAAAL1ZJs2roKAACmsTKV1Ref+6yu+Gs/85HBsd//zU/uTWc8bcV/\nfWIZPA+ZgfbA/VOnALPgA1YAADCRlamsAgDsNi0qqwAAMBmVVQCABdtsKqsAADAJlVUAgIWyZhUA\nACaksgoAsFAtlY0Vrz2u9r0DAGDRVFYBABZs1XcD2LWT1Z4Wqr/76fd0jf3S8y7rTWceqvPJruXm\nyet5zD3eDLXg1/Ke/fsHx27ee++ImQBzsWsnqwAAS2c3AAAAmJDJKgAAs2UZAADAYlU22mrXHlf7\n3gEAsGgqqwAAC9WSbK547XG17x0AAIumsgoAsGC2rgIAgImorAIALFRrdgMAAIDJqKwO8NLzLuuK\n/7efes/g2J9+bN/YrJgxe7L39IefUW/43WLPmWd2xW/ec8/w4AWfz8177506BVicTWtWAQBgGiqr\nAAAL1ZJsrHjtcbXvHQAAi6ayCgCwWHYDAACAyaisAgAsVEuyueK1x9W+dwAALJrJKgAAs2UZAADA\ngm00TQEAAGASKqsj6Gmh+se3fbBr7B8+cGlvOsPNqUXjnrXhsZsb4+XRq6fFaTLuYz6n88nf0dU+\nFeAYWkpTAAAAmIrKKgDAgm1qCgAAANNQWQUAWKiWWLMKAABTUVkFAFiolrLPKgAATEVlFQBgwTZX\nvPa42vcOAIBFU1kFAFio1pIN+6wCAMA05l1Z7emzvtA+6D984NKu+D+67f1d8T9y4OnDg+fU135z\nY7yxx7TQ5yELsGdteOxSXz/Jrnjfh1Orshm7AQAAwCRMVgEAmK15LwMAAOCYWnzACgAAJqOyCgCw\nYBsrXntc7XsHAMCiqawCACxUS2Wz2boKAAAmobIKALBg1qwCAMBEVFYBABaqJdlc8X1W5z1Z7en7\nPKe+9r25dPiRA0/viv/9W/9scOyPn3dZbzrATtncmDqDnTHme/NcjP3zqmf8Of0s3A3nngdl3pNV\nAACOo7IRuwEAAMAkVFYBABZqN6xZXe17BwDAoqmsAgAsmDWrAAAwEZVVAICFaq2sWQUAgKmYrAIA\nMFsmqwAAC7bR9uzIZYiquryqPlZVB6vqlUf5+nlV9a6q+nBVfbSqfvBEY67OmtU5tWmbUfu6Hz/3\nWYNj//i2D3SN/cMHLu2KB5jUXNqQjv3zqjrqUG3ENr5z+rnMjqiqtSSvTfLcJLclua6qrm6t3bQt\n7J8neXNr7d9U1cVJrkly/vHGXZ3JKgDALtOSbM5n66pLkxxsrd2SJFX1piQvSLJ9stqSnLV1/aFJ\nPnOiQU1WAQAY4uyqun7b8ZWttSu3HZ+T5NZtx7cl+e4jxvifkvx/VfWPk5yZ5PtO9E1NVgEAFqsG\nryc9Bb7QWrvkJMd4UZLXt9b+l6p6RpI3VtWTWmubx/oPPmAFAMCpcHuSc7cdH9i6bbsrkrw5SVpr\n70uyP8nZxxvUZBUAYKFaks1WO3IZ4LokF1bVBVV1WpIXJrn6iJhPJ/neJKmqb8vhyerfHG9Qk1UA\nAE5aa209ycuTXJvk5hz+1P+NVfWaqnr+VtjPJ3lpVf15kj9I8pOtHX/rCGtWAQAWbGNGtcfW2jU5\nvB3V9tteve36TUmG76sZlVUAAGZMZRUAYKFaBq8nXSyVVQAAZktlFQBgwTZXvPa4MpPVOv30rvh2\n330jZTKyEXst//CBS7viv+NDw//s8NFLOl9ImyP2q56TPWvDY8d8THrySPpzmcv97DX248KOWrvo\nWwbHbnzs4IiZjGwu7xVzycPLcvFWZrIKALDbtJZs7PY1q1X1uqq6o6r+ctttj6iqt1fVJ7b+ffi4\naQIAsBsN+dvs65NcfsRtr0zyjtbahUnesXUMAACn1AmXAbTW/rSqzj/i5hckefbW9TckeXeSXzqF\neQEAMICtq47u0a21z25d/1ySR5+ifAAA4GtO+gNWrbVWVcf8iHpVvSzJy5Jkf8442W8HAMCWw00B\nVnvrqgd77z5fVY9Jkq1/7zhWYGvtytbaJa21S/alb3spAAB2twc7Wb06yUu2rr8kyVtPTToAAPTY\nSO3IZSpDtq76gyTvS/KtVXVbVV2R5NeTPLeqPpHk+7aOAQDglBqyG8CLjvGl7z3FuQAA0KFl9XcD\nWJkOVr3tU/eceebg2M177ulNZ7Dad1pXfHvg/q74vX9v+EYN65/7fNfYH33q8NavV3y8r3XhVRdd\n0BW/WHNpzzl2HnO5n7068+55Pfe+ljl5i26hOhdzeS3PJQ92xMpMVgEAdh+7AQAAwGRUVgEAFmxz\nwk/q7wSVVQAAZktlFQBgoVpLNlZ8NwCVVQAAZktlFQBgwewGAAAAEzFZBQBgtiwDAABYqJZa+Xar\nKqsAAMzW6lRWq++3is1Dh0ZKpNOecX8bWv/c50cdf6irLrqgK/5Xb7lhcOyvPO5pvelwpM7XT1ob\nJ4+Faw/cP3UKhzmfR9fzuIz4mOx5yEO64je/8pWRMhlZ7/Owx255zg6kKQAAAExkdSqrAAC7TEus\nWQUAgKmorAIALJimAAAAMBGVVQCApWr2WQUAgMmorAIALFSLfVYBAGAyKqsAAAu26mtWd3SyWvv2\nZe+jv3lw/Prtnxk++EJbr7X77ps6hVnqaaH6i3/1F11j/+a3fHtvOqtvoa+fJKm9472NtfX10cYe\n1YLP56hm8rgstn1qr5k83iyfyioAwELpYAUAABMyWQUAYLYsAwAAWDDLAAAAYCIqqwAAC9Wi3SoA\nAExGZRUAYMG0WwUAgImorAIALFWzGwAAAExmRyur7YEHsn77Z0YZu/ad1pnL/aPkkSR7zjxzcOzm\nPfeMlseS9ZzP3/yWb+8a+7//2K1d8f/nU58wONb5nEAN/517zNd9rzm9Z7GznHtOJe1WAQBgQtas\nAgAsmMoqAABMRGUVAGChdLACAIAJqawCACxYU1kFAIBpmKwCADBblgEAACzYZiwDAACASaisAgAs\nVGur3xRgZSar3b2Tq+PEttY19OahQ3259OjJe2ydj0vX0CP2wv4/vvXcrvgnf/iewbEfeUpvNpys\nnudKnX5639j33debzvCx1x8Ybewlq73Dfyy19fURMxlP7/vb2sUXdcVv3PTxrvhF6voZPl4a7IyV\nmawCAOxGtq4CAICJqKwCACyWdqsAADAZlVUAgAWzZhUAACaisgoAsFAtq7/PqsoqAACzpbIKALBU\nbdQePbOgsgoAwGztbGW1qqvd4ZitDkf9NaRj7L2PO79r6PVbPtmXS4feVpRjGvXcd+ppofqo9z6s\na+y/eeZdndlwMub0vBrzPWhObWW7ra0Nj11ou9Veve1T1846a/jYd9/dm848rHopsdNmrFkFAIBJ\nmKwCADBbPmAFALBQLZoCAADAZFRWAQAWqzQFAACAqaisAgAs2Krv5KWyCgDAbKmsAgAsmN0AAABg\nIiqrAAAL1drqV1Z3drLa2rx6UA+09vCHd8Vv3Hnn4Nj1Wz7Zmc14lnhu5uZvnnlXV/z/ePA/D479\nXx//hN502KXm9Fres39/V/zmvfeOlMl8jP2YbNx9d1d8j57cd8O5ZGeorAIALJh9VgEAYCIqqwAA\nC2afVQAAmIjKKgDAgq36bgAqqwAAzJbJKgAAs2UZAADAQrWUZQAAADAVlVUAgAVb8Z2r5j1Z/at/\n9YzBsd/yC+/rG7yGl8x72qdCj54Wql/6qeGvhyR5xO91viY6rD/naV3xe995w0iZMHdabv5dS35M\nlpw7yzXrySoAAMfRbF0FAACTUVkFAFiyFV+0qrIKAMApUVWXV9XHqupgVb3yGDE/VlU3VdWNVfV/\nnWhMlVUAgAWby5rVqlpL8tokz01yW5Lrqurq1tpN22IuTPKqJM9qrd1ZVd90onFVVgEAOBUuTXKw\ntXZLa+3+JG9K8oIjYl6a5LWttTuTpLV2x4kGNVkFAFiw1nbmMsA5SW7ddnzb1m3bXZTkoqr6s6p6\nf1VdfqJBLQMAAGCIs6vq+m3HV7bWruwcY2+SC5M8O8mBJH9aVd/eWrvreP8BAIAFatnRNatfaK1d\ncpyv357k3G3HB7Zu2+62JB9orT2Q5K+r6uM5PHm97liDWgYAAMCpcF2SC6vqgqo6LckLk1x9RMx/\nyOGqaqrq7BxeFnDL8QZVWQUAWKqWZCa7AbTW1qvq5UmuTbKW5HWttRur6jVJrm+tXb31tX9QVTcl\n2UjyC621Lx5v3FlPVi/6tRsHx270Dj5wpXCS1N6+h2nPGWcMjt24++6+sffv74rXx/nk9Zz/tr4+\nWh7f9PZPd8VfeMPwvG9+Wl/e+/70z7viV3y/agC2tNauSXLNEbe9etv1luTnti6DWAYAAMBszbqy\nCgDA8XX8sXiRVFYBAJgtlVUAgCVTWQUAgGmorAIALFbtZFOASaisAgAwWyqrAABLZs0qAABMQ2UV\nAGCpWlZ+zeqsJ6s9rUh7W6L2tMXsbaHZ20K1h/apO2/MFqo9rXnXb7u9a+ybnzY89tf++rqusX/5\ngu/qimd19L7X1mmndcVvHjrUFQ+svllPVgEAOAFrVgEAYBonnKxW1blV9a6quqmqbqyqn926/RFV\n9faq+sTWvw8fP10AAL5e7dBlGkMqq+tJfr61dnGSpyf5maq6OMkrk7yjtXZhkndsHQMAwClzwslq\na+2zrbUPbV3/SpKbk5yT5AVJ3rAV9oYkPzRWkgAAHEPboctEuj5gVVXnJ3lKkg8keXRr7bNbX/pc\nkkcf4/+8LMnLkmR/hn/yGQAABn/Aqqq+MckfJXlFa+3r9mZqrR1zzt1au7K1dklr7ZJ9Of2kkgUA\nYHcZNFmtqn05PFH9/dbav9+6+fNV9Zitrz8myR3jpAgAwDGt+DKAIbsBVJKrktzcWvutbV+6OslL\ntq6/JMlbT316AADsZkPWrD4ryYuT/EVVfWTrtn+a5NeTvLmqrkjyqSQ/Nk6KAAAcVUuy29utttbe\nk2NvrvW9pzYdAAD4W7Nut/qZX3zm4Nhv/s33jpjJePbs398Vv3nvvSNlwhS6+qDvWescfGNw6C9f\n8F1dQ3/8957WFX/RT93QFc98tfX1UeOBfk27VQAAmMasK6sAAJyAyioAAExDZRUAYMlWfDcAlVUA\nAGZLZRUAYMHKmlUAAJiGyioAwFK12A0AAACmorIKALBYZTcAAACYyqwrq9/8m++dOoXRbd5779Qp\nPHjV8ZvcqjcufrB6HsPNjfHy6HTRT93QFf/GW/9scOyLz31Wbzocqed5lXh9Mtie/fsHxy765xuz\nMuvJKgAAJ7Div29aBgAAwGyprAIALJnKKgAATENlFQBgyVRWAQBgGiqrAABL1aIpAAAATEVlFQBg\nwcqaVQAAmMbqVFa1F9x5PY+h87Orvfi8ywbH/s4nh7dmTZJXnP/M3nRWn9cPY9m3b3isdqs7Z8Vf\n8iqrAADMlskqAACzZbIKAMBsrc6aVQCAXchuAAAAMBGVVQCAJdPBCgAApmGyCgDAbFkGAACwVC2a\nAgAAwFRUVgEAlmzFK6s7Olm979wz84lf+u7B8Rf+4w8MH1wv7Hlzfo5utzwuHffzFec/s2voW379\nGYNjH/fK93WNDaPYs9YXv7kxTh4PwuZXvjJ1CuxCKqsAAAumKQAAAExEZRUAYMlUVgEAYBoqqwAA\nS6ayCgAA01BZBQBYqGp2AwAAgMmorAIALFmrqTMYlcoqAACztaOV1dNvvaevhSonrfYOP8Vtfb1z\n8I7f5EZsK9pzH5MHcT+Ztce96v2DY6/69Hu6xr7ivMt604ETa5tTZzBPM/mZskgr/nCorAIAMFsm\nqwAAzJYPWAEALJitqwAAYCIqqwAAS6ayCgAA01BZBQBYKu1WAQBgOiqrAABLprIKAADTUFkFAFiy\nFa+s7uhktfbsyZ5vOGNw/OahQ4Nj95wxfNzesZesra8Pjl07+5FdY2984Yu96Yyi5z6OzfNwAh09\nwq8477KuoZ9/U99z/OqL+15D7Kye1+eor0197Y/O48IxqKwCACyY3QAAAGAiJqsAAMyWySoAALNl\nzSoAwJJZswoAANMwWQUAYLYsAwAAWKpm6yoAAJiMyioAwJKteGV1RyerbXNztBZ22lYeXe07bXBs\nb/vUnrHbA/d3jb1US30e7jnzzK74zXvuGSmTcfU8Z5P+9qlXfPyvB8deddEFXWNz8toD82jNvFte\nb3CqqKwCACzZildWrVkFAGC2VFYBABaqYjcAAACYjMoqAMCSqawCAMA0VFYBAJZKBysAAJiOyioA\nwJKprAIAwIlV1eVV9bGqOlhVrzxO3I9UVauqS040pskqAMCStR26nEBVrSV5bZIfSHJxkhdV1cVH\niXtIkp9N8oEhd2/3LgOoGh7b+urre570hMGxmzd+rGvs3lzaA/f3jT+TsTl5tXf4y3u39B4f+zl7\n1UUXDI593o13do39tic+vDcdjjCX96zd8npjV7o0ycHW2i1JUlVvSvKCJDcdEfdrSX4jyS8MGVRl\nFQCAIc6uquu3XV52xNfPSXLrtuPbtm77mqp6apJzW2v/ceg33b2VVQCAFbCDW1d9obV2wjWmx1JV\ne5L8VpKf7Pl/KqsAAJwKtyc5d9vxga3bvuohSZ6U5N1V9ckkT09y9Yk+ZKWyCgCwZPPZuuq6JBdW\n1QU5PEl9YZL/7qtfbK19OcnZXz2uqncn+SetteuPN6jKKgAAJ621tp7k5UmuTXJzkje31m6sqtdU\n1fMf7LgqqwAASzVwW6md0lq7Jsk1R9z26mPEPnvImCqrAADMlsoqAMCC7eBuAJNQWQUAYLZUVgEA\nlkxlFQAAprF7K6ttvF9DNv/yP4829tpZZ3XFb9x990iZMHdtfX3qFDiOtz3x4V3xv3rLDYNjf+Vx\nT+tLpmp47IjvnTDU2sMeOji27l4bMZN5sGYVAAAmsnsrqwAAq0BlFQAApqGyCgCwVDPrYDUGlVUA\nAGbLZBUAgNmyDAAAYKFq67LKVFYBAJgtlVUAgCXzASsAAJjGzldWx2rr1zNu79hj6sxb+9QVo80l\nA/W0UL3q0+/pGvulT/gHg2M3Dx3qGntWvN5WxsZdXx4c29rGiJnMg3arAAAwkRNOVqtqf1V9sKr+\nvKpurKpf3br9gqr6QFUdrKo/rKrTxk8XAICv03boMpEhldX7kjyntfadSZ6c5PKqenqS30jy2621\nxye5M8kV46UJAMBudMLJajvsv2wd7tu6tCTPSfKWrdvfkOSHRskQAIBjU1lNqmqtqj6S5I4kb0/y\nV0nuaq2tb4XcluSccVIEAGC3GrQbQDv8UbonV9XDkvxxkicM/QZV9bIkL0uS/TnjweQIAMDRNLsB\nfJ3W2l1J3pXkGUkeVlVfneweSHL7Mf7Pla21S1prl+zL6SeVLAAAu8uQ3QAetVVRTVV9Q5LnJrk5\nhyetP7oV9pIkbx0rSQAAjmHF16wOWQbwmCRvqKq1HJ7cvrm19raquinJm6rqXyT5cJKrRswTAIBd\n6IST1dbaR5M85Si335Lk0jGSAgBgGGtWAQBgIoN2Azilevot71kbHFr7+u5Ku+++rvjRjN1/uuMx\nzObq90+eHf3HGcFLv+37u+L/+V/+p8Gxr3ncU3vT6VKnD/8gbvf7uNcbLNLOT1YBADh1Vvz3MMsA\nAACYLZVVAIAF8wErAACYiMoqAMBSTbxh/05QWQUAYLZUVgEAlkxlFQAApqGyCgCwUBW7AQAAwGTm\nXVntaP/Z7tMq9Ki0UGWInra8Sf/zqnf8HmM+x8d+XEbS7n+gK76nheq//dR7usb+6cde1hU/m1bY\nCz337FIqqwAAMI15V1YBADiuaqtdWlVZBQBgtlRWAQCWSgcrAACYjskqAACzZRkAAMCCaQoAAAAT\nUVkFAFgylVUAAJiGyioAwIKt+prVlZms1t6+u9LW10fKBJZnz2n7uuI37+3sg77UvukLzbs9cH9X\nfM/7508/9rKusV99y4e64v/Fd37P4Nj2X/9r19hd7/sLPfewilZmsgoAsCuteGXVmlUAAGZLZRUA\nYKna6q9ZVVkFAGC2VFYBAJZMZRUAAKahsgoAsFAVa1YBAGAyKqsAAEvWVru0qrIKAMBsmawCADBb\nK7MMoKvnc6e1Rz6iK37ji18aKZMke9bGG1sv7F1r8957+/5D7/PQc2vW6vTTB8f2vte+5nFP7Yr/\nmU/cMDj2tRde1DU2K6TnPWgXvP34gBUAAExkZSqrAAC7ToumAAAAMBWVVQCABavNqTMYl8oqAACz\npbIKALBk1qwCAMA0VFYBABbMPqsAADARlVUAgKVqSdpql1ZnPVmtvcPT++KLv6tr7Ef83vsGx47a\nPrVXZ9vKnsewrfjWF5xC2qeulM177pk6ha/paaH65Wse3zX2Q3/wYG86zJX3oF1l1pNVAACOz5pV\nAACYiMoqAMCSqawCAMA0TFYBAJgtywAAABaq4gNWAAAwGZVVAIClam3lmwKorAIAMFsqqwAAC2bN\nKgAATGTWldW2vj449hGvf/+ImSxXz2NYe8d7OvTkATDEQ3/wYFf8m2973+DYHzvwjN50YDoqqwAA\nMI1ZV1YBADg+a1YBAGAiKqsAAEvVkmyudmlVZRUAgNlSWQUAWLLVLqyqrAIAMF8qqwAAC2Y3AAAA\nmIjJKgAAs7WzywCqr6VnV4vOtswa+N/8o76Wfo/6N8PbBSYjPt6delu5as/KUHN5jjN/PS1UP/GG\np3aNfeFLPtSbzixos70iFjoHGkplFQCA2fIBKwCABfMBKwAAGKCqLq+qj1XVwap65VG+/nNVdVNV\nfbSq3lFVjz3RmCarAABL1XbwcgJVtZbktUl+IMnFSV5UVRcfEfbhJJe01r4jyVuS/OaJxjVZBQDg\nVLg0ycHW2i2ttfuTvCnJC7YHtNbe1Vo7tHX4/iQHTjSoNasAAAtVSWrndgM4u6qu33Z8ZWvtym3H\n5yS5ddvxbUm++zjjXZHk/z3RNzVZBQBgiC+01i45FQNV1U8kuSTJ95wo1mQVAGDJNqdO4GtuT3Lu\ntuMDW7d9nar6viT/LMn3tNbuO9Gg1qwCAHAqXJfkwqq6oKpOS/LCJFdvD6iqpyT535M8v7V2x5BB\nVVYBABZsB9esHldrbb2qXp7k2iRrSV7XWruxql6T5PrW2tVJ/lWSb0zyf1dVkny6tfb8441rsgoA\nwCnRWrsmyTVH3Pbqbde/r3fMnZ2stvF6BS+19/w3/e51XfG9vzvN5X7OJY9d5fBvrMPM5LfyB6Nt\nbEydAgtRp58+OPbCl3yoa+z/7VPvGRz7Pzz2sq6xx+S9eQUM3AN1yaxZBQBgtiwDAABYrLbov44N\nobIKAMBsqawCACxYrXZhVWUVAID5MlkFAGC2LAMAAFgyH7ACAIBpqKwCACxVS2pz6iTGpbIKAMBs\nrUxldakt45aa99j2nHHG4NjNQ4dGzGTBVnwN09fslvvJSWv33z/a2D0tVF//6eGtWZPkJ8+bT3tW\nZmrF3wdVVgEAmK2VqawCAOxKq11YVVkFAGC+VFYBABasrFkFAIBpqKwCACyZyioAAExDZRUAYKla\nEh2sAABgGiqrAAALVWl2AwAAgKmorA5Qp5/eFd/uu2+kTOaVy5g2Dx2aOoWvOfjGpwyOffyLPzxi\nJpysOb1+5pTLrjGT6tNPnndZV/xXXvj0rviHvOn9XfFLtPawhw6OrbvXRsyEnWCyCgCwZDP5RWws\ng5cBVNVaVX24qt62dXxBVX2gqg5W1R9W1WnjpQkAwG7Us2b1Z5PcvO34N5L8dmvt8UnuTHLFqUwM\nAIABWtuZy0QGTVar6kCSf5jk320dV5LnJHnLVsgbkvzQGAkCALB7DV2z+jtJfjHJQ7aOH5nkrtba\n+tbxbUnOOcW5AQBwPJoCJFXEYOUGAAAJE0lEQVT1vCR3tNZueDDfoKpeVlXXV9X1D8SnWQEAGG5I\nZfVZSZ5fVT+YZH+Ss5L86yQPq6q9W9XVA0luP9p/bq1dmeTKJDmrHrHaH1cDANhhu74pQGvtVa21\nA62185O8MMk7W2s/nuRdSX50K+wlSd46WpYAAOxKJ9PB6peS/FxVHczhNaxXnZqUAAAYbMV3A+hq\nCtBae3eSd29dvyXJpac+JQAAOEwHKwCAxZq26rkTdu1ktbcv91z09gffe+6BwbGf/InzusY+8C/f\n2xW/VI9/8YcHx+r3Pm9jP94959+5Xy09535P5/vEQ970/q74L17xjMGxj7zqfV1j91g766yu+I27\n7x4ee9eXB8e2ttGVB/OzayerAACL17LyldWT+YAVAACMSmUVAGDJdnsHKwAAmIrJKgAAs2UZAADA\ngu36dqsAADAVlVUAgCVTWQUAgGmorAIALFVLsrnaldWVmazW3r67Mmqrw6rxxu4s9a/fetvg2AP/\ncngsR7dbWmjuOeOMrvjNQ4dGymRedsv55+/qOfcbIz9Pelqo/tpfX9c19q886dmDY3vap8LxrMxk\nFQBg92nWrAIAwFRUVgEAlkxlFQAApqGyCgCwZCqrAAAwDZVVAICl2gX7rKqsAgAwWyqrAACL1ZK2\nOXUSo1JZBQBgtkxWAQCYrZVZBtDW16dO4W+t+BYSs1PVF+/8nLTNQ4f6/sOetcGhtTY8NknaA/f3\n5bIbdDzeSZLNjXHyYPZ++XGXdsX/7qeuHRz70vMu60um573c+/jXW/HHQ2UVAIDZWpnKKgDArmPr\nKgAAmI7KKgDAklmzCgAA01BZBQBYMpVVAACYhsoqAMBiNZVVAACYisoqAMBStSSbm1NnMSqT1QH2\n7N/fFb95770jZdJv7YnfOjj25lec1TX2RS+9rjedcaz4nz9WQkc7z6b158nzGB5Vz3v5nN7He+09\n55sHx67f/pmusXtaqK4/52ldY+995w1d8eweJqsAAEu24kUba1YBAJgtlVUAgCVTWQUAgGmYrAIA\nMFuWAQAALFZLNi0DAACASaisAgAsVUtaW+2mACqrAADMlsoqAMCSWbMKAADTmHVltfYOT6+tr4+W\nx6x6RFd1hW/c+LHBsRe9tDeZhep8DFd9s+XdpOc9JXkQ7yu9z60Oe04/fXDs5n33dY299shHDI7d\n+MIXu8buNeb7fu/jslTrt39mvME7nuN733lD19DPu/HOwbH/8TsfNXzg8aYH87HiP6dUVgEAmK1Z\nV1YBADiO1pJNuwEAAMAkVFYBAJbMmlUAAJiGyioAwII1a1YBAGAaKqsAAIvVrFkFAICpmKwCADBb\ns14GMGYL1R6jt2gcU0/7xxX/M8LX9N7PpT6GS817RKO/Nkd8HOuhZw0P/vwdXWOP3UK1R9vYGHHw\njvMzYuvcRb/eRsz9bU98+ODYV/3VhwbH/qPnH3ow6SxHS7K54OfUACqrAADM1qwrqwAAnECzdRUA\nAExCZRUAYKFakmbNKgAATENlFQBgqVqzZhUAAKZisgoAsGBts+3IZYiquryqPlZVB6vqlUf5+ulV\n9YdbX/9AVZ1/ojFNVgEAOGlVtZbktUl+IMnFSV5UVRcfEXZFkjtba49P8ttJfuNE45qsAgAsWdvc\nmcuJXZrkYGvtltba/UnelOQFR8S8IMkbtq6/Jcn3Vh2/ZZzJKgAAp8I5SW7ddnzb1m1HjWmtrSf5\ncpJHHm/QHd0N4Cu58wt/0t7yqaN86ewkX9jJXLo8MHUC28x/K7V5n8sHY/6P+dGdmrxX73wu1edO\nySjzP59zeb3NJY9jm/+5HNGfPK4r/LEjpTELX8md1/5Je8vZO/Tt9lfV9duOr2ytXTn2N93RyWpr\n7VFHu72qrm+tXbKTuTAO53K1OJ+rxflcHc4lX9Vau3zqHLa5Pcm5244PbN12tJjbqmpvkocm+eLx\nBrUMAACAU+G6JBdW1QVVdVqSFya5+oiYq5O8ZOv6jyZ5Z2vtuH/L0BQAAICT1lpbr6qXJ7k2yVqS\n17XWbqyq1yS5vrV2dZKrkryxqg4m+VIOT2iPay6T1dHXO7BjnMvV4nyuFudzdTiXzFJr7Zok1xxx\n26u3Xb83yX/bM2adoPIKAACTsWYVAIDZmnSyeqKWXMxbVb2uqu6oqr/cdtsjqurtVfWJrX8fPmWO\nDFNV51bVu6rqpqq6sap+dut253OBqmp/VX2wqv5863z+6tbtF2y1Nzy41e7wtKlzZZiqWquqD1fV\n27aOnUt2jckmqwNbcjFvr09y5JYZr0zyjtbahUnesXXM/K0n+fnW2sVJnp7kZ7Zej87nMt2X5Dmt\nte9M8uQkl1fV03O4reFvb7U5vDOH2x6yDD+b5OZtx84lu8aUldUhLbmYsdban+bwJ/m2295G7Q1J\nfmhHk+JBaa19trX2oa3rX8nhH4rnxPlcpHbYf9k63Ld1aUmek8PtDRPnczGq6kCSf5jk320dV5xL\ndpEpJ6tDWnKxPI9urX126/rnkjx6ymToV1XnJ3lKkg/E+VysrT8bfyTJHUnenuSvkty11d4w8Z67\nJL+T5BeTfLU5+yPjXLKL+IAVo9na5Nd2EwtSVd+Y5I+SvKK1dvf2rzmfy9Ja22itPTmHO8hcmuQJ\nE6fEg1BVz0tyR2vthqlzgalMuc/qkJZcLM/nq+oxrbXPVtVjcriqwwJU1b4cnqj+fmvt32/d7Hwu\nXGvtrqp6V5JnJHlYVe3dqsh5z12GZyV5flX9YJL9Sc5K8q/jXLKLTFlZHdKSi+XZ3kbtJUneOmEu\nDLS1Bu6qJDe31n5r25eczwWqqkdV1cO2rn9Dkufm8Drkd+Vwe8PE+VyE1tqrWmsHWmvn5/DPyXe2\n1n48ziW7yKRNAbZ+U/yd/G1Lrv95smToVlV/kOTZSc5O8vkkv5LkPyR5c5LzknwqyY+11o78EBYz\nU1WXJflPSf4if7su7p/m8LpV53Nhquo7cvhDN2s5XJR4c2vtNVX1uBz+MOsjknw4yU+01u6bLlN6\nVNWzk/yT1trznEt2Ex2sAACYLR+wAgBgtkxWAQCYLZNVAABmy2QVAIDZMlkFAGC2TFYBAJgtk1UA\nAGbLZBUAgNn6/wFVHMd/Ug+EkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1824c50da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "C = confusion_matrix(ytsp,yhatp)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "#Csum = np.sum(C,1)\n",
    "#C = C / Csum[None,:]\n",
    "C = normalize(C, norm='l1', axis=1)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(np.array_str(C, precision=3, suppress_small=True))\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(C, interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84325397 0.73854962 0.95687885 0.99441341 0.94639175 0.94305239\n",
      " 0.97797357 0.97194389 0.96082474 0.975      0.78409091 0.86440678\n",
      " 0.90769231 0.89230769 0.85483871 0.78787879 0.74       0.87804878\n",
      " 0.31279621 0.77333333 0.90909091 0.56610169 0.95679012 0.93023256\n",
      " 0.47150259 0.95092025 0.83333333 0.86440678 0.82857143 0.90849673\n",
      " 0.94977169 0.90322581 0.94117647 0.9047619  0.875      0.66233766\n",
      " 0.96969697 0.57142857 0.93835616 0.95396419 0.27906977 0.31914894\n",
      " 0.9109589  0.94565217 0.2        0.82938389 0.89495798]\n",
      "(array([ 1, 10, 15, 16, 18, 19, 21, 24, 35, 37, 40, 41, 44]),)\n"
     ]
    }
   ],
   "source": [
    "Cd=C.diagonal()\n",
    "print(Cd)\n",
    "print(np.where(Cd<0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a loop to find the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=16,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 29s 640us/step - loss: 0.7548 - acc: 0.7863 - val_loss: 0.4637 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 28s 614us/step - loss: 0.3944 - acc: 0.8632 - val_loss: 0.3924 - val_acc: 0.8644\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 28s 606us/step - loss: 0.3256 - acc: 0.8814 - val_loss: 0.3784 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 29s 625us/step - loss: 0.2899 - acc: 0.8925 - val_loss: 0.3865 - val_acc: 0.8741\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 28s 608us/step - loss: 0.2595 - acc: 0.9013 - val_loss: 0.3888 - val_acc: 0.8737\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 28s 611us/step - loss: 0.2366 - acc: 0.9104 - val_loss: 0.3883 - val_acc: 0.8748\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 28s 612us/step - loss: 0.2188 - acc: 0.9144 - val_loss: 0.3990 - val_acc: 0.8720\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 28s 619us/step - loss: 0.2039 - acc: 0.9202 - val_loss: 0.3967 - val_acc: 0.8731\n",
      "nc=16,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 33s 718us/step - loss: 0.9367 - acc: 0.7661 - val_loss: 0.4698 - val_acc: 0.8445\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 32s 695us/step - loss: 0.4140 - acc: 0.8562 - val_loss: 0.4129 - val_acc: 0.8624\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 732us/step - loss: 0.3476 - acc: 0.8755 - val_loss: 0.4079 - val_acc: 0.8610\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 32s 705us/step - loss: 0.3040 - acc: 0.8877 - val_loss: 0.3806 - val_acc: 0.8751\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2752 - acc: 0.8972 - val_loss: 0.3831 - val_acc: 0.8743\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 32s 697us/step - loss: 0.2509 - acc: 0.9032 - val_loss: 0.3857 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 32s 698us/step - loss: 0.2324 - acc: 0.9101 - val_loss: 0.3840 - val_acc: 0.8749\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2162 - acc: 0.9161 - val_loss: 0.3982 - val_acc: 0.8698\n",
      "nc=16,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 1.1055 - acc: 0.7681 - val_loss: 0.4701 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 34s 735us/step - loss: 0.4161 - acc: 0.8578 - val_loss: 0.4386 - val_acc: 0.8524\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 736us/step - loss: 0.3495 - acc: 0.8747 - val_loss: 0.3977 - val_acc: 0.8678\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 34s 742us/step - loss: 0.3093 - acc: 0.8872 - val_loss: 0.3826 - val_acc: 0.8720\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2801 - acc: 0.8947 - val_loss: 0.3785 - val_acc: 0.8744\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 36s 785us/step - loss: 0.2580 - acc: 0.9020 - val_loss: 0.3776 - val_acc: 0.8766\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 0.2370 - acc: 0.9081 - val_loss: 0.3822 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2226 - acc: 0.9134 - val_loss: 0.3790 - val_acc: 0.8799\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7467 - acc: 0.7870 - val_loss: 0.4430 - val_acc: 0.8529\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.3786 - acc: 0.8685 - val_loss: 0.4073 - val_acc: 0.8492\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3143 - acc: 0.8846 - val_loss: 0.3723 - val_acc: 0.8758\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2737 - acc: 0.8976 - val_loss: 0.3672 - val_acc: 0.8773\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2433 - acc: 0.9068 - val_loss: 0.3808 - val_acc: 0.8786\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2220 - acc: 0.9125 - val_loss: 0.3756 - val_acc: 0.8825\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2002 - acc: 0.9217 - val_loss: 0.3798 - val_acc: 0.8818\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.1863 - acc: 0.9258 - val_loss: 0.3988 - val_acc: 0.8786\n",
      "nc=32,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8393 - acc: 0.7738 - val_loss: 0.4995 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3981 - acc: 0.8619 - val_loss: 0.4007 - val_acc: 0.8703\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3368 - acc: 0.8789 - val_loss: 0.4001 - val_acc: 0.8655\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.2970 - acc: 0.8907 - val_loss: 0.3835 - val_acc: 0.8703\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2661 - acc: 0.8997 - val_loss: 0.4000 - val_acc: 0.8728\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2428 - acc: 0.9067 - val_loss: 0.3850 - val_acc: 0.8765\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2253 - acc: 0.9131 - val_loss: 0.3970 - val_acc: 0.8721\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2074 - acc: 0.9179 - val_loss: 0.3977 - val_acc: 0.8760\n",
      "nc=32,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 1.0933 - acc: 0.7525 - val_loss: 0.5016 - val_acc: 0.8304\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.4315 - acc: 0.8511 - val_loss: 0.4396 - val_acc: 0.8518\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.3682 - acc: 0.8710 - val_loss: 0.3942 - val_acc: 0.8636\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.3274 - acc: 0.8834 - val_loss: 0.4291 - val_acc: 0.8579\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 1ms/step - loss: 0.3016 - acc: 0.8899 - val_loss: 0.4213 - val_acc: 0.8612\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2770 - acc: 0.8971 - val_loss: 0.3989 - val_acc: 0.8724\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2579 - acc: 0.9027 - val_loss: 0.3909 - val_acc: 0.8725\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2439 - acc: 0.9063 - val_loss: 0.4026 - val_acc: 0.8754\n",
      "nc=48,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.7368 - acc: 0.7928 - val_loss: 0.4215 - val_acc: 0.8560\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.3713 - acc: 0.8690 - val_loss: 0.3846 - val_acc: 0.8729\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.3077 - acc: 0.8882 - val_loss: 0.3720 - val_acc: 0.8741\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.2687 - acc: 0.8981 - val_loss: 0.3581 - val_acc: 0.8798\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 89s 2ms/step - loss: 0.2377 - acc: 0.9083 - val_loss: 0.3744 - val_acc: 0.8811\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.2139 - acc: 0.9153 - val_loss: 0.3856 - val_acc: 0.8696\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1950 - acc: 0.9222 - val_loss: 0.3846 - val_acc: 0.8770\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1801 - acc: 0.9273 - val_loss: 0.4065 - val_acc: 0.8778\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.8842 - acc: 0.7788 - val_loss: 0.4717 - val_acc: 0.8389\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3909 - acc: 0.8625 - val_loss: 0.3847 - val_acc: 0.8690\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3213 - acc: 0.8834 - val_loss: 0.3819 - val_acc: 0.8684\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2832 - acc: 0.8954 - val_loss: 0.3800 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2515 - acc: 0.9045 - val_loss: 0.3748 - val_acc: 0.8773\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2298 - acc: 0.9128 - val_loss: 0.3798 - val_acc: 0.8769\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2072 - acc: 0.9190 - val_loss: 0.3862 - val_acc: 0.8799\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.1919 - acc: 0.9245 - val_loss: 0.3916 - val_acc: 0.8775\n",
      "nc=48,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 1.1306 - acc: 0.7643 - val_loss: 0.4796 - val_acc: 0.8355\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.4054 - acc: 0.8600 - val_loss: 0.4278 - val_acc: 0.8609\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.3385 - acc: 0.8786 - val_loss: 0.3986 - val_acc: 0.8627\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2957 - acc: 0.8904 - val_loss: 0.3771 - val_acc: 0.8767\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2683 - acc: 0.8990 - val_loss: 0.3837 - val_acc: 0.8752\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.2419 - acc: 0.9078 - val_loss: 0.3949 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.2225 - acc: 0.9138 - val_loss: 0.4025 - val_acc: 0.8731\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2052 - acc: 0.9203 - val_loss: 0.3930 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([384,512,640])\n",
    "nconvs=np.array([16,32,48])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8748 0.8751 0.8799]\n",
      " [0.8825 0.8765 0.8754]\n",
      " [0.8811 0.8799 0.8785]]\n",
      "(3, 3)\n",
      "the best nconvs is 32,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8825\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go further from above to find better nconvs and n node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=24,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.6819 - acc: 0.7966 - val_loss: 0.4265 - val_acc: 0.8564\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 978us/step - loss: 0.3733 - acc: 0.8688 - val_loss: 0.3767 - val_acc: 0.8716\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 45s 981us/step - loss: 0.3129 - acc: 0.8860 - val_loss: 0.3692 - val_acc: 0.8732\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 46s 996us/step - loss: 0.2722 - acc: 0.8973 - val_loss: 0.3689 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2435 - acc: 0.9066 - val_loss: 0.3786 - val_acc: 0.8713\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2218 - acc: 0.9136 - val_loss: 0.3775 - val_acc: 0.8767\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2036 - acc: 0.9215 - val_loss: 0.3817 - val_acc: 0.8789\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.1874 - acc: 0.9259 - val_loss: 0.3933 - val_acc: 0.8784\n",
      "nc=24,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.7690 - acc: 0.7808 - val_loss: 0.5000 - val_acc: 0.8369\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 983us/step - loss: 0.3935 - acc: 0.8620 - val_loss: 0.4044 - val_acc: 0.8660\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 46s 991us/step - loss: 0.3333 - acc: 0.8807 - val_loss: 0.3885 - val_acc: 0.8681\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.2929 - acc: 0.8925 - val_loss: 0.3682 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.2651 - acc: 0.9006 - val_loss: 0.3882 - val_acc: 0.8704\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 46s 994us/step - loss: 0.2411 - acc: 0.9090 - val_loss: 0.3714 - val_acc: 0.8795\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 46s 999us/step - loss: 0.2201 - acc: 0.9150 - val_loss: 0.3832 - val_acc: 0.8778\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 46s 1000us/step - loss: 0.2071 - acc: 0.9185 - val_loss: 0.3901 - val_acc: 0.8772\n",
      "nc=24,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.8109 - acc: 0.7831 - val_loss: 0.4671 - val_acc: 0.8441\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3964 - acc: 0.8613 - val_loss: 0.3873 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3324 - acc: 0.8814 - val_loss: 0.3888 - val_acc: 0.8712\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2894 - acc: 0.8936 - val_loss: 0.3917 - val_acc: 0.8714\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2648 - acc: 0.8996 - val_loss: 0.3812 - val_acc: 0.8801\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2417 - acc: 0.9075 - val_loss: 0.3808 - val_acc: 0.8787\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2232 - acc: 0.9136 - val_loss: 0.3913 - val_acc: 0.8702\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2078 - acc: 0.9188 - val_loss: 0.4019 - val_acc: 0.8776\n",
      "nc=32,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7116 - acc: 0.7918 - val_loss: 0.4572 - val_acc: 0.8496\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3812 - acc: 0.8660 - val_loss: 0.3714 - val_acc: 0.8718\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3205 - acc: 0.8832 - val_loss: 0.3627 - val_acc: 0.8745\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2816 - acc: 0.8960 - val_loss: 0.3585 - val_acc: 0.8771\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2538 - acc: 0.9049 - val_loss: 0.3656 - val_acc: 0.8797\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2294 - acc: 0.9112 - val_loss: 0.3647 - val_acc: 0.8806\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2098 - acc: 0.9179 - val_loss: 0.3715 - val_acc: 0.8808\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.1939 - acc: 0.9227 - val_loss: 0.3870 - val_acc: 0.8790\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.8128 - acc: 0.7774 - val_loss: 0.4525 - val_acc: 0.8467\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3975 - acc: 0.8618 - val_loss: 0.3925 - val_acc: 0.8663\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3298 - acc: 0.8815 - val_loss: 0.3806 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2913 - acc: 0.8930 - val_loss: 0.3850 - val_acc: 0.8708\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2621 - acc: 0.9032 - val_loss: 0.3690 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2389 - acc: 0.9090 - val_loss: 0.3833 - val_acc: 0.8792\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2200 - acc: 0.9156 - val_loss: 0.3751 - val_acc: 0.8794\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2038 - acc: 0.9205 - val_loss: 0.3897 - val_acc: 0.8780\n",
      "nc=32,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8310 - acc: 0.7800 - val_loss: 0.4775 - val_acc: 0.8469\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3909 - acc: 0.8627 - val_loss: 0.4053 - val_acc: 0.8635\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3223 - acc: 0.8837 - val_loss: 0.3899 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2803 - acc: 0.8947 - val_loss: 0.3653 - val_acc: 0.8786\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2470 - acc: 0.9056 - val_loss: 0.3732 - val_acc: 0.8790\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2245 - acc: 0.9127 - val_loss: 0.3889 - val_acc: 0.8734\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2038 - acc: 0.9200 - val_loss: 0.3866 - val_acc: 0.8780\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.1895 - acc: 0.9256 - val_loss: 0.4003 - val_acc: 0.8761\n",
      "nc=40,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.7113 - acc: 0.7892 - val_loss: 0.4624 - val_acc: 0.8470\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 76s 2ms/step - loss: 0.3717 - acc: 0.8683 - val_loss: 0.3713 - val_acc: 0.8695\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3081 - acc: 0.8891 - val_loss: 0.3742 - val_acc: 0.8725\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2702 - acc: 0.8975 - val_loss: 0.3625 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2395 - acc: 0.9074 - val_loss: 0.3652 - val_acc: 0.8756\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2171 - acc: 0.9156 - val_loss: 0.3814 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1967 - acc: 0.9228 - val_loss: 0.3803 - val_acc: 0.8768\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1831 - acc: 0.9267 - val_loss: 0.3785 - val_acc: 0.8786\n",
      "nc=40,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.7509 - acc: 0.7862 - val_loss: 0.4441 - val_acc: 0.8465\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3778 - acc: 0.8675 - val_loss: 0.3792 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3165 - acc: 0.8853 - val_loss: 0.3916 - val_acc: 0.8672\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2771 - acc: 0.8974 - val_loss: 0.3649 - val_acc: 0.8750\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2486 - acc: 0.9049 - val_loss: 0.3561 - val_acc: 0.8791\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2245 - acc: 0.9128 - val_loss: 0.3640 - val_acc: 0.8813\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2052 - acc: 0.9194 - val_loss: 0.3761 - val_acc: 0.8823\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1915 - acc: 0.9252 - val_loss: 0.3983 - val_acc: 0.8824\n",
      "nc=40,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.8051 - acc: 0.7795 - val_loss: 0.5280 - val_acc: 0.8251\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.3920 - acc: 0.8632 - val_loss: 0.3846 - val_acc: 0.8642\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.3228 - acc: 0.8827 - val_loss: 0.3765 - val_acc: 0.8719\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2829 - acc: 0.8955 - val_loss: 0.3778 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2494 - acc: 0.9052 - val_loss: 0.3731 - val_acc: 0.8770\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2253 - acc: 0.9132 - val_loss: 0.3756 - val_acc: 0.8777\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2030 - acc: 0.9196 - val_loss: 0.3984 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1875 - acc: 0.9262 - val_loss: 0.4055 - val_acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([320,384,448])\n",
    "nconvs=np.array([24,32,40])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8789 0.8795 0.8801]\n",
      " [0.8808 0.8794 0.879 ]\n",
      " [0.8789 0.8824 0.8777]]\n",
      "(3, 3)\n",
      "the best nconvs is 40,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8824\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the best parameters in adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlrate=0.5\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 15.3239 - acc: 0.0481 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3373 - val_acc: 0.0484\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "nlrate=0.1\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 1.0963 - acc: 0.7346 - val_loss: 0.9226 - val_acc: 0.7157\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4747 - acc: 0.8390 - val_loss: 0.4604 - val_acc: 0.8503\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4100 - acc: 0.8592 - val_loss: 0.4354 - val_acc: 0.8538\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.3737 - acc: 0.8695 - val_loss: 0.4122 - val_acc: 0.8661\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.3481 - acc: 0.8754 - val_loss: 0.4121 - val_acc: 0.8616\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3329 - acc: 0.8828 - val_loss: 0.4099 - val_acc: 0.8646\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3160 - acc: 0.8868 - val_loss: 0.4043 - val_acc: 0.8681\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3060 - acc: 0.8900 - val_loss: 0.3969 - val_acc: 0.8676\n",
      "nlrate=0.05\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 84s 2ms/step - loss: 0.7607 - acc: 0.7818 - val_loss: 0.5171 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3861 - acc: 0.8643 - val_loss: 0.3910 - val_acc: 0.8650\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3209 - acc: 0.8834 - val_loss: 0.3837 - val_acc: 0.8688\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 80s 2ms/step - loss: 0.2822 - acc: 0.8963 - val_loss: 0.4366 - val_acc: 0.8546\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2535 - acc: 0.9035 - val_loss: 0.3835 - val_acc: 0.8735\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.2317 - acc: 0.9097 - val_loss: 0.3803 - val_acc: 0.8773\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2111 - acc: 0.9179 - val_loss: 0.3918 - val_acc: 0.8766\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1954 - acc: 0.9233 - val_loss: 0.3879 - val_acc: 0.8778\n",
      "nlrate=0.01\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.6256 - acc: 0.8043 - val_loss: 0.4380 - val_acc: 0.8444\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3671 - acc: 0.8697 - val_loss: 0.3794 - val_acc: 0.8731\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2971 - acc: 0.8896 - val_loss: 0.3908 - val_acc: 0.8694\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.2508 - acc: 0.9035 - val_loss: 0.3646 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.2177 - acc: 0.9149 - val_loss: 0.3810 - val_acc: 0.8764\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.1913 - acc: 0.9236 - val_loss: 0.3842 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1673 - acc: 0.9328 - val_loss: 0.3969 - val_acc: 0.8751\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1506 - acc: 0.9382 - val_loss: 0.4106 - val_acc: 0.8787\n",
      "nlrate=0.005\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.6163 - acc: 0.8041 - val_loss: 0.4270 - val_acc: 0.8538\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3637 - acc: 0.8706 - val_loss: 0.3810 - val_acc: 0.8679\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2955 - acc: 0.8908 - val_loss: 0.3866 - val_acc: 0.8659\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2517 - acc: 0.9033 - val_loss: 0.3508 - val_acc: 0.8832\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2184 - acc: 0.9144 - val_loss: 0.3793 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1919 - acc: 0.9229 - val_loss: 0.4034 - val_acc: 0.8738\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1701 - acc: 0.9308 - val_loss: 0.3874 - val_acc: 0.8777\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1522 - acc: 0.9385 - val_loss: 0.3983 - val_acc: 0.8771\n"
     ]
    }
   ],
   "source": [
    "history1=[]\n",
    "nlrate=np.array([0.5,0.1,0.05,0.01,0.005])\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(convmax, (3, 3), \n",
    "                     padding='valid', \n",
    "                     input_shape=x_train.shape[1:],\n",
    "                     activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(convmax, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nodemax, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "    #model.add(BatchNormalization())\n",
    "    decay = lrate/epochs\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    print('nlrate={}'.format(lrate))\n",
    "    hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,\n",
    "                           validation_data=(x_test, y_test),shuffle=True)\n",
    "    history1.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0485]\n",
      " [0.8681]\n",
      " [0.8778]\n",
      " [0.8789]\n",
      " [0.8832]]\n",
      "the best nlrate is 0.005\n",
      "the maximum val_accuracy is 0.8832\n"
     ]
    }
   ],
   "source": [
    "hh=np.zeros((5,1))\n",
    "for n in range(5):\n",
    "    hh[n]=np.max(history1[n].history['val_acc'])\n",
    "print(hh)\n",
    "c=0\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    if hh[i]>c:\n",
    "        c=hh[i]\n",
    "        lratemax=lrate\n",
    "print('the best nlrate is {}'.format(lratemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(hh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
