{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load emnist samples and adjust data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "def load_emnist(file_path='emnist-bymerge.mat'):\n",
    "    \"\"\"\n",
    "    Loads training and test data with ntr and nts training and test samples\n",
    "    The `file_path` is the location of the `eminst-balanced.mat`.\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Load the MATLAB file\n",
    "    mat = scipy.io.loadmat(file_path)\n",
    "    \n",
    "    # Get the training data\n",
    "    Xtr = mat['dataset'][0][0][0][0][0][0][:]\n",
    "    ntr = Xtr.shape[0]\n",
    "    ytr = mat['dataset'][0][0][0][0][0][1][:].reshape(ntr).astype(int)\n",
    "    \n",
    "    # Get the test data\n",
    "    Xts = mat['dataset'][0][0][1][0][0][0][:]\n",
    "    nts = Xts.shape[0]\n",
    "    yts = mat['dataset'][0][0][1][0][0][1][:].reshape(nts).astype(int)\n",
    "    \n",
    "    print(\"%d training samples, %d test samples loaded\" % (ntr, nts))\n",
    "\n",
    "    return [Xtr, Xts, ytr, yts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697932 training samples, 116323 test samples loaded\n"
     ]
    }
   ],
   "source": [
    "Xtr, Xts, ytr, yts = load_emnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(697932, 784) (116323, 784) (697932,) (116323,)\n"
     ]
    }
   ],
   "source": [
    "print(Xtr.shape,Xts.shape,ytr.shape,yts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrd=np.reshape(Xtr,(697932,28,28),order='F')\n",
    "Xtsd=np.reshape(Xts,(116323,28,28),order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10e4982b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1814d58518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(Xtrd[np.random.randint(1,20000),:,:],cmap='Greys_r')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(Xtsd[np.random.randint(1,10000),:,:],cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntr = 46000\n",
    "nts = 10000\n",
    "\n",
    "# TODO: proper decide the number of samples and the ratio between dig and let\n",
    "\n",
    "# Create sub-sampled training and test data\n",
    "nsamp = Xtr.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xtr1 = Xtrd[Iperm[:ntr],:,:]\n",
    "ytr1 = ytr[Iperm[:ntr]]\n",
    "nsamp = Xts.shape[0]\n",
    "Iperm = np.random.permutation(nsamp)\n",
    "Xts1 = Xtsd[Iperm[:nts],:,:]\n",
    "yts1 = yts[Iperm[:nts]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28)\n",
      "[[37  0  0  0  0]\n",
      " [37  0  0  0  0]\n",
      " [37  0  0  0  0]\n",
      " [21  0  0  0  0]\n",
      " [ 4  0  0  0  0]]\n",
      "[21  5  8 ...  2  8 29]\n"
     ]
    }
   ],
   "source": [
    "print(Xtr1.shape)\n",
    "print(Xtr1[233,15:20,15:20])\n",
    "print(ytr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model #save and load models\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46000, 28, 28, 1) (10000, 28, 28, 1) (46000, 1) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = Xtr1.astype('float32')\n",
    "x_test = Xts1.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train=x_train.reshape((ntr,28,28,1))\n",
    "x_test=x_test.reshape((nts,28,28,1))\n",
    "y_train=ytr1.reshape((len(ytr1),1))\n",
    "y_test=yts1.reshape((len(yts1),1))\n",
    "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18160f5c88>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoP4fJ1CefUs63LwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18153957f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myxt=np.zeros((28,28))\n",
    "myxt[:,:]=x_test[np.random.randint(1,10000),:,:,0]\n",
    "plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum=15\\nfor m in range(len(y_train)):\\n    r=np.where(y_train==num)\\n    x_train[r,:,:,0]=0\\nfor m in range(len(y_test)):\\n    r=np.where(y_test==num)\\n    x_test[r,:,:,0]=0\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove the confusing data: F\n",
    "'''\n",
    "num=15\n",
    "for m in range(len(y_train)):\n",
    "    r=np.where(y_train==num)\n",
    "    x_train[r,:,:,0]=0\n",
    "for m in range(len(y_test)):\n",
    "    r=np.where(y_test==num)\n",
    "    x_test[r,:,:,0]=0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADTCAYAAACRDeixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1814d42dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# find the respective letter\n",
    "num=21\n",
    "for m in range(10):\n",
    "    r=np.where(y_train==num)[0][m]\n",
    "    myxt[:,:]=x_train[r,:,:,0]\n",
    "    plt.subplot(2,5,m+1)\n",
    "    plt.imshow(myxt,cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 47\n"
     ]
    }
   ],
   "source": [
    "num_classes = 47\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('Number of classes:', y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 8\n",
    "lrate = 0.05\n",
    "decay = lrate/epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: 36/62 channels?\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), \n",
    "                 padding='valid', \n",
    "                 input_shape=x_train.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 800)               3200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               410112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                24111     \n",
      "=================================================================\n",
      "Total params: 449,167\n",
      "Trainable params: 446,479\n",
      "Non-trainable params: 2,688\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# initiate Adam optimizer\n",
    "opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "\n",
    "# Let's train the model using Adam\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.6302 - acc: 0.8027 - val_loss: 0.4271 - val_acc: 0.8616\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.3775 - acc: 0.8687 - val_loss: 0.4012 - val_acc: 0.8638\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.3123 - acc: 0.8860 - val_loss: 0.4087 - val_acc: 0.8629\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2725 - acc: 0.8980 - val_loss: 0.3972 - val_acc: 0.8673\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2424 - acc: 0.9065 - val_loss: 0.4307 - val_acc: 0.8664\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2147 - acc: 0.9152 - val_loss: 0.4057 - val_acc: 0.8675\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.1971 - acc: 0.9217 - val_loss: 0.4199 - val_acc: 0.8656\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.1821 - acc: 0.9266 - val_loss: 0.4359 - val_acc: 0.8622\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "seed=7\n",
    "k=1\n",
    "c=4\n",
    "# Fit the model\n",
    "np.random.seed(seed)\n",
    "class_weight={0:k*1.3,1:k/1.8,2:k/1.1,3:k,4:k,5:k*1.5,6:k,7:k,8:k*2,9:k*2,10:k,11:k,12:k,13:k,14:k,15:k*2,16:k,17:k,18:k,19:k,20:k,21:k*2,\n",
    "              22:k,23:k,24:k*2,25:k,26:k,27:k,28:k,29:k,30:k,31:k,32:k,33:k,34:k,35:k*3,36:k,37:k*2,38:k,39:k,40:k*10,41:k*8,42:k,\n",
    "              43:k,44:k*9,45:k,46:k}\n",
    "hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(x_test, y_test),shuffle=True,\n",
    "                       #class_weight='auto'\n",
    "                      )\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")\n",
    "model = load_model(\"emnist_BatchNormalization_47_classwight_kernel2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prediction based on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23515804e-07 3.72122955e-08 3.33188154e-07 5.02231833e-11\n",
      "  2.66159816e-07 7.74245723e-10 2.93974267e-06 8.25428259e-09\n",
      "  1.30503670e-08 1.94423663e-07 3.92194522e-07 9.07756856e-08\n",
      "  1.39135006e-11 5.67333132e-08 9.90347626e-09 1.37257697e-12\n",
      "  2.46301897e-06 1.12099215e-05 5.48474244e-09 7.67718422e-09\n",
      "  4.68045869e-07 3.39324941e-08 7.82226408e-08 5.11993903e-07\n",
      "  8.78020046e-10 9.56579260e-12 2.21214600e-06 4.08123579e-09\n",
      "  1.49083142e-11 3.07458663e-08 9.69479024e-01 2.14808570e-05\n",
      "  3.04293744e-02 2.45321161e-07 4.07663583e-06 3.00070724e-08\n",
      "  8.79328638e-07 2.48769254e-08 9.38984249e-06 4.17549190e-10\n",
      "  1.87652158e-10 1.93214706e-08 3.35925979e-05 6.83860790e-08\n",
      "  2.19290456e-07 5.72550236e-11 3.96266930e-09]]\n",
      "30\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "u\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfiCo/wdWrFdE5cT/PgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1854d95390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "myn=np.random.randint(1,20000)\n",
    "myxtr=np.array(x_train[myn,:,:,:])\n",
    "myxtrp=np.reshape(myxtr,(28,28))\n",
    "plt.imshow(myxtrp,cmap='Greys_r')\n",
    "myxtr=np.reshape(myxtr,(1,28,28,1))\n",
    "preds = model.predict(myxtr)\n",
    "print(preds)\n",
    "print(np.argmax(preds))\n",
    "print(y_train[myn])\n",
    "if np.argmax(y_train[myn])<10:\n",
    "    ascii=48+np.argmax(y_train[myn])\n",
    "else:\n",
    "    ascii=87+np.argmax(y_train[myn])\n",
    "print(chr(ascii))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuaracy = 0.862200\n"
     ]
    }
   ],
   "source": [
    "yhat=model.predict(x_test)\n",
    "yhatp=np.argmax(yhat,axis=1)\n",
    "ytsp=np.argmax(y_test,axis=1)\n",
    "acc = np.mean(yhatp == ytsp)\n",
    "print('Accuaracy = {0:f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.843 0.    0.    ... 0.    0.    0.   ]\n",
      " [0.    0.739 0.002 ... 0.    0.    0.   ]\n",
      " [0.    0.    0.957 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.2   0.    0.   ]\n",
      " [0.    0.014 0.    ... 0.    0.829 0.014]\n",
      " [0.    0.    0.    ... 0.    0.    0.895]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x18304090b8>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAqsAAAJCCAYAAADnUI67AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAGbLZBUAgNn6/wFVHMd/Ug+EkAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1824c50da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "C = confusion_matrix(ytsp,yhatp)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "#Csum = np.sum(C,1)\n",
    "#C = C / Csum[None,:]\n",
    "C = normalize(C, norm='l1', axis=1)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(np.array_str(C, precision=3, suppress_small=True))\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.imshow(C, interpolation='none')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84325397 0.73854962 0.95687885 0.99441341 0.94639175 0.94305239\n",
      " 0.97797357 0.97194389 0.96082474 0.975      0.78409091 0.86440678\n",
      " 0.90769231 0.89230769 0.85483871 0.78787879 0.74       0.87804878\n",
      " 0.31279621 0.77333333 0.90909091 0.56610169 0.95679012 0.93023256\n",
      " 0.47150259 0.95092025 0.83333333 0.86440678 0.82857143 0.90849673\n",
      " 0.94977169 0.90322581 0.94117647 0.9047619  0.875      0.66233766\n",
      " 0.96969697 0.57142857 0.93835616 0.95396419 0.27906977 0.31914894\n",
      " 0.9109589  0.94565217 0.2        0.82938389 0.89495798]\n",
      "(array([ 1, 10, 15, 16, 18, 19, 21, 24, 35, 37, 40, 41, 44]),)\n"
     ]
    }
   ],
   "source": [
    "Cd=C.diagonal()\n",
    "print(Cd)\n",
    "print(np.where(Cd<0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use a loop to find the best parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=16,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 29s 640us/step - loss: 0.7548 - acc: 0.7863 - val_loss: 0.4637 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 28s 614us/step - loss: 0.3944 - acc: 0.8632 - val_loss: 0.3924 - val_acc: 0.8644\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 28s 606us/step - loss: 0.3256 - acc: 0.8814 - val_loss: 0.3784 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 29s 625us/step - loss: 0.2899 - acc: 0.8925 - val_loss: 0.3865 - val_acc: 0.8741\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 28s 608us/step - loss: 0.2595 - acc: 0.9013 - val_loss: 0.3888 - val_acc: 0.8737\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 28s 611us/step - loss: 0.2366 - acc: 0.9104 - val_loss: 0.3883 - val_acc: 0.8748\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 28s 612us/step - loss: 0.2188 - acc: 0.9144 - val_loss: 0.3990 - val_acc: 0.8720\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 28s 619us/step - loss: 0.2039 - acc: 0.9202 - val_loss: 0.3967 - val_acc: 0.8731\n",
      "nc=16,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 33s 718us/step - loss: 0.9367 - acc: 0.7661 - val_loss: 0.4698 - val_acc: 0.8445\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 32s 695us/step - loss: 0.4140 - acc: 0.8562 - val_loss: 0.4129 - val_acc: 0.8624\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 732us/step - loss: 0.3476 - acc: 0.8755 - val_loss: 0.4079 - val_acc: 0.8610\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 32s 705us/step - loss: 0.3040 - acc: 0.8877 - val_loss: 0.3806 - val_acc: 0.8751\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2752 - acc: 0.8972 - val_loss: 0.3831 - val_acc: 0.8743\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 32s 697us/step - loss: 0.2509 - acc: 0.9032 - val_loss: 0.3857 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 32s 698us/step - loss: 0.2324 - acc: 0.9101 - val_loss: 0.3840 - val_acc: 0.8749\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 32s 696us/step - loss: 0.2162 - acc: 0.9161 - val_loss: 0.3982 - val_acc: 0.8698\n",
      "nc=16,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 1.1055 - acc: 0.7681 - val_loss: 0.4701 - val_acc: 0.8417\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 34s 735us/step - loss: 0.4161 - acc: 0.8578 - val_loss: 0.4386 - val_acc: 0.8524\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 34s 736us/step - loss: 0.3495 - acc: 0.8747 - val_loss: 0.3977 - val_acc: 0.8678\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 34s 742us/step - loss: 0.3093 - acc: 0.8872 - val_loss: 0.3826 - val_acc: 0.8720\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2801 - acc: 0.8947 - val_loss: 0.3785 - val_acc: 0.8744\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 36s 785us/step - loss: 0.2580 - acc: 0.9020 - val_loss: 0.3776 - val_acc: 0.8766\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 35s 771us/step - loss: 0.2370 - acc: 0.9081 - val_loss: 0.3822 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.2226 - acc: 0.9134 - val_loss: 0.3790 - val_acc: 0.8799\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7467 - acc: 0.7870 - val_loss: 0.4430 - val_acc: 0.8529\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.3786 - acc: 0.8685 - val_loss: 0.4073 - val_acc: 0.8492\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3143 - acc: 0.8846 - val_loss: 0.3723 - val_acc: 0.8758\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2737 - acc: 0.8976 - val_loss: 0.3672 - val_acc: 0.8773\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2433 - acc: 0.9068 - val_loss: 0.3808 - val_acc: 0.8786\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2220 - acc: 0.9125 - val_loss: 0.3756 - val_acc: 0.8825\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2002 - acc: 0.9217 - val_loss: 0.3798 - val_acc: 0.8818\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.1863 - acc: 0.9258 - val_loss: 0.3988 - val_acc: 0.8786\n",
      "nc=32,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8393 - acc: 0.7738 - val_loss: 0.4995 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3981 - acc: 0.8619 - val_loss: 0.4007 - val_acc: 0.8703\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 62s 1ms/step - loss: 0.3368 - acc: 0.8789 - val_loss: 0.4001 - val_acc: 0.8655\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 66s 1ms/step - loss: 0.2970 - acc: 0.8907 - val_loss: 0.3835 - val_acc: 0.8703\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2661 - acc: 0.8997 - val_loss: 0.4000 - val_acc: 0.8728\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 65s 1ms/step - loss: 0.2428 - acc: 0.9067 - val_loss: 0.3850 - val_acc: 0.8765\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2253 - acc: 0.9131 - val_loss: 0.3970 - val_acc: 0.8721\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.2074 - acc: 0.9179 - val_loss: 0.3977 - val_acc: 0.8760\n",
      "nc=32,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 1.0933 - acc: 0.7525 - val_loss: 0.5016 - val_acc: 0.8304\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 64s 1ms/step - loss: 0.4315 - acc: 0.8511 - val_loss: 0.4396 - val_acc: 0.8518\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.3682 - acc: 0.8710 - val_loss: 0.3942 - val_acc: 0.8636\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.3274 - acc: 0.8834 - val_loss: 0.4291 - val_acc: 0.8579\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 1ms/step - loss: 0.3016 - acc: 0.8899 - val_loss: 0.4213 - val_acc: 0.8612\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2770 - acc: 0.8971 - val_loss: 0.3989 - val_acc: 0.8724\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.2579 - acc: 0.9027 - val_loss: 0.3909 - val_acc: 0.8725\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.2439 - acc: 0.9063 - val_loss: 0.4026 - val_acc: 0.8754\n",
      "nc=48,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.7368 - acc: 0.7928 - val_loss: 0.4215 - val_acc: 0.8560\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.3713 - acc: 0.8690 - val_loss: 0.3846 - val_acc: 0.8729\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.3077 - acc: 0.8882 - val_loss: 0.3720 - val_acc: 0.8741\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 87s 2ms/step - loss: 0.2687 - acc: 0.8981 - val_loss: 0.3581 - val_acc: 0.8798\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 89s 2ms/step - loss: 0.2377 - acc: 0.9083 - val_loss: 0.3744 - val_acc: 0.8811\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.2139 - acc: 0.9153 - val_loss: 0.3856 - val_acc: 0.8696\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1950 - acc: 0.9222 - val_loss: 0.3846 - val_acc: 0.8770\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.1801 - acc: 0.9273 - val_loss: 0.4065 - val_acc: 0.8778\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.8842 - acc: 0.7788 - val_loss: 0.4717 - val_acc: 0.8389\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3909 - acc: 0.8625 - val_loss: 0.3847 - val_acc: 0.8690\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.3213 - acc: 0.8834 - val_loss: 0.3819 - val_acc: 0.8684\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2832 - acc: 0.8954 - val_loss: 0.3800 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2515 - acc: 0.9045 - val_loss: 0.3748 - val_acc: 0.8773\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2298 - acc: 0.9128 - val_loss: 0.3798 - val_acc: 0.8769\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.2072 - acc: 0.9190 - val_loss: 0.3862 - val_acc: 0.8799\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.1919 - acc: 0.9245 - val_loss: 0.3916 - val_acc: 0.8775\n",
      "nc=48,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 1.1306 - acc: 0.7643 - val_loss: 0.4796 - val_acc: 0.8355\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.4054 - acc: 0.8600 - val_loss: 0.4278 - val_acc: 0.8609\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.3385 - acc: 0.8786 - val_loss: 0.3986 - val_acc: 0.8627\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2957 - acc: 0.8904 - val_loss: 0.3771 - val_acc: 0.8767\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.2683 - acc: 0.8990 - val_loss: 0.3837 - val_acc: 0.8752\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.2419 - acc: 0.9078 - val_loss: 0.3949 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 94s 2ms/step - loss: 0.2225 - acc: 0.9138 - val_loss: 0.4025 - val_acc: 0.8731\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.2052 - acc: 0.9203 - val_loss: 0.3930 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([384,512,640])\n",
    "nconvs=np.array([16,32,48])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8748 0.8751 0.8799]\n",
      " [0.8825 0.8765 0.8754]\n",
      " [0.8811 0.8799 0.8785]]\n",
      "(3, 3)\n",
      "the best nconvs is 32,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8825\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go further from above to find better nconvs and n node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=24,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.6819 - acc: 0.7966 - val_loss: 0.4265 - val_acc: 0.8564\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 978us/step - loss: 0.3733 - acc: 0.8688 - val_loss: 0.3767 - val_acc: 0.8716\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 45s 981us/step - loss: 0.3129 - acc: 0.8860 - val_loss: 0.3692 - val_acc: 0.8732\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 46s 996us/step - loss: 0.2722 - acc: 0.8973 - val_loss: 0.3689 - val_acc: 0.8759\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2435 - acc: 0.9066 - val_loss: 0.3786 - val_acc: 0.8713\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2218 - acc: 0.9136 - val_loss: 0.3775 - val_acc: 0.8767\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2036 - acc: 0.9215 - val_loss: 0.3817 - val_acc: 0.8789\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.1874 - acc: 0.9259 - val_loss: 0.3933 - val_acc: 0.8784\n",
      "nc=24,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.7690 - acc: 0.7808 - val_loss: 0.5000 - val_acc: 0.8369\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 45s 983us/step - loss: 0.3935 - acc: 0.8620 - val_loss: 0.4044 - val_acc: 0.8660\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 46s 991us/step - loss: 0.3333 - acc: 0.8807 - val_loss: 0.3885 - val_acc: 0.8681\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.2929 - acc: 0.8925 - val_loss: 0.3682 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 46s 1ms/step - loss: 0.2651 - acc: 0.9006 - val_loss: 0.3882 - val_acc: 0.8704\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 46s 994us/step - loss: 0.2411 - acc: 0.9090 - val_loss: 0.3714 - val_acc: 0.8795\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 46s 999us/step - loss: 0.2201 - acc: 0.9150 - val_loss: 0.3832 - val_acc: 0.8778\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 46s 1000us/step - loss: 0.2071 - acc: 0.9185 - val_loss: 0.3901 - val_acc: 0.8772\n",
      "nc=24,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 48s 1ms/step - loss: 0.8109 - acc: 0.7831 - val_loss: 0.4671 - val_acc: 0.8441\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3964 - acc: 0.8613 - val_loss: 0.3873 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.3324 - acc: 0.8814 - val_loss: 0.3888 - val_acc: 0.8712\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2894 - acc: 0.8936 - val_loss: 0.3917 - val_acc: 0.8714\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2648 - acc: 0.8996 - val_loss: 0.3812 - val_acc: 0.8801\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2417 - acc: 0.9075 - val_loss: 0.3808 - val_acc: 0.8787\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2232 - acc: 0.9136 - val_loss: 0.3913 - val_acc: 0.8702\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 47s 1ms/step - loss: 0.2078 - acc: 0.9188 - val_loss: 0.4019 - val_acc: 0.8776\n",
      "nc=32,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.7116 - acc: 0.7918 - val_loss: 0.4572 - val_acc: 0.8496\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3812 - acc: 0.8660 - val_loss: 0.3714 - val_acc: 0.8718\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.3205 - acc: 0.8832 - val_loss: 0.3627 - val_acc: 0.8745\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2816 - acc: 0.8960 - val_loss: 0.3585 - val_acc: 0.8771\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2538 - acc: 0.9049 - val_loss: 0.3656 - val_acc: 0.8797\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2294 - acc: 0.9112 - val_loss: 0.3647 - val_acc: 0.8806\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.2098 - acc: 0.9179 - val_loss: 0.3715 - val_acc: 0.8808\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.1939 - acc: 0.9227 - val_loss: 0.3870 - val_acc: 0.8790\n",
      "nc=32,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 60s 1ms/step - loss: 0.8128 - acc: 0.7774 - val_loss: 0.4525 - val_acc: 0.8467\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3975 - acc: 0.8618 - val_loss: 0.3925 - val_acc: 0.8663\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.3298 - acc: 0.8815 - val_loss: 0.3806 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 59s 1ms/step - loss: 0.2913 - acc: 0.8930 - val_loss: 0.3850 - val_acc: 0.8708\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2621 - acc: 0.9032 - val_loss: 0.3690 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2389 - acc: 0.9090 - val_loss: 0.3833 - val_acc: 0.8792\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2200 - acc: 0.9156 - val_loss: 0.3751 - val_acc: 0.8794\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 58s 1ms/step - loss: 0.2038 - acc: 0.9205 - val_loss: 0.3897 - val_acc: 0.8780\n",
      "nc=32,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 63s 1ms/step - loss: 0.8310 - acc: 0.7800 - val_loss: 0.4775 - val_acc: 0.8469\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3909 - acc: 0.8627 - val_loss: 0.4053 - val_acc: 0.8635\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.3223 - acc: 0.8837 - val_loss: 0.3899 - val_acc: 0.8740\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2803 - acc: 0.8947 - val_loss: 0.3653 - val_acc: 0.8786\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2470 - acc: 0.9056 - val_loss: 0.3732 - val_acc: 0.8790\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2245 - acc: 0.9127 - val_loss: 0.3889 - val_acc: 0.8734\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.2038 - acc: 0.9200 - val_loss: 0.3866 - val_acc: 0.8780\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 61s 1ms/step - loss: 0.1895 - acc: 0.9256 - val_loss: 0.4003 - val_acc: 0.8761\n",
      "nc=40,node=320\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.7113 - acc: 0.7892 - val_loss: 0.4624 - val_acc: 0.8470\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 76s 2ms/step - loss: 0.3717 - acc: 0.8683 - val_loss: 0.3713 - val_acc: 0.8695\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3081 - acc: 0.8891 - val_loss: 0.3742 - val_acc: 0.8725\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2702 - acc: 0.8975 - val_loss: 0.3625 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2395 - acc: 0.9074 - val_loss: 0.3652 - val_acc: 0.8756\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2171 - acc: 0.9156 - val_loss: 0.3814 - val_acc: 0.8750\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1967 - acc: 0.9228 - val_loss: 0.3803 - val_acc: 0.8768\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1831 - acc: 0.9267 - val_loss: 0.3785 - val_acc: 0.8786\n",
      "nc=40,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.7509 - acc: 0.7862 - val_loss: 0.4441 - val_acc: 0.8465\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3778 - acc: 0.8675 - val_loss: 0.3792 - val_acc: 0.8691\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.3165 - acc: 0.8853 - val_loss: 0.3916 - val_acc: 0.8672\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2771 - acc: 0.8974 - val_loss: 0.3649 - val_acc: 0.8750\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2486 - acc: 0.9049 - val_loss: 0.3561 - val_acc: 0.8791\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2245 - acc: 0.9128 - val_loss: 0.3640 - val_acc: 0.8813\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 0.2052 - acc: 0.9194 - val_loss: 0.3761 - val_acc: 0.8823\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1915 - acc: 0.9252 - val_loss: 0.3983 - val_acc: 0.8824\n",
      "nc=40,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.8051 - acc: 0.7795 - val_loss: 0.5280 - val_acc: 0.8251\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.3920 - acc: 0.8632 - val_loss: 0.3846 - val_acc: 0.8642\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.3228 - acc: 0.8827 - val_loss: 0.3765 - val_acc: 0.8719\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2829 - acc: 0.8955 - val_loss: 0.3778 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.2494 - acc: 0.9052 - val_loss: 0.3731 - val_acc: 0.8770\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2253 - acc: 0.9132 - val_loss: 0.3756 - val_acc: 0.8777\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2030 - acc: 0.9196 - val_loss: 0.3984 - val_acc: 0.8738\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1875 - acc: 0.9262 - val_loss: 0.4055 - val_acc: 0.8746\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([320,384,448])\n",
    "nconvs=np.array([24,32,40])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8789 0.8795 0.8801]\n",
      " [0.8808 0.8794 0.879 ]\n",
      " [0.8789 0.8824 0.8777]]\n",
      "(3, 3)\n",
      "the best nconvs is 40,the best nnode is 384\n",
      "the maximum val_accuracy is 0.8824\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the best parameters in adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlrate=0.5\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 15.3239 - acc: 0.0481 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3373 - val_acc: 0.0484\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3380 - val_acc: 0.0484\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 15.3378 - acc: 0.0484 - val_loss: 15.3364 - val_acc: 0.0485\n",
      "nlrate=0.1\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 1.0963 - acc: 0.7346 - val_loss: 0.9226 - val_acc: 0.7157\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4747 - acc: 0.8390 - val_loss: 0.4604 - val_acc: 0.8503\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.4100 - acc: 0.8592 - val_loss: 0.4354 - val_acc: 0.8538\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.3737 - acc: 0.8695 - val_loss: 0.4122 - val_acc: 0.8661\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.3481 - acc: 0.8754 - val_loss: 0.4121 - val_acc: 0.8616\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3329 - acc: 0.8828 - val_loss: 0.4099 - val_acc: 0.8646\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3160 - acc: 0.8868 - val_loss: 0.4043 - val_acc: 0.8681\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3060 - acc: 0.8900 - val_loss: 0.3969 - val_acc: 0.8676\n",
      "nlrate=0.05\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 84s 2ms/step - loss: 0.7607 - acc: 0.7818 - val_loss: 0.5171 - val_acc: 0.8324\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3861 - acc: 0.8643 - val_loss: 0.3910 - val_acc: 0.8650\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.3209 - acc: 0.8834 - val_loss: 0.3837 - val_acc: 0.8688\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 80s 2ms/step - loss: 0.2822 - acc: 0.8963 - val_loss: 0.4366 - val_acc: 0.8546\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2535 - acc: 0.9035 - val_loss: 0.3835 - val_acc: 0.8735\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 77s 2ms/step - loss: 0.2317 - acc: 0.9097 - val_loss: 0.3803 - val_acc: 0.8773\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2111 - acc: 0.9179 - val_loss: 0.3918 - val_acc: 0.8766\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1954 - acc: 0.9233 - val_loss: 0.3879 - val_acc: 0.8778\n",
      "nlrate=0.01\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.6256 - acc: 0.8043 - val_loss: 0.4380 - val_acc: 0.8444\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.3671 - acc: 0.8697 - val_loss: 0.3794 - val_acc: 0.8731\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 73s 2ms/step - loss: 0.2971 - acc: 0.8896 - val_loss: 0.3908 - val_acc: 0.8694\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 78s 2ms/step - loss: 0.2508 - acc: 0.9035 - val_loss: 0.3646 - val_acc: 0.8789\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.2177 - acc: 0.9149 - val_loss: 0.3810 - val_acc: 0.8764\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 74s 2ms/step - loss: 0.1913 - acc: 0.9236 - val_loss: 0.3842 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.1673 - acc: 0.9328 - val_loss: 0.3969 - val_acc: 0.8751\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1506 - acc: 0.9382 - val_loss: 0.4106 - val_acc: 0.8787\n",
      "nlrate=0.005\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 0.6163 - acc: 0.8041 - val_loss: 0.4270 - val_acc: 0.8538\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.3637 - acc: 0.8706 - val_loss: 0.3810 - val_acc: 0.8679\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.2955 - acc: 0.8908 - val_loss: 0.3866 - val_acc: 0.8659\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2517 - acc: 0.9033 - val_loss: 0.3508 - val_acc: 0.8832\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.2184 - acc: 0.9144 - val_loss: 0.3793 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1919 - acc: 0.9229 - val_loss: 0.4034 - val_acc: 0.8738\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 72s 2ms/step - loss: 0.1701 - acc: 0.9308 - val_loss: 0.3874 - val_acc: 0.8777\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.1522 - acc: 0.9385 - val_loss: 0.3983 - val_acc: 0.8771\n"
     ]
    }
   ],
   "source": [
    "history1=[]\n",
    "nlrate=np.array([0.5,0.1,0.05,0.01,0.005])\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    K.clear_session()\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(convmax, (3, 3), \n",
    "                     padding='valid', \n",
    "                     input_shape=x_train.shape[1:],\n",
    "                     activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(convmax, (3, 3), padding='valid', activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "    #model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(nodemax, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "    #model.add(BatchNormalization())\n",
    "    decay = lrate/epochs\n",
    "    opt = keras.optimizers.adam(lr=lrate, decay=decay)\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
    "    print('nlrate={}'.format(lrate))\n",
    "    hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,\n",
    "                           validation_data=(x_test, y_test),shuffle=True)\n",
    "    history1.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0485]\n",
      " [0.8681]\n",
      " [0.8778]\n",
      " [0.8789]\n",
      " [0.8832]]\n",
      "the best nlrate is 0.005\n",
      "the maximum val_accuracy is 0.8832\n"
     ]
    }
   ],
   "source": [
    "hh=np.zeros((5,1))\n",
    "for n in range(5):\n",
    "    hh[n]=np.max(history1[n].history['val_acc'])\n",
    "print(hh)\n",
    "c=0\n",
    "for i,lrate in enumerate(nlrate):\n",
    "    if hh[i]>c:\n",
    "        c=hh[i]\n",
    "        lratemax=lrate\n",
    "print('the best nlrate is {}'.format(lratemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(hh)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=16,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 37s 796us/step - loss: 1.2729 - acc: 0.6410 - val_loss: 0.5674 - val_acc: 0.8055\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 36s 779us/step - loss: 0.7804 - acc: 0.7468 - val_loss: 0.4584 - val_acc: 0.8478\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 36s 773us/step - loss: 0.6805 - acc: 0.7771 - val_loss: 0.4319 - val_acc: 0.8491\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 36s 775us/step - loss: 0.6393 - acc: 0.7880 - val_loss: 0.4124 - val_acc: 0.8636\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 36s 779us/step - loss: 0.5937 - acc: 0.8018 - val_loss: 0.3883 - val_acc: 0.8653\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 37s 805us/step - loss: 0.5775 - acc: 0.8049 - val_loss: 0.3867 - val_acc: 0.8647\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 37s 808us/step - loss: 0.5607 - acc: 0.8110 - val_loss: 0.3786 - val_acc: 0.8699\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 37s 815us/step - loss: 0.5412 - acc: 0.8159 - val_loss: 0.3699 - val_acc: 0.8688\n",
      "nc=16,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 41s 901us/step - loss: 1.3346 - acc: 0.6392 - val_loss: 0.6212 - val_acc: 0.8090\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 40s 867us/step - loss: 0.8131 - acc: 0.7401 - val_loss: 0.4785 - val_acc: 0.8366\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6981 - acc: 0.7670 - val_loss: 0.4590 - val_acc: 0.8447\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6484 - acc: 0.7845 - val_loss: 0.4146 - val_acc: 0.8608\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 40s 869us/step - loss: 0.6149 - acc: 0.7942 - val_loss: 0.4003 - val_acc: 0.8615\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 42s 909us/step - loss: 0.5765 - acc: 0.8076 - val_loss: 0.3867 - val_acc: 0.8673\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 40s 876us/step - loss: 0.5633 - acc: 0.8073 - val_loss: 0.3749 - val_acc: 0.8695\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 40s 880us/step - loss: 0.5497 - acc: 0.8138 - val_loss: 0.3660 - val_acc: 0.8719\n",
      "nc=16,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 44s 947us/step - loss: 1.3598 - acc: 0.6418 - val_loss: 0.5219 - val_acc: 0.8391\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 42s 915us/step - loss: 0.7936 - acc: 0.7502 - val_loss: 0.4434 - val_acc: 0.8509\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 42s 914us/step - loss: 0.6773 - acc: 0.7789 - val_loss: 0.4229 - val_acc: 0.8539\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 42s 918us/step - loss: 0.6164 - acc: 0.7935 - val_loss: 0.4258 - val_acc: 0.8542\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 43s 936us/step - loss: 0.5852 - acc: 0.8010 - val_loss: 0.4110 - val_acc: 0.8607\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 41s 897us/step - loss: 0.5554 - acc: 0.8106 - val_loss: 0.3787 - val_acc: 0.8684\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 42s 903us/step - loss: 0.5415 - acc: 0.8161 - val_loss: 0.3738 - val_acc: 0.8702\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 41s 901us/step - loss: 0.5295 - acc: 0.8191 - val_loss: 0.3640 - val_acc: 0.8723\n",
      "nc=32,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 1.1349 - acc: 0.6782 - val_loss: 0.5100 - val_acc: 0.8386\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.6749 - acc: 0.7797 - val_loss: 0.4271 - val_acc: 0.8536\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5776 - acc: 0.8067 - val_loss: 0.3877 - val_acc: 0.8683\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5389 - acc: 0.8187 - val_loss: 0.3714 - val_acc: 0.8711\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5080 - acc: 0.8282 - val_loss: 0.3638 - val_acc: 0.8730\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4843 - acc: 0.8330 - val_loss: 0.3535 - val_acc: 0.8802\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4644 - acc: 0.8391 - val_loss: 0.3509 - val_acc: 0.8767\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.4511 - acc: 0.8435 - val_loss: 0.3407 - val_acc: 0.8809\n",
      "nc=32,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 69s 2ms/step - loss: 1.1723 - acc: 0.6795 - val_loss: 0.4996 - val_acc: 0.8317\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.6826 - acc: 0.7793 - val_loss: 0.4339 - val_acc: 0.8549\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.5816 - acc: 0.8035 - val_loss: 0.3944 - val_acc: 0.8654\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 67s 1ms/step - loss: 0.5239 - acc: 0.8205 - val_loss: 0.3891 - val_acc: 0.8635\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4963 - acc: 0.8290 - val_loss: 0.3595 - val_acc: 0.8764\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4777 - acc: 0.8348 - val_loss: 0.3571 - val_acc: 0.8767\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4546 - acc: 0.8417 - val_loss: 0.3523 - val_acc: 0.8781\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 68s 1ms/step - loss: 0.4429 - acc: 0.8455 - val_loss: 0.3452 - val_acc: 0.8840\n",
      "nc=32,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 71s 2ms/step - loss: 1.2270 - acc: 0.6756 - val_loss: 0.5179 - val_acc: 0.8399\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.6895 - acc: 0.7800 - val_loss: 0.4390 - val_acc: 0.8503\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.5845 - acc: 0.8052 - val_loss: 0.3838 - val_acc: 0.8710\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.5290 - acc: 0.8210 - val_loss: 0.3860 - val_acc: 0.8665\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4901 - acc: 0.8312 - val_loss: 0.3652 - val_acc: 0.8731\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4714 - acc: 0.8367 - val_loss: 0.3511 - val_acc: 0.8763\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4517 - acc: 0.8417 - val_loss: 0.3434 - val_acc: 0.8797\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 70s 2ms/step - loss: 0.4396 - acc: 0.8448 - val_loss: 0.3443 - val_acc: 0.8774\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.0758 - acc: 0.6925 - val_loss: 0.5251 - val_acc: 0.8308\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.6324 - acc: 0.7931 - val_loss: 0.4158 - val_acc: 0.8581\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.5486 - acc: 0.8146 - val_loss: 0.3914 - val_acc: 0.8617\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.4994 - acc: 0.8279 - val_loss: 0.3591 - val_acc: 0.8746\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4665 - acc: 0.8383 - val_loss: 0.3488 - val_acc: 0.8789\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4476 - acc: 0.8434 - val_loss: 0.3461 - val_acc: 0.8825\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4322 - acc: 0.8489 - val_loss: 0.3383 - val_acc: 0.8843\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4181 - acc: 0.8538 - val_loss: 0.3354 - val_acc: 0.8843\n",
      "nc=48,node=640\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.1281 - acc: 0.6913 - val_loss: 0.5035 - val_acc: 0.8354\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.6516 - acc: 0.7875 - val_loss: 0.4171 - val_acc: 0.8629\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.5504 - acc: 0.8166 - val_loss: 0.4056 - val_acc: 0.8603\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.4978 - acc: 0.8319 - val_loss: 0.3809 - val_acc: 0.8651\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4719 - acc: 0.8366 - val_loss: 0.3474 - val_acc: 0.8803\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.4444 - acc: 0.8439 - val_loss: 0.3524 - val_acc: 0.8758\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4270 - acc: 0.8517 - val_loss: 0.3423 - val_acc: 0.8798\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 97s 2ms/step - loss: 0.4150 - acc: 0.8538 - val_loss: 0.3395 - val_acc: 0.8797\n",
      "nc=48,node=768\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 103s 2ms/step - loss: 1.1533 - acc: 0.6935 - val_loss: 0.5473 - val_acc: 0.8152\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.6521 - acc: 0.7908 - val_loss: 0.4104 - val_acc: 0.8612\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.5419 - acc: 0.8181 - val_loss: 0.3869 - val_acc: 0.8659\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4921 - acc: 0.8304 - val_loss: 0.3684 - val_acc: 0.8692\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4597 - acc: 0.8403 - val_loss: 0.3562 - val_acc: 0.8767\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 108s 2ms/step - loss: 0.4366 - acc: 0.8469 - val_loss: 0.3424 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.4199 - acc: 0.8522 - val_loss: 0.3447 - val_acc: 0.8776\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 107s 2ms/step - loss: 0.4030 - acc: 0.8551 - val_loss: 0.3310 - val_acc: 0.8817\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([512,640,768])\n",
    "nconvs=np.array([16,32,48])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.45))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        decay = 0.005/epochs\n",
    "        opt = keras.optimizers.adam(lr=0.005, decay=decay)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8699 0.8719 0.8723]\n",
      " [0.8809 0.884  0.8797]\n",
      " [0.8843 0.8803 0.8817]]\n",
      "(3, 3)\n",
      "the best nconvs is 48,the best nnode is 512\n",
      "the maximum val_accuracy is 0.8843\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nc=40,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 82s 2ms/step - loss: 1.0752 - acc: 0.6863 - val_loss: 0.4895 - val_acc: 0.8375\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 75s 2ms/step - loss: 0.6582 - acc: 0.7842 - val_loss: 0.4107 - val_acc: 0.8603\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 79s 2ms/step - loss: 0.5750 - acc: 0.8083 - val_loss: 0.3882 - val_acc: 0.8663\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 81s 2ms/step - loss: 0.5250 - acc: 0.8217 - val_loss: 0.3738 - val_acc: 0.8697\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 81s 2ms/step - loss: 0.5053 - acc: 0.8291 - val_loss: 0.3694 - val_acc: 0.8748\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 82s 2ms/step - loss: 0.4780 - acc: 0.8376 - val_loss: 0.3529 - val_acc: 0.8781\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 79s 2ms/step - loss: 0.4576 - acc: 0.8413 - val_loss: 0.3447 - val_acc: 0.8788\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 84s 2ms/step - loss: 0.4474 - acc: 0.8456 - val_loss: 0.3397 - val_acc: 0.8798\n",
      "nc=40,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 85s 2ms/step - loss: 1.1003 - acc: 0.6875 - val_loss: 0.5015 - val_acc: 0.8349\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.6545 - acc: 0.7861 - val_loss: 0.4229 - val_acc: 0.8571\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.5659 - acc: 0.8091 - val_loss: 0.4093 - val_acc: 0.8632\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 86s 2ms/step - loss: 0.5264 - acc: 0.8199 - val_loss: 0.3847 - val_acc: 0.8644\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 88s 2ms/step - loss: 0.4991 - acc: 0.8288 - val_loss: 0.3658 - val_acc: 0.8745\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4678 - acc: 0.8405 - val_loss: 0.3589 - val_acc: 0.8762\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 93s 2ms/step - loss: 0.4561 - acc: 0.8428 - val_loss: 0.3537 - val_acc: 0.8793\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 90s 2ms/step - loss: 0.4425 - acc: 0.8459 - val_loss: 0.3403 - val_acc: 0.8818\n",
      "nc=40,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 1.1038 - acc: 0.6884 - val_loss: 0.4882 - val_acc: 0.8383\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.6549 - acc: 0.7887 - val_loss: 0.4467 - val_acc: 0.8471\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 91s 2ms/step - loss: 0.5650 - acc: 0.8113 - val_loss: 0.3991 - val_acc: 0.8631\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.5174 - acc: 0.8247 - val_loss: 0.3805 - val_acc: 0.8693\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 90s 2ms/step - loss: 0.4929 - acc: 0.8307 - val_loss: 0.3537 - val_acc: 0.8781\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 100s 2ms/step - loss: 0.4641 - acc: 0.8397 - val_loss: 0.3490 - val_acc: 0.8791\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.4499 - acc: 0.8444 - val_loss: 0.3433 - val_acc: 0.8791\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 92s 2ms/step - loss: 0.4319 - acc: 0.8479 - val_loss: 0.3436 - val_acc: 0.8790\n",
      "nc=48,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 100s 2ms/step - loss: 1.0457 - acc: 0.6952 - val_loss: 0.5170 - val_acc: 0.8286\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.6355 - acc: 0.7892 - val_loss: 0.4384 - val_acc: 0.8501\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 108s 2ms/step - loss: 0.5656 - acc: 0.8097 - val_loss: 0.4064 - val_acc: 0.8556\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.5124 - acc: 0.8260 - val_loss: 0.3686 - val_acc: 0.8742\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.4886 - acc: 0.8347 - val_loss: 0.3578 - val_acc: 0.8749\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4681 - acc: 0.8403 - val_loss: 0.3511 - val_acc: 0.8768\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 104s 2ms/step - loss: 0.4450 - acc: 0.8450 - val_loss: 0.3398 - val_acc: 0.8808\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4361 - acc: 0.8478 - val_loss: 0.3323 - val_acc: 0.8835\n",
      "nc=48,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 105s 2ms/step - loss: 1.0653 - acc: 0.6941 - val_loss: 0.4666 - val_acc: 0.8464\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 109s 2ms/step - loss: 0.6332 - acc: 0.7906 - val_loss: 0.4300 - val_acc: 0.8522\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 104s 2ms/step - loss: 0.5474 - acc: 0.8136 - val_loss: 0.3901 - val_acc: 0.8679\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.5023 - acc: 0.8280 - val_loss: 0.3654 - val_acc: 0.8744\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 106s 2ms/step - loss: 0.4734 - acc: 0.8393 - val_loss: 0.3648 - val_acc: 0.8719\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 109s 2ms/step - loss: 0.4594 - acc: 0.8401 - val_loss: 0.3515 - val_acc: 0.8803\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 118s 3ms/step - loss: 0.4406 - acc: 0.8487 - val_loss: 0.3416 - val_acc: 0.8836\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 107s 2ms/step - loss: 0.4247 - acc: 0.8513 - val_loss: 0.3481 - val_acc: 0.8810\n",
      "nc=48,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 1.0833 - acc: 0.6926 - val_loss: 0.5314 - val_acc: 0.8313\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 0.6381 - acc: 0.7911 - val_loss: 0.4092 - val_acc: 0.8555\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 0.5485 - acc: 0.8146 - val_loss: 0.3858 - val_acc: 0.8728\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.5069 - acc: 0.8273 - val_loss: 0.3635 - val_acc: 0.8770\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4704 - acc: 0.8380 - val_loss: 0.3632 - val_acc: 0.8734\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4558 - acc: 0.8431 - val_loss: 0.3424 - val_acc: 0.8828\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4355 - acc: 0.8485 - val_loss: 0.3460 - val_acc: 0.8811\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.4149 - acc: 0.8541 - val_loss: 0.3302 - val_acc: 0.8845\n",
      "nc=56,node=384\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 115s 3ms/step - loss: 1.0240 - acc: 0.7032 - val_loss: 0.5051 - val_acc: 0.8292\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.6298 - acc: 0.7922 - val_loss: 0.4298 - val_acc: 0.8581\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.5586 - acc: 0.8143 - val_loss: 0.3833 - val_acc: 0.8642\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.5036 - acc: 0.8298 - val_loss: 0.3682 - val_acc: 0.8732\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.4776 - acc: 0.8373 - val_loss: 0.3636 - val_acc: 0.8758\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 0.4537 - acc: 0.8428 - val_loss: 0.3483 - val_acc: 0.8798\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4362 - acc: 0.8463 - val_loss: 0.3468 - val_acc: 0.8785\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4271 - acc: 0.8500 - val_loss: 0.3345 - val_acc: 0.8839\n",
      "nc=56,node=448\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 111s 2ms/step - loss: 1.0446 - acc: 0.6986 - val_loss: 0.5040 - val_acc: 0.8311\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.6290 - acc: 0.7943 - val_loss: 0.4286 - val_acc: 0.8554\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.5382 - acc: 0.8173 - val_loss: 0.3962 - val_acc: 0.8671\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4979 - acc: 0.8320 - val_loss: 0.3711 - val_acc: 0.8693\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4682 - acc: 0.8381 - val_loss: 0.3619 - val_acc: 0.8786\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4456 - acc: 0.8440 - val_loss: 0.3439 - val_acc: 0.8809\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4272 - acc: 0.8510 - val_loss: 0.3545 - val_acc: 0.8796\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 110s 2ms/step - loss: 0.4074 - acc: 0.8566 - val_loss: 0.3356 - val_acc: 0.8836\n",
      "nc=56,node=512\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/8\n",
      "46000/46000 [==============================] - 114s 2ms/step - loss: 1.0466 - acc: 0.7035 - val_loss: 0.4966 - val_acc: 0.8411\n",
      "Epoch 2/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.6194 - acc: 0.7956 - val_loss: 0.4209 - val_acc: 0.8579\n",
      "Epoch 3/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.5297 - acc: 0.8215 - val_loss: 0.3742 - val_acc: 0.8702\n",
      "Epoch 4/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4857 - acc: 0.8341 - val_loss: 0.3782 - val_acc: 0.8707\n",
      "Epoch 5/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4574 - acc: 0.8446 - val_loss: 0.3570 - val_acc: 0.8791\n",
      "Epoch 6/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4387 - acc: 0.8469 - val_loss: 0.3459 - val_acc: 0.8762\n",
      "Epoch 7/8\n",
      "46000/46000 [==============================] - 112s 2ms/step - loss: 0.4168 - acc: 0.8547 - val_loss: 0.3332 - val_acc: 0.8839\n",
      "Epoch 8/8\n",
      "46000/46000 [==============================] - 113s 2ms/step - loss: 0.4045 - acc: 0.8574 - val_loss: 0.3380 - val_acc: 0.8825\n"
     ]
    }
   ],
   "source": [
    "history=[]\n",
    "nodes=np.array([384,448,512])\n",
    "nconvs=np.array([40,48,56])\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        K.clear_session()\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(nc, (3, 3), \n",
    "                         padding='valid', \n",
    "                         input_shape=x_train.shape[1:],\n",
    "                         activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        #model.add(Conv2D(32, (3, 3), padding='valid', activation='relu'))   #+0.01\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        model.add(Dropout(0.15))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(nc, (3, 3), padding='valid', activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(28, (3, 3), padding='valid', activation='relu'))   #+0.00\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.45))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(node, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        #model.add(Dense(62, activation='relu'))   #+0.01\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.6))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        decay = 0.005/epochs\n",
    "        opt = keras.optimizers.adam(lr=0.005, decay=decay)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "        print('nc={},node={}'.format(nc,node))\n",
    "        hist_basic = model.fit(x_train, y_train,batch_size=batch_size,\n",
    "                               epochs=epochs,validation_data=(x_test, y_test),shuffle=True)\n",
    "        history.append(hist_basic)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8798 0.8818 0.8791]\n",
      " [0.8835 0.8836 0.8845]\n",
      " [0.8839 0.8836 0.8839]]\n",
      "(3, 3)\n",
      "the best nconvs is 48,the best nnode is 512\n",
      "the maximum val_accuracy is 0.8845\n"
     ]
    }
   ],
   "source": [
    "h=np.zeros((9,1))\n",
    "for n in range(9):\n",
    "    h[n]=np.max(history[n].history['val_acc'])\n",
    "h1=h.reshape((len(nconvs),len(nodes)))\n",
    "print(h1)\n",
    "print(h1.shape)\n",
    "c=0\n",
    "for i,nc in enumerate(nconvs):\n",
    "    for j,node in enumerate(nodes):\n",
    "        if h1[i,j]>c:\n",
    "            c=h1[i,j]\n",
    "            convmax=nc\n",
    "            nodemax=node\n",
    "print('the best nconvs is {},the best nnode is {}'.format(convmax,nodemax))\n",
    "print('the maximum val_accuracy is {}'.format(np.max(h1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 48)        480       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 48)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 13, 13, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 13, 13, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 48)        20784     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 48)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 5, 5, 48)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1200)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1200)              4800      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               614912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 47)                24111     \n",
      "=================================================================\n",
      "Total params: 667,327\n",
      "Trainable params: 663,807\n",
      "Non-trainable params: 3,520\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 46000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "46000/46000 [==============================] - 103s 2ms/step - loss: 1.2814 - acc: 0.6480 - val_loss: 0.5126 - val_acc: 0.8288\n",
      "Epoch 2/50\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.6810 - acc: 0.7832 - val_loss: 0.4203 - val_acc: 0.8573\n",
      "Epoch 3/50\n",
      "46000/46000 [==============================] - 102s 2ms/step - loss: 0.5750 - acc: 0.8106 - val_loss: 0.4116 - val_acc: 0.8497\n",
      "Epoch 4/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.5283 - acc: 0.8245 - val_loss: 0.4018 - val_acc: 0.8578\n",
      "Epoch 5/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4996 - acc: 0.8318 - val_loss: 0.3769 - val_acc: 0.8688\n",
      "Epoch 6/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4777 - acc: 0.8389 - val_loss: 0.3631 - val_acc: 0.8749\n",
      "Epoch 7/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4605 - acc: 0.8425 - val_loss: 0.3540 - val_acc: 0.8754\n",
      "Epoch 8/50\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.4484 - acc: 0.8453 - val_loss: 0.3445 - val_acc: 0.8807\n",
      "Epoch 9/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4366 - acc: 0.8490 - val_loss: 0.3431 - val_acc: 0.8821\n",
      "Epoch 10/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4323 - acc: 0.8523 - val_loss: 0.3419 - val_acc: 0.8819\n",
      "Epoch 11/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4256 - acc: 0.8531 - val_loss: 0.3510 - val_acc: 0.8786\n",
      "Epoch 12/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4167 - acc: 0.8550 - val_loss: 0.3345 - val_acc: 0.8803\n",
      "Epoch 13/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4140 - acc: 0.8555 - val_loss: 0.3333 - val_acc: 0.8814\n",
      "Epoch 14/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4087 - acc: 0.8598 - val_loss: 0.3326 - val_acc: 0.8813\n",
      "Epoch 15/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4053 - acc: 0.8593 - val_loss: 0.3344 - val_acc: 0.8811\n",
      "Epoch 16/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.4023 - acc: 0.8582 - val_loss: 0.3265 - val_acc: 0.8869\n",
      "Epoch 17/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3999 - acc: 0.8610 - val_loss: 0.3330 - val_acc: 0.8802\n",
      "Epoch 18/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3946 - acc: 0.8633 - val_loss: 0.3428 - val_acc: 0.8777\n",
      "Epoch 19/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3893 - acc: 0.8656 - val_loss: 0.3357 - val_acc: 0.8806\n",
      "Epoch 20/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3882 - acc: 0.8649 - val_loss: 0.3359 - val_acc: 0.8812\n",
      "Epoch 21/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3849 - acc: 0.8648 - val_loss: 0.3469 - val_acc: 0.8819\n",
      "Epoch 22/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3819 - acc: 0.8673 - val_loss: 0.3231 - val_acc: 0.8845\n",
      "Epoch 23/50\n",
      "46000/46000 [==============================] - 101s 2ms/step - loss: 0.3781 - acc: 0.8692 - val_loss: 0.3366 - val_acc: 0.8812\n",
      "Epoch 24/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3751 - acc: 0.8685 - val_loss: 0.3212 - val_acc: 0.8875\n",
      "Epoch 25/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3762 - acc: 0.8670 - val_loss: 0.3291 - val_acc: 0.8899\n",
      "Epoch 26/50\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.3743 - acc: 0.8697 - val_loss: 0.3299 - val_acc: 0.8821\n",
      "Epoch 27/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3715 - acc: 0.8714 - val_loss: 0.3265 - val_acc: 0.8845\n",
      "Epoch 28/50\n",
      "46000/46000 [==============================] - 98s 2ms/step - loss: 0.3704 - acc: 0.8698 - val_loss: 0.3272 - val_acc: 0.8824\n",
      "Epoch 29/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3672 - acc: 0.8697 - val_loss: 0.3137 - val_acc: 0.8905\n",
      "Epoch 30/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3651 - acc: 0.8713 - val_loss: 0.3345 - val_acc: 0.8860\n",
      "Epoch 31/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3652 - acc: 0.8736 - val_loss: 0.3200 - val_acc: 0.8878\n",
      "Epoch 32/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3703 - acc: 0.8703 - val_loss: 0.3176 - val_acc: 0.8900\n",
      "Epoch 33/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3634 - acc: 0.8712 - val_loss: 0.3167 - val_acc: 0.8890\n",
      "Epoch 34/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3654 - acc: 0.8716 - val_loss: 0.3248 - val_acc: 0.8874\n",
      "Epoch 35/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3637 - acc: 0.8724 - val_loss: 0.3216 - val_acc: 0.8886\n",
      "Epoch 36/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3589 - acc: 0.8741 - val_loss: 0.3158 - val_acc: 0.8922\n",
      "Epoch 37/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3568 - acc: 0.8731 - val_loss: 0.3156 - val_acc: 0.8881\n",
      "Epoch 38/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3606 - acc: 0.8721 - val_loss: 0.3173 - val_acc: 0.8874\n",
      "Epoch 39/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3588 - acc: 0.8718 - val_loss: 0.3175 - val_acc: 0.8924\n",
      "Epoch 40/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3545 - acc: 0.8737 - val_loss: 0.3171 - val_acc: 0.8882\n",
      "Epoch 41/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3577 - acc: 0.8725 - val_loss: 0.3169 - val_acc: 0.8889\n",
      "Epoch 42/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3611 - acc: 0.8729 - val_loss: 0.3163 - val_acc: 0.8948\n",
      "Epoch 43/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3500 - acc: 0.8755 - val_loss: 0.3235 - val_acc: 0.8813\n",
      "Epoch 44/50\n",
      "46000/46000 [==============================] - 99s 2ms/step - loss: 0.3516 - acc: 0.8757 - val_loss: 0.3117 - val_acc: 0.8922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.3486 - acc: 0.8769 - val_loss: 0.3142 - val_acc: 0.8900\n",
      "Epoch 46/50\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.3470 - acc: 0.8770 - val_loss: 0.3160 - val_acc: 0.8920\n",
      "Epoch 47/50\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.3494 - acc: 0.8764 - val_loss: 0.3125 - val_acc: 0.8942\n",
      "Epoch 48/50\n",
      "46000/46000 [==============================] - 96s 2ms/step - loss: 0.3497 - acc: 0.8768 - val_loss: 0.3161 - val_acc: 0.8863\n",
      "Epoch 49/50\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.3471 - acc: 0.8765 - val_loss: 0.3098 - val_acc: 0.8945\n",
      "Epoch 50/50\n",
      "46000/46000 [==============================] - 95s 2ms/step - loss: 0.3463 - acc: 0.8763 - val_loss: 0.3138 - val_acc: 0.8930\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(48, (3, 3), \n",
    "                 padding='valid', \n",
    "                 input_shape=x_train.shape[1:],\n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(48, (3, 3), padding='valid', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "seed=7\n",
    "# Fit the model\n",
    "np.random.seed(seed)\n",
    "hist_basic = model.fit(x_train, y_train,batch_size=batch_size,epochs=50,validation_data=(x_test, y_test),shuffle=True)\n",
    "\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
